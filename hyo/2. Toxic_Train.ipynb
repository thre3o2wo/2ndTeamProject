{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d2fe6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2136503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DOCX만 대상으로:\n",
    "1) 정상 조항(표준계약서/법령) 자동 추출\n",
    "2) 독소 후보(분쟁문서) 자동 추출 + 자동 태깅(휴리스틱)\n",
    "3) CSV로 저장(라벨링 가능)\n",
    "\n",
    "사용 방법:\n",
    "- 아래 CONFIG만 네 경로로 수정\n",
    "- python build_clause_dataset_docx_config.py 실행\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    NORMAL_DIRS = [\n",
    "        \"data/raw/law/\",\n",
    "        \"data/raw/rule/\",\n",
    "    ]\n",
    "    DISPUTE_DIRS = [\n",
    "        \"data/raw/case/\",\n",
    "    ]\n",
    "    OUT_CSV = \"data/processed/csv/clauses_to_label.csv\"\n",
    "    LIMIT_EACH = 3000\n",
    "    MIN_LEN = 15\n",
    "    MAX_LEN = 350\n",
    "\n",
    "\n",
    "CFG = Config() \n",
    "    \n",
    "# -------------------------\n",
    "# 1) DOCX 텍스트 추출\n",
    "# -------------------------\n",
    "def read_docx(path: Path) -> str:\n",
    "    doc = Document(str(path))\n",
    "    parts = []\n",
    "    for p in doc.paragraphs:\n",
    "        t = (p.text or \"\").strip()\n",
    "        if t:\n",
    "            parts.append(t)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# -------------------------\n",
    "# 2) 문장 분리/정제\n",
    "# -------------------------\n",
    "# 문장 경계(구분자)를 캡처해서 split: lookbehind 없이 안전\n",
    "_KO_SENT_SPLIT = re.compile(r\"(\\.|!|\\?|다\\.|다\\)|함\\.|됨\\.|한다\\.|한다\\))\\s+\")\n",
    "\n",
    "def split_sentences(text: str, min_len: int, max_len: int):\n",
    "    lines = []\n",
    "    for line in (text or \"\").splitlines():\n",
    "        line = normalize_text(line)\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "\n",
    "    joined = normalize_text(\" \".join(lines))\n",
    "    if not joined:\n",
    "        return []\n",
    "\n",
    "    parts = _KO_SENT_SPLIT.split(joined)\n",
    "\n",
    "    # parts: [문장조각, 구분자, 문장조각, 구분자, ...]\n",
    "    sents = []\n",
    "    buf = \"\"\n",
    "    for i, part in enumerate(parts):\n",
    "        if i % 2 == 0:\n",
    "            buf = part\n",
    "        else:\n",
    "            sent = normalize_text(buf + part)  # 구분자를 다시 붙여서 문장 완성\n",
    "            if min_len <= len(sent) <= max_len:\n",
    "                sents.append(sent)\n",
    "\n",
    "    # 마지막 조각(구분자 없이 끝난 경우)\n",
    "    if len(parts) % 2 == 1:\n",
    "        tail = normalize_text(parts[-1])\n",
    "        if min_len <= len(tail) <= max_len:\n",
    "            sents.append(tail)\n",
    "\n",
    "    return sents\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) 독소 후보 자동 태깅(휴리스틱)\n",
    "# -------------------------\n",
    "TOXIC_PATTERNS: List[Tuple[str, str, int]] = [\n",
    "    (r\"(정산\\s*완료|협의에\\s*따른다|상당\\s*기간|추후\\s*협의)\", \"반환지연형\", 2),\n",
    "    (r\"(보증금.*공제|원상복구.*비용|하자.*비용|추가\\s*정산)\", \"보증금공제확대형\", 2),\n",
    "    (r\"(연체|지급기일|지체|납부\\s*요청|최고)\", \"연체가중형\", 1),\n",
    "    (r\"(해지\\s*통지|계약\\s*해지|해제)\", \"해지압박형\", 2),\n",
    "    (r\"(명도|인도\\s*절차|퇴거|강제집행)\", \"명도압박형\", 2),\n",
    "    (r\"(임대인.*판단|임대인의\\s*재량|임대인이\\s*정한|임대인이\\s*제시)\", \"임대인재량형\", 2),\n",
    "    (r\"(협조한다|거절하지\\s*못한다|서명하여야\\s*한다)\", \"협조강제형\", 1),\n",
    "]\n",
    "\n",
    "def auto_tag_and_risk(sentence: str) -> Tuple[str, int]:\n",
    "    tags = []\n",
    "    risk = 0\n",
    "    for pat, tag, lvl in TOXIC_PATTERNS:\n",
    "        if re.search(pat, sentence):\n",
    "            tags.append(tag)\n",
    "            risk = max(risk, lvl)\n",
    "    return (\";\".join(sorted(set(tags))) if tags else \"\"), risk\n",
    "\n",
    "# -------------------------\n",
    "# 4) 중복 제거\n",
    "# -------------------------\n",
    "def dedup_keep_best(rows: List[Dict]) -> List[Dict]:\n",
    "    best = {}\n",
    "    for r in rows:\n",
    "        key = r[\"text\"]\n",
    "        score = (r.get(\"risk_level_suggested\", 0), -abs(len(key) - 120))\n",
    "        if key not in best or score > best[key][0]:\n",
    "            best[key] = (score, r)\n",
    "    return [v[1] for v in best.values()]\n",
    "\n",
    "# -------------------------\n",
    "# 5) 디렉토리에서 문장 추출\n",
    "# -------------------------\n",
    "def build_rows_from_dir(dir_path: Path, source_group: str, doc_type: str, cfg: Config) -> List[Dict]:\n",
    "    rows = []\n",
    "    for p in sorted(dir_path.rglob(\"*.docx\")):\n",
    "        text = read_docx(p)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        sents = split_sentences(text, cfg.MIN_LEN, cfg.MAX_LEN)\n",
    "        for s in sents:\n",
    "            if doc_type == \"toxic_candidate\":\n",
    "                tags, risk = auto_tag_and_risk(s)\n",
    "            else:\n",
    "                tags, risk = \"\", 0\n",
    "\n",
    "            rows.append({\n",
    "                \"id\": f\"{doc_type}_{uuid.uuid4().hex[:12]}\",\n",
    "                \"doc_type\": doc_type,                 # normal / toxic_candidate\n",
    "                \"source_group\": source_group,         # standard_or_law / dispute_docs\n",
    "                \"source_file\": str(p),\n",
    "                \"text\": s,\n",
    "                \"auto_tags\": tags,\n",
    "                \"risk_level_suggested\": risk,         # 0 정상, 1 경계, 2 독소후보\n",
    "                \"human_label\": \"\",                    # 네가 나중에 채우는 라벨\n",
    "                \"notes\": \"\",\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def write_csv(rows: List[Dict], out_csv: Path):\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cols = [\"id\",\"doc_type\",\"source_group\",\"source_file\",\"text\",\"auto_tags\",\"risk_level_suggested\",\"human_label\",\"notes\"]\n",
    "    with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({c: r.get(c, \"\") for c in cols})\n",
    "\n",
    "def validate_dirs(cfg: Config):\n",
    "    for p in cfg.NORMAL_DIRS + cfg.DISPUTE_DIRS:\n",
    "        pp = Path(p)\n",
    "        if not pp.exists():\n",
    "            raise FileNotFoundError(f\"폴더가 없습니다: {pp}\")\n",
    "\n",
    "def main(cfg: Config):\n",
    "    validate_dirs(cfg)\n",
    "\n",
    "    normal_rows = []\n",
    "    for nd in cfg.NORMAL_DIRS:\n",
    "        normal_rows += build_rows_from_dir(Path(nd), \"standard_or_law\", \"normal\", cfg)\n",
    "\n",
    "    toxic_rows = []\n",
    "    for dd in cfg.DISPUTE_DIRS:\n",
    "        toxic_rows += build_rows_from_dir(Path(dd), \"dispute_docs\", \"toxic_candidate\", cfg)\n",
    "\n",
    "    normal_rows = dedup_keep_best(normal_rows)[:cfg.LIMIT_EACH]\n",
    "    toxic_rows  = dedup_keep_best(toxic_rows)[:cfg.LIMIT_EACH]\n",
    "\n",
    "    all_rows = normal_rows + toxic_rows\n",
    "    out_csv = Path(cfg.OUT_CSV)\n",
    "    write_csv(all_rows, out_csv)\n",
    "\n",
    "    print(f\"[OK] normal={len(normal_rows)}, toxic_candidate={len(toxic_rows)}\")\n",
    "    print(f\"[SAVED] {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce230bc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 174\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m    172\u001b[0m normal_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nd \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mNORMAL_DIRS:\n\u001b[1;32m--> 174\u001b[0m     normal_rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mbuild_rows_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstandard_or_law\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m toxic_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dd \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mDISPUTE_DIRS:\n",
      "Cell \u001b[1;32mIn[5], line 134\u001b[0m, in \u001b[0;36mbuild_rows_from_dir\u001b[1;34m(dir_path, source_group, doc_type, cfg)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m sents \u001b[38;5;241m=\u001b[39m \u001b[43msplit_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMIN_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sents:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m doc_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxic_candidate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[5], line 60\u001b[0m, in \u001b[0;36msplit_sentences\u001b[1;34m(text, min_len, max_len)\u001b[0m\n\u001b[0;32m     58\u001b[0m lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m (text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[1;32m---> 60\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_text\u001b[49m(line)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[0;32m     62\u001b[0m         lines\u001b[38;5;241m.\u001b[39mappend(line)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize_text' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "둘 중 하나만 고르라면 **“둘 다”가 제일 성능이 잘 나와.** 다만 역할이 달라.\n",
    "\n",
    "### A. 법령/시행령/민법/규칙/판례/사례집만 RAG로 찾게 하기\n",
    "\n",
    "장점\n",
    "\n",
    "* 근거가 튼튼하고 “정답(법적 기준)”을 잘 가져옴\n",
    "* 최신/정확한 조문 인용, 요건 정리(2기 연체, 대항력 요건 등)에 강함\n",
    "\n",
    "한계\n",
    "\n",
    "* 사용자가 보는 건 **‘계약서 특약 문장’**인데, 법령은 보통 **“그 문장이 위험한 이유”를 직접적으로 말해주지 않음**\n",
    "* “교묘한 문장(모호한 반환시점, 협조 안 하면 지연 등)”은 **검색이 잘 안 걸리거나** “위법/불공정 판단 포인트”로 연결이 약해질 수 있음\n",
    "\n",
    "### B. 만든 “독소조항 예시(라벨링 데이터)”를 많이 뽑아 넣기\n",
    "\n",
    "장점\n",
    "\n",
    "* 사용자 입력이 계약서 문장일 때, **유사 문장 매칭이 매우 잘 됨**\n",
    "* “반환지연형/임대인재량형” 같은 **실무형 분류**가 가능해짐\n",
    "\n",
    "한계\n",
    "\n",
    "* 예시가 너무 적거나 편향되면 “이런 말투만 독소”로 학습될 수 있음\n",
    "* 독소 예시만 늘리면 오탐이 늘 수 있어서 **정상 조항/중립 조항도 같이** 필요\n",
    "\n",
    "---\n",
    "\n",
    "## 추천 전략 (실제로는 이렇게 많이 함)\n",
    "\n",
    "### 1) RAG 지식베이스 2트랙\n",
    "\n",
    "1. **규범 트랙(근거)**:\n",
    "   `주택임대차보호법/시행령/시행규칙/민법(임대차)/대법원규칙/판례/사례집`\n",
    "2. **패턴 트랙(탐지)**:\n",
    "   만든 **독소조항/경계조항/정상조항** 문장 + 라벨(반환지연형 등)\n",
    "\n",
    "질문이 들어오면\n",
    "\n",
    "* 패턴 트랙으로 “이 문장 유형이 뭐냐”를 잡고\n",
    "* 규범 트랙으로 “왜 문제인지/관련 조문·판례 근거”를 붙이는 식\n",
    "\n",
    "이게 “탐지 + 설명”이 동시에 잘 돼.\n",
    "\n",
    "---\n",
    "\n",
    "## 그럼 독소조항 예시는 “몇 개”가 좋냐?\n",
    "\n",
    "정답은 “가능한 많이”인데, 감을 주면:\n",
    "\n",
    "* **최소 동작**: 독소 50~100개 + 정상 150~300개\n",
    "* **쓸만함**: 독소 300~800개 + 정상 1,000~2,000개\n",
    "* **꽤 탄탄**: 독소 1,500개+ + 정상 5,000개+\n",
    "\n",
    "여기서 “정상”이 더 많은 게 보통 좋아(현실 계약서에서 정상 문장이 압도적으로 많아서).\n",
    "\n",
    "또 독소도 한 덩어리로 뽑지 말고 **유형별로 균형**이 중요해:\n",
    "\n",
    "* 반환지연형\n",
    "* 보증금공제확대형\n",
    "* 사후청구형\n",
    "* 임대인재량형(모호한 ‘협의’, ‘협조’)\n",
    "* 명도압박형\n",
    "* 갱신권 제한형(주택임대차는 이쪽도 큼)\n",
    "* 수선/원상복구 전가형\n",
    "* 정보비대칭형(선순위/하자/권리관계 고지 회피 등)\n",
    "\n",
    "---\n",
    "\n",
    "## 내 추천 결론\n",
    "\n",
    "* **“법령/판례/사례집만”**으로는 독소조항 ‘탐지’가 약해질 가능성이 큼\n",
    "* **“독소조항 예시만”**으로는 근거 제시가 약해지고 편향 위험이 큼\n",
    "* 그래서 **패턴(예시 라벨링) + 근거(법/판례/사례집) 두 축**으로 가는 게 최적"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm (ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
