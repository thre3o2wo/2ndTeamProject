"""
ê·œì¹™(ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™, ëŒ€ë²•ì›ê·œì¹™) DOCX íŒŒì¼ì„ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ CSVë¡œ ì €ì¥
law íŒŒì‹± ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš© - ë””ë ‰í† ë¦¬ë§Œ ë³€ê²½
"""

from pathlib import Path
from docx import Document
import pandas as pd
import re
import uuid
from datetime import datetime

# =========================
# Path ì„¤ì • (ruleìš©ìœ¼ë¡œ ë³€ê²½)
# =========================
BASE_DIR = Path(__file__).resolve().parents[2]

DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
CSV_DIR = PROCESSED_DIR / "csv"
LOG_DIR = PROCESSED_DIR / "log"

RULE_RAW_DIR = RAW_DIR / "rule"  # â† ì—¬ê¸°ë§Œ ë³€ê²½!

RULE_CSV_PATH = CSV_DIR / "rule.csv"  # â† ì¶œë ¥ íŒŒì¼ëª…

# ë””ë ‰í† ë¦¬ ìƒì„±
CSV_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)
RULE_RAW_DIR.mkdir(parents=True, exist_ok=True)

# =========================
# ì¡°ë¬¸ ë¶„ë¦¬ íŒ¨í„´ (ë™ì¼)
# =========================
ARTICLE_PATTERN = re.compile(r"(ì œ\s*\d+ì¡°(?:ì˜\d+)?)(?:\((.*?)\))?")

def parse_law_docx(
    file_path,
    law_name,
    law_type,
    priority,
    effective_date
):
    """
    ê·œì¹™ DOCX íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    (law íŒŒì‹±ê³¼ ë™ì¼í•œ ë¡œì§)
    """
    try:
        doc = Document(file_path)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return pd.DataFrame()
    
    rows = []
    current_article = None
    current_title = ""
    buffer = []

    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            continue

        match = ARTICLE_PATTERN.match(text)

        if match:
            # ì´ì „ ì¡°ë¬¸ ì €ì¥
            if current_article and buffer:
                rows.append({
                    "id": str(uuid.uuid4()),
                    "content": " ".join(buffer),
                    "law_family": "ì£¼íƒì„ëŒ€ì°¨",
                    "law_name": law_name,
                    "law_type": law_type,
                    "article": current_article,
                    "article_title": current_title,
                    "priority": priority,
                    "effective_date": effective_date,
                    "source_file": Path(file_path).name,
                    "parsed_at": datetime.now().isoformat()
                })

            # ìƒˆ ì¡°ë¬¸ ì‹œì‘
            current_article = match.group(1)
            current_title = match.group(2) or ""
            buffer = [text.replace(match.group(0), "").strip()]
        else:
            buffer.append(text)

    # ë§ˆì§€ë§‰ ì¡°ë¬¸ ì €ì¥
    if current_article and buffer:
        rows.append({
            "id": str(uuid.uuid4()),
            "content": " ".join(buffer),
            "law_family": "ì£¼íƒì„ëŒ€ì°¨",
            "law_name": law_name,
            "law_type": law_type,
            "article": current_article,
            "article_title": current_title,
            "priority": priority,
            "effective_date": effective_date,
            "source_file": Path(file_path).name,
            "parsed_at": datetime.now().isoformat()
        })

    return pd.DataFrame(rows)


def log_parsing_result(log_path, law_name, row_count, status="SUCCESS"):
    """íŒŒì‹± ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(f"[{datetime.now().isoformat()}] {status} | {law_name} | {row_count} rows\n")


# =========================
# ì‹¤í–‰ë¶€
# =========================
if __name__ == "__main__":
    
    print("=" * 60)
    print("ê·œì¹™ DOCX íŒŒì¼ íŒŒì‹± ì‹œì‘")
    print("=" * 60)
    
    # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  DOCX íŒŒì¼ ì°¾ê¸°
    docx_files = list(RULE_RAW_DIR.glob("*.docx"))
    
    if not docx_files:
        print(f"\nâš ï¸  ê·œì¹™ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ë‹¤ìŒ ê²½ë¡œì— DOCX íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:")
        print(f"   {RULE_RAW_DIR}")
        print(f"\nâœ… í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸:")
        print(f"   ì¡´ì¬ ì—¬ë¶€: {RULE_RAW_DIR.exists()}")
        if RULE_RAW_DIR.exists():
            all_files = list(RULE_RAW_DIR.glob("*"))
            if all_files:
                print(f"   ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")
                for f in all_files[:5]:
                    print(f"      - {f.name}")
            else:
                print(f"   ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        print(f"\nğŸ“ ì˜ˆì‹œ íŒŒì¼ëª…:")
        print(f"   - ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹)(ì œ35947í˜¸)(20260102).docx")
        print(f"   - ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ê·œì¹™.docx")
        print(f"   - í™•ì •ì¼ì_ëŒ€ë²•ì›ê·œì¹™_2986_20210610.docx")
        
        # ìƒ˜í”Œ CSV ìƒì„±
        print(f"\nğŸ“ ìƒ˜í”Œ CSV íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        sample_df = pd.DataFrame([
            {
                "id": str(uuid.uuid4()),
                "content": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ì ë¶€ì—¬ì‹ ì²­ì€...",
                "law_family": "ì£¼íƒì„ëŒ€ì°¨",
                "law_name": "í™•ì •ì¼ì ê·œì¹™",
                "law_type": "ì‹œí–‰ê·œì¹™",
                "article": "ì œ2ì¡°",
                "article_title": "í™•ì •ì¼ì ë¶€ì—¬ì‹ ì²­",
                "priority": 3,
                "effective_date": "2021-06-10",
                "source_file": "sample.docx",
                "parsed_at": datetime.now().isoformat()
            }
        ])
        sample_df.to_csv(RULE_CSV_PATH, index=False, encoding="utf-8-sig")
        print(f"âœ… ìƒ˜í”Œ CSV ì €ì¥: {RULE_CSV_PATH}")
        
    else:
        # ë°œê²¬ëœ ëª¨ë“  DOCX íŒŒì¼ íŒŒì‹±
        print(f"\nğŸ“‚ ë°œê²¬ëœ DOCX íŒŒì¼: {len(docx_files)}ê°œ")
        for f in docx_files:
            print(f"   - {f.name}")
        
        all_dfs = []
        
        for docx_file in docx_files:
            print(f"\nğŸ”„ íŒŒì‹± ì¤‘: {docx_file.name}")
            
            # íŒŒì¼ëª…ì—ì„œ ê·œì¹™ ì •ë³´ ì¶”ì¶œ
            filename = docx_file.stem
            
            if "ì‹œí–‰ë ¹" in filename:
                law_name = "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹"
                law_type = "ì‹œí–‰ë ¹"
                priority = 2
            elif "ì‹œí–‰ê·œì¹™" in filename:
                law_name = "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ê·œì¹™"
                law_type = "ì‹œí–‰ê·œì¹™"
                priority = 3
            elif "í™•ì •ì¼ì" in filename or "ëŒ€ë²•ì›ê·œì¹™" in filename:
                law_name = "í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™"
                law_type = "ëŒ€ë²•ì›ê·œì¹™"
                priority = 3
            else:
                law_name = filename
                law_type = "ê·œì¹™"
                priority = 3
            
            df = parse_law_docx(
                file_path=docx_file,
                law_name=law_name,
                law_type=law_type,
                priority=priority,
                effective_date="2026-01-02"
            )
            
            if not df.empty:
                all_dfs.append(df)
                print(f"   âœ… {len(df)} ì¡°ë¬¸ íŒŒì‹± ì™„ë£Œ")
            else:
                print(f"   âš ï¸ íŒŒì‹±ëœ ì¡°ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
        if all_dfs:
            final_df = pd.concat(all_dfs, ignore_index=True)
            final_df.to_csv(RULE_CSV_PATH, index=False, encoding="utf-8-sig")
            
            log_parsing_result(
                LOG_DIR / "parsing_log.txt",
                "ê·œì¹™ ì „ì²´",
                len(final_df)
            )
            
            print(f"\nâœ… rule.csv ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ {len(final_df)} ì¡°ë¬¸ ì €ì¥")
            print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {RULE_CSV_PATH}")
            
            # ê·œì¹™ë³„ í†µê³„
            print(f"\nğŸ“ˆ ê·œì¹™ë³„ í†µê³„:")
            stats = final_df.groupby('law_name').size()
            for law, count in stats.items():
                print(f"   - {law}: {count}ê°œ")
        else:
            print("\nâŒ íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)
    print("íŒŒì‹± ì™„ë£Œ")
    print("=" * 60)