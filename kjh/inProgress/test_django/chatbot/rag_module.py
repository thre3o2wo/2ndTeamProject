""" 
Unified Hybrid RAG module (Dense + Sparse) for Korean legal Q&A (e.g., housing lease).

Best-of fusion based on your three candidates:
- final_module_gpt.py: robust pipeline + optional deps + candidate-level BM25 fusion
- final_module_cld.py: modular fusion/rerank utilities (but removed Ollama/EXAONE fallback in this final)
- final_module_gmn.py: optional global-BM25 (true sparse search) pattern

Model choices (as requested)
- normalize_query(): Upstage SOLAR Pro2 (chat)  -> model="solar-pro2"
- generate_answer(): OpenAI GPT-4o-mini         -> model="gpt-4o-mini"
- embeddings (dense retrieval): Upstage SOLAR embedding (configurable)

Hybrid retrieval (Dense + Sparse)
- Dense: PineconeVectorStore similarity_search_with_score (fallback: similarity_search)
- Sparse:
  * default: BM25 on *dense candidates* (no extra corpus preload)
  * optional: *global BM25* (true sparse retrieval) if you call build_global_bm25(...)
- Fusion: rank-based RRF (default) or rank_sum

Environment variables
- PINECONE_API_KEY (required)
- UPSTAGE_API_KEY  (required for Upstage embeddings & normalize_query)
- OPENAI_API_KEY   (required for generate_answer)
- COHERE_API_KEY   (optional, only if enable_rerank=True)

No FastAPI integration. Pure Python module.
"""
from __future__ import annotations

import logging
import math
import os
import re
import heapq
from abc import ABC, abstractmethod
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

from langchain_pinecone import PineconeVectorStore

# ----------------------------
# Optional: Upstage (embeddings + chat)
# ----------------------------
try:
    from langchain_upstage import UpstageEmbeddings, ChatUpstage  # type: ignore
    UPSTAGE_AVAILABLE = True
except Exception:
    UpstageEmbeddings = None  # type: ignore
    ChatUpstage = None  # type: ignore
    UPSTAGE_AVAILABLE = False

# ----------------------------
# Optional: OpenAI chat
# ----------------------------
try:
    from langchain_openai import ChatOpenAI  # type: ignore
    OPENAI_AVAILABLE = True
except Exception:
    ChatOpenAI = None  # type: ignore
    OPENAI_AVAILABLE = False

# ----------------------------
# Optional: BM25 (rank_bm25)
# ----------------------------
try:
    from rank_bm25 import BM25Okapi, BM25Plus  # type: ignore
    BM25_AVAILABLE = True
except Exception:
    BM25Okapi = None  # type: ignore
    BM25Plus = None  # type: ignore
    BM25_AVAILABLE = False

# ----------------------------
# Optional: Kiwi tokenizer
# ----------------------------
try:
    from kiwipiepy import Kiwi  # type: ignore
    KIWI_AVAILABLE = True
except Exception:
    Kiwi = None  # type: ignore
    KIWI_AVAILABLE = False

# ----------------------------
# Optional: Cohere rerank
# ----------------------------
try:
    import cohere  # type: ignore
    COHERE_AVAILABLE = True
except Exception:
    cohere = None  # type: ignore
    COHERE_AVAILABLE = False


# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# --------------------------------------------------------------------------------------
# Index names
# --------------------------------------------------------------------------------------
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index-final",
    "rule": "rule-index-final",
    "case": "case-index-final",
}


# --------------------------------------------------------------------------------------
# Keyword dictionary (query normalization)
# --------------------------------------------------------------------------------------
KEYWORD_DICT: Dict[str, str] = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ìž„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ìž„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ìž„ëŒ€ì¸",
    "ìž„ëŒ€ì—…ìž": "ìž„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ìž„ëŒ€ì¸",
    "ì„¸ìž…ìž": "ìž„ì°¨ì¸", "ì›”ì„¸ìž…ìž": "ìž„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëžŒ": "ìž„ì°¨ì¸",
    "ìž„ì°¨ìž": "ìž„ì°¨ì¸", "ìž…ì£¼ìž": "ìž„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ìž„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ìž„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ìž„ì°¨ì£¼íƒ",
    "ìš°ë¦¬ì§‘": "ìž„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ìž„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ìž„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ìž„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ìž„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „
    "ë³´ì¦ê¸ˆ": "ìž„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ì „ì„¸ê¸ˆ": "ìž„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ë³´í—˜": "ë³´ì¦ê¸ˆë°˜í™˜ë³´ì¦",
    "ëˆëª»ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ì•ˆëŒë ¤ì¤Œ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ëª»ëŒë ¤ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜",
    "ì›”ì„¸": "ì°¨ìž„", "ê´€ë¦¬ë¹„": "ê´€ë¦¬ë¹„", "ì—°ì²´": "ì°¨ìž„ì—°ì²´", "ë°€ë¦¼": "ì°¨ìž„ì—°ì²´",
    "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ": "ì¤‘ê°œë³´ìˆ˜", "ì¤‘ê°œë¹„": "ì¤‘ê°œë³´ìˆ˜",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ìž„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ë”ë‹¬ë¼ê³ í•¨": "ì¦ì•¡",
    "ì›”ì„¸ê¹Žê¸°": "ì°¨ìž„ê°ì•¡", "í• ì¸": "ê°ì•¡", "ë‚´ë¦¬ê¸°": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ìž¥ì¹˜": "ëŒ€í•­ë ¥",
    "ëŒë ¤ë°›ê¸°": "ë³´ì¦ê¸ˆë°˜í™˜",

    # 3. ê¸°ê°„ ë° ì¢…ë£Œ/ê°±ì‹ 
    "ìž¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ì—°ìž¥": "ê³„ì•½ê°±ì‹ ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ê°±ì‹ ì²­êµ¬": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2ë…„ë”": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2í”ŒëŸ¬ìŠ¤2": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ",
    "ìžë™ì—°ìž¥": "ë¬µì‹œì ê°±ì‹ ", "ë¬µì‹œ": "ë¬µì‹œì ê°±ì‹ ", "ì—°ë½ì—†ìŒ": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€", "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ìž…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ìž„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ìž„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë§¤ë§¤": "ìž„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´",
    "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ìž", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ìž¥": "í•˜ìž", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ëž˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ",
    "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½",
    "ë‹´ë°°": "í¡ì—°ê¸ˆì§€íŠ¹ì•½",

    # 5. ê¶Œë¦¬/ëŒ€í•­ë ¥/í™•ì •ì¼ìž
    "í™•ì •ì¼ìž": "í™•ì •ì¼ìž", "ì „ìž…": "ì£¼ë¯¼ë“±ë¡", "ëŒ€í•­ë ¥": "ëŒ€í•­ë ¥",
    "ìš°ì„ ë³€ì œ": "ìš°ì„ ë³€ì œê¶Œ", "ìµœìš°ì„ ": "ìµœìš°ì„ ë³€ì œê¶Œ",
    "ê²½ë§¤": "ê²½ë§¤ì ˆì°¨", "ê³µë§¤": "ê³µë§¤ì ˆì°¨",
    "ë“±ê¸°": "ë“±ê¸°ë¶€ë“±ë³¸", "ë“±ë³¸": "ë“±ê¸°ë¶€ë“±ë³¸",
    "ê·¼ì €ë‹¹": "ê·¼ì €ë‹¹ê¶Œ", "ê°€ì••ë¥˜": "ê°€ì••ë¥˜", "ê°€ì²˜ë¶„": "ê°€ì²˜ë¶„",
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ê²½ë§¤ë„˜ì–´ê°": "ê¶Œë¦¬ë¦¬ìŠ¤í¬",

    # 6. ë¶„ìŸ í•´ê²°
    "ë‚´ìš©ì¦ëª…": "ë‚´ìš©ì¦ëª…", "ì†Œì†¡": "ì†Œì†¡", "ë¯¼ì‚¬": "ë¯¼ì‚¬ì†Œì†¡",
    "ì¡°ì •ìœ„": "ì£¼íƒìž„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •",
    "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ìž„ì°¨ê¶ŒìŠ¹ê³„", "ìžì‹ìƒì†": "ìž„ì°¨ê¶ŒìŠ¹ê³„",
    "íŠ¹ì•½": "íŠ¹ì•½ì‚¬í•­", "ë¶ˆê³µì •": "ê°•í–‰ê·œì •ìœ„ë°˜", "ë…ì†Œì¡°í•­": "ë¶ˆë¦¬í•œì•½ì •",
    "íš¨ë ¥ìžˆë‚˜": "ë¬´íš¨ì—¬ë¶€",
}


# --------------------------------------------------------------------------------------
# Prompts
# --------------------------------------------------------------------------------------
NORMALIZATION_PROMPT: str = """
ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìžìž…ë‹ˆë‹¤.
ì•„ëž˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©ížˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìžˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë‹¨ì–´ë¥¼ ë³€ê²½í•  ë•Œ ë¬¸ë§¥ì— ë§žê²Œ ì¡°ì‚¬(ì´/ê°€, ì„/ë¥¼ ë“±)ë‚˜ ì„œìˆ ì–´ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€, ë³„ë„ì˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
4. ë³€ê²½ ì „ ë‹¨ì–´ ë’¤ì— ë³€ê²½ëœ ë‹¨ì–´ë¥¼ ê´„í˜¸ë¡œ ë§ë¶™ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ex. "ì§‘ì£¼ì¸(ìž„ëŒ€ì¸)ì´..."

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ìž ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:
"""

SYSTEM_PROMPT: str = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ìž„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ìž…ë‹ˆë‹¤.
ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**:
   - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
   - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
   - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´,
     "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.

2. **ë‹µë³€ êµ¬ì¡°**:
   - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ (ê°€ëŠ¥/ë¶ˆê°€ëŠ¥/ìœ íš¨/ë¬´íš¨)ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
   - **ë²•ì  ê·¼ê±°**: "ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
   - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
   - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)

3. **ì£¼ì˜ì‚¬í•­**:
   - ì‚¬ìš©ìžì˜ ê³„ì•½ì„œ ë‚´ìš©ì´ ë²•ë ¹(ê°•í–‰ê·œì •)ì— ìœ„ë°˜ë˜ë©´ "íš¨ë ¥ì´ ì—†ë‹¤(ë¬´íš¨)"ê³  ëª…í™•ížˆ ê²½ê³ í•˜ì„¸ìš”.
   - ë²•ë¥ ì  ì¡°ì–¸ì¼ ë¿ì´ë¯€ë¡œ, ìµœì¢…ì ìœ¼ë¡œëŠ” ë³€í˜¸ì‚¬ ë“±ì˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ë°˜ë“œì‹œ ê³ ì§€í•˜ì„¸ìš”.

[ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
{context}
"""


# --------------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------------
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)  # type: ignore[arg-type]
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "â€¦"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¤‘ë³µ ì œê±° (chunk_id/id ìš°ì„ )."""
    seen: set[str] = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key: Optional[str] = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# --------------------------------------------------------------------------------------
# Tokenizers (for BM25)
# --------------------------------------------------------------------------------------
class Tokenizer(ABC):
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        raise NotImplementedError


class SimpleTokenizer(Tokenizer):
    """Regex ê¸°ë°˜ í† í¬ë‚˜ì´ì € (kiwi ë¯¸ì„¤ì¹˜ ì‹œ fallback)."""

    def __init__(self, min_length: int = 1):
        self.min_length = min_length
        self._pattern = re.compile(r"[ê°€-íž£a-zA-Z0-9]+")

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = self._pattern.findall(text.lower())
        return [t for t in tokens if len(t) >= self.min_length]


class KiwiTokenizer(Tokenizer):
    """Kiwi í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì €."""

    def __init__(self, pos_tags: Optional[Tuple[str, ...]] = None, min_length: int = 1):
        if not KIWI_AVAILABLE:
            raise ImportError("kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install kiwipiepy")
        self._kiwi = Kiwi()  # type: ignore[call-arg]
        self.pos_tags = pos_tags or ("NNG", "NNP", "VV", "VA", "SL", "SH")
        self.min_length = min_length

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens: List[str] = []
        for t in self._kiwi.tokenize(text):  # type: ignore[union-attr]
            if t.tag in self.pos_tags and len(t.form) >= self.min_length:
                tokens.append(t.form.lower())
        return tokens


# --------------------------------------------------------------------------------------
# BM25 (candidate-level) scoring
# --------------------------------------------------------------------------------------
def _bm25_lite_scores(
    query_tokens: List[str],
    docs_tokens: List[List[str]],
    *,
    k1: float = 1.5,
    b: float = 0.75,
) -> List[float]:
    """BM25Okapi-lite. Candidate-level only (N is small, e.g., 10~80)."""
    N = len(docs_tokens)
    if N == 0:
        return []
    if not query_tokens:
        return [0.0] * N

    doc_lens = [len(toks) for toks in docs_tokens]
    avgdl = (sum(doc_lens) / N) if N else 1.0
    avgdl = max(avgdl, 1e-9)

    df: Dict[str, int] = defaultdict(int)
    for toks in docs_tokens:
        for term in set(toks):
            df[term] += 1

    idf: Dict[str, float] = {}
    for term, dfi in df.items():
        idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

    qtf = Counter(query_tokens)
    scores: List[float] = []
    for toks, dl in zip(docs_tokens, doc_lens):
        tf = Counter(toks)
        score = 0.0
        norm = (1.0 - b) + b * (dl / avgdl)
        for term, qf in qtf.items():
            f = tf.get(term, 0)
            if f <= 0:
                continue
            denom = f + k1 * norm
            if denom <= 0:
                continue
            score += (idf.get(term, 0.0) * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))
        scores.append(float(score))
    return scores


def _compute_bm25_scores(
    query: str,
    docs: List[Document],
    *,
    tokenizer: Tokenizer,
    algorithm: str,
    k1: float,
    b: float,
    max_doc_chars: int,
) -> List[float]:
    """Returns BM25 scores aligned with docs (higher is better)."""
    if not docs:
        return []

    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(d.page_content or "", max_doc_chars)) for d in docs]

    if BM25_AVAILABLE:
        BM25Class = BM25Plus if algorithm == "plus" else BM25Okapi
        if BM25Class is None:
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)
        try:
            bm25 = BM25Class(docs_tokens, k1=k1, b=b)  # type: ignore[misc]
            scores = bm25.get_scores(query_tokens)
            return [float(x) for x in list(scores)]
        except Exception as e:
            logger.warning(f"âš ï¸ rank_bm25 ì‹¤íŒ¨ â†’ lite BM25ë¡œ í´ë°±: {e}")
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)

    return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)



# --------------------------------------------------------------------------------------
# Global BM25 index (optional, true sparse retrieval)
# --------------------------------------------------------------------------------------
class BM25InvertedIndex:
    """Lightweight BM25 inverted index for *global* sparse retrieval.

    - Build once with a corpus (per source: law/rule/case) using build().
    - Search returns top-k Documents with BM25 scores.
    - Uses the same doc identity logic as _dedupe_docs (metadata key_fields first, fallback to content hash).

    Notes:
      * This is NOT required for the default candidate-level BM25 in _dense_sparse_fuse().
      * For large corpora, memory usage grows with the number of unique terms.
    """

    def __init__(
        self,
        *,
        tokenizer: Tokenizer,
        key_fields: Sequence[str] = ("chunk_id", "id"),
        k1: float = 1.5,
        b: float = 0.75,
        max_doc_chars: int = 4000,
    ) -> None:
        self.tokenizer = tokenizer
        self.key_fields = tuple(key_fields)
        self.k1 = float(k1)
        self.b = float(b)
        self.max_doc_chars = int(max_doc_chars)

        self._docs: List[Document] = []
        self._doc_lens: List[int] = []
        self._avgdl: float = 0.0

        # postings[term] = list of (doc_idx, tf)
        self._postings: Dict[str, List[Tuple[int, int]]] = defaultdict(list)
        self._idf: Dict[str, float] = {}
        self._built: bool = False

    def build(self, docs: Sequence[Document]) -> None:
        deduped = _dedupe_docs(docs, self.key_fields)
        self._docs = list(deduped)
        self._postings.clear()
        self._idf.clear()
        self._doc_lens = []

        df: Dict[str, int] = defaultdict(int)

        for idx, d in enumerate(self._docs):
            text = _truncate(d.page_content or "", self.max_doc_chars)
            toks = self.tokenizer.tokenize(text)
            dl = len(toks)
            self._doc_lens.append(dl)

            tf = Counter(toks)
            for term, f in tf.items():
                if not term:
                    continue
                self._postings[term].append((idx, int(f)))
            for term in tf.keys():
                df[term] += 1

        N = len(self._docs)
        self._avgdl = (sum(self._doc_lens) / N) if N else 0.0

        for term, dfi in df.items():
            self._idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

        self._built = True

    def is_built(self) -> bool:
        return self._built and bool(self._docs)

    def search(self, query: str, *, top_k: int = 20) -> List[Tuple[Document, float]]:
        if not self.is_built():
            return []
        q_tokens = self.tokenizer.tokenize(query)
        if not q_tokens:
            return []

        qtf = Counter(q_tokens)
        scores: Dict[int, float] = defaultdict(float)

        avgdl = self._avgdl or 1.0
        k1 = self.k1
        b = self.b

        for term, qf in qtf.items():
            postings = self._postings.get(term)
            if not postings:
                continue
            idf = self._idf.get(term, 0.0)
            if idf == 0.0:
                continue
            for doc_idx, f in postings:
                dl = self._doc_lens[doc_idx] or 0
                norm = (1.0 - b) + b * (dl / avgdl)
                denom = f + k1 * norm
                if denom <= 0:
                    continue
                scores[doc_idx] += (idf * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))

        if not scores:
            return []

        top = heapq.nlargest(int(top_k), scores.items(), key=lambda x: x[1])
        return [(self._docs[i], float(s)) for (i, s) in top]

# --------------------------------------------------------------------------------------
# Hybrid fusion (rank-based default)
# --------------------------------------------------------------------------------------
def _rank_fusion(
    dense_ranks: List[int],
    sparse_ranks: List[int],
    *,
    mode: str = "rrf",          # "rrf" | "rank_sum" | "weighted"
    w_dense: float = 0.6,
    w_sparse: float = 0.4,
    rrf_k: int = 60,
) -> List[float]:
    """Return fused scores aligned with docs (higher is better)."""
    n = len(dense_ranks)
    if n == 0:
        return []

    mode = (mode or "rrf").lower()
    if mode == "rrf":
        k = max(1, int(rrf_k))
        return [(w_dense / (k + dense_ranks[i])) + (w_sparse / (k + sparse_ranks[i])) for i in range(n)]

    if mode == "rank_sum":
        if n == 1:
            return [w_dense + w_sparse]

        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)

        return [(w_dense * to_unit(dense_ranks[i])) + (w_sparse * to_unit(sparse_ranks[i])) for i in range(n)]

    # mode == "weighted": min-max normalize (dense=1/rank, sparse=1/rank) then weighted sum
    dense_scores = [1.0 / max(1, r) for r in dense_ranks]
    sparse_scores = [1.0 / max(1, r) for r in sparse_ranks]

    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    d = minmax(dense_scores)
    s = minmax(sparse_scores)
    return [(w_dense * d[i]) + (w_sparse * s[i]) for i in range(n)]


# --------------------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------------------
@dataclass
class RAGConfig:
    # -------- LLMs --------
    normalize_model: str = "solar-pro2"  # Upstage chat model
    generation_model: str = "gpt-4o-mini"  # OpenAI model
    temperature: float = 0.1
    normalize_temperature: float = 0.0

    # -------- Embeddings --------
    embedding_backend: str = "upstage"  # "upstage" | "auto" (auto keeps option for other backends if you inject)
    embedding_model: str = "solar-embedding-1-large-passage"

    # -------- Retrieval sizes --------
    k_law: int = 5
    k_rule: int = 5
    k_case: int = 3
    search_multiplier: int = 2

    # -------- Candidate-level BM25 --------
    enable_bm25: bool = True
    # ============ Sparse retrieval mode (true sparse BM25) ============
    # - candidate: BM25 reorders only dense candidates (fast, no corpus preload)
    # - global: BM25 searches a prebuilt BM25 index over the full corpus you provide via build_global_bm25()
    # - auto: use global if available, else candidate
    sparse_mode: str = "auto"  # "auto" | "candidate" | "global"
    sparse_k_law: Optional[int] = None
    sparse_k_rule: Optional[int] = None
    sparse_k_case: Optional[int] = None

    bm25_algorithm: str = "okapi"  # "okapi" | "plus"
    bm25_k1: float = 1.5
    bm25_b: float = 0.75
    bm25_use_kiwi: bool = True
    bm25_max_doc_chars: int = 4000

    # -------- Fusion --------
    hybrid_fusion: str = "rrf"  # "rrf" | "rank_sum" | "weighted"
    hybrid_dense_weight: float = 0.6
    hybrid_sparse_weight: float = 0.4
    rrf_k: int = 60

    # -------- Rerank (optional) --------
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80
    rerank_doc_max_chars: int = 2000

    # -------- 2-stage case expansion --------
    case_candidate_k: int = 40
    case_expand_top_n: Optional[int] = None  # None => k_case
    case_context_top_k: int = 50

    # -------- Deduping --------
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")

    def __post_init__(self) -> None:
        if not (0 <= self.temperature <= 2):
            raise ValueError("temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.normalize_temperature <= 2):
            raise ValueError("normalize_temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.search_multiplier < 1:
            raise ValueError("search_multiplierëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.case_candidate_k < 1 or self.case_context_top_k < 1:
            raise ValueError("case_* ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        if self.enable_bm25:
            if self.bm25_k1 <= 0:
                raise ValueError("bm25_k1ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
            if not (0 <= self.bm25_b <= 1):
                raise ValueError("bm25_bëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            if self.bm25_algorithm not in ("okapi", "plus"):
                raise ValueError('bm25_algorithmì€ "okapi" ë˜ëŠ” "plus" ì´ì–´ì•¼ í•©ë‹ˆë‹¤.')

        if self.hybrid_fusion not in ("rrf", "rank_sum", "weighted"):
            raise ValueError('hybrid_fusionì€ "rrf" | "rank_sum" | "weighted" ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.')
        if self.rrf_k < 1:
            raise ValueError("rrf_këŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_dense_weight < 0 or self.hybrid_sparse_weight < 0:
            raise ValueError("hybrid_*_weightëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_dense_weight == 0 and self.hybrid_sparse_weight == 0:
            raise ValueError("hybrid_dense_weightì™€ hybrid_sparse_weightê°€ ëª¨ë‘ 0ì¼ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.")


# --------------------------------------------------------------------------------------
# Pipeline
# --------------------------------------------------------------------------------------
class RAGPipeline:
    """Unified Hybrid RAG pipeline (no web framework integration)."""

    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        upstage_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
        embedding: Optional[object] = None,
        normalize_llm: Optional[object] = None,
        generation_llm: Optional[object] = None,
        cohere_client: Optional[object] = None,
        tokenizer: Optional[Tokenizer] = None,
    ) -> None:
        self.config = config or RAGConfig()

        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._upstage_api_key = upstage_api_key or os.getenv("UPSTAGE_API_KEY")
        self._openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")

        if not self._pc_api_key:
            raise ValueError("PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. pc_api_key ì¸ìž ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")

        # ---- Embedding (dense) ----
        if embedding is not None:
            self._embedding = embedding
        else:
            backend = (self.config.embedding_backend or "auto").lower()
            if backend in ("auto", "upstage"):
                if not UPSTAGE_AVAILABLE:
                    raise ImportError("langchain_upstageê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install langchain-upstage")
                if not self._upstage_api_key:
                    raise ValueError("UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤ (UpstageEmbeddings).")
                os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
                self._embedding = UpstageEmbeddings(model=self.config.embedding_model)  # type: ignore[call-arg]
            else:
                raise ValueError(
                    "í˜„ìž¬ unified ëª¨ë“ˆì€ ê¸°ë³¸ì ìœ¼ë¡œ Upstage(SOLAR) embeddingì„ ì‚¬ìš©í•©ë‹ˆë‹¤. "
                    "ë‹¤ë¥¸ embeddingì„ ì“°ë ¤ë©´ embedding ê°ì²´ë¥¼ ì§ì ‘ ì£¼ìž…í•˜ì„¸ìš”."
                )

        # ---- Pinecone vector stores ----
        logger.info("ðŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        self._law_store = PineconeVectorStore(
            index_name=INDEX_NAMES["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=INDEX_NAMES["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=INDEX_NAMES["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        logger.info("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

        # ---- LLMs ----
        # normalize: Upstage solar-pro2
        if normalize_llm is not None:
            self._normalize_llm = normalize_llm
        else:
            if not UPSTAGE_AVAILABLE or ChatUpstage is None:
                raise ImportError("normalize_queryì— Upstage chatì„ ì“°ë ¤ë©´ langchain_upstageê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            if not self._upstage_api_key:
                raise ValueError("normalize_queryì— UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
            self._normalize_llm = ChatUpstage(
                model=self.config.normalize_model,
                temperature=self.config.normalize_temperature,
            )

        # generation: OpenAI gpt-4o-mini
        if generation_llm is not None:
            self._generation_llm = generation_llm
        else:
            if not OPENAI_AVAILABLE or ChatOpenAI is None:
                raise ImportError("generate_answerì— OpenAI chatì„ ì“°ë ¤ë©´ langchain_openaiê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            if not self._openai_api_key:
                raise ValueError("generate_answerì— OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            os.environ.setdefault("OPENAI_API_KEY", self._openai_api_key)
            self._generation_llm = ChatOpenAI(
                model=self.config.generation_model,
                temperature=self.config.temperature,
            )

        # ---- Tokenizer (for BM25) ----
        if tokenizer is not None:
            self._tokenizer = tokenizer
        else:
            if self.config.bm25_use_kiwi and KIWI_AVAILABLE:
                logger.info("âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)")
                self._tokenizer = KiwiTokenizer()
            else:
                logger.info("â„¹ï¸ SimpleTokenizer ì‚¬ìš© (BM25)")
                self._tokenizer = SimpleTokenizer()

        # ---- Cohere rerank client (optional) ----
        self._cohere_client = None
        if self.config.enable_rerank:
            if not COHERE_AVAILABLE:
                logger.warning("âš ï¸ cohere íŒ¨í‚¤ì§€ê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            elif not self._cohere_api_key:
                logger.warning("âš ï¸ COHERE_API_KEYê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            else:
                self._cohere_client = cohere_client or cohere.Client(self._cohere_api_key)  # type: ignore[attr-defined]

        # ---- Global BM25 indices (optional, for true sparse retrieval) ----
        self._global_bm25: Dict[str, BM25InvertedIndex] = {}

    # ----------------------------
    # Stores
    # ----------------------------
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store

    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store

    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store

    # ----------------------------
    # Query normalization
    # ----------------------------
    def normalize_query(self, user_query: str) -> str:
        """Upstage SOLAR Pro2ë¡œ ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”."""
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()

        try:
            normalized = chain.invoke({"dictionary": KEYWORD_DICT, "question": user_query})
            out = str(normalized).strip()
            return out or user_query
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query

    # ----------------------------
    # Case expansion
    # ----------------------------
    def get_full_case_context(self, case_no: str) -> str:
        """íŠ¹ì • ì‚¬ê±´ë²ˆí˜¸(case_no)ì˜ íŒë¡€ ì „ë¬¸(ì²­í¬ë“¤ì„ ì—°ê²°)ì„ ê°€ì ¸ì˜´."""
        try:
            results = self.case_store.similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰",
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            sorted_docs = sorted(results, key=lambda x: str((x.metadata or {}).get("chunk_id", "")))
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
            return ""

    # ----------------------------
    # Internal helpers
    # ----------------------------
    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs

    def build_global_bm25(
        self,
        *,
        law_docs: Optional[Sequence[Document]] = None,
        rule_docs: Optional[Sequence[Document]] = None,
        case_docs: Optional[Sequence[Document]] = None,
    ) -> None:
        """(Optional) Build *global* BM25 indices for true sparse retrieval.

        Provide the same corpus you indexed into Pinecone (or a superset). Metadata should contain
        stable identifiers (e.g., chunk_id) to enable deduplication/merge with dense results.
        """
        cfg = self.config

        def _build(name: str, docs: Optional[Sequence[Document]]) -> None:
            if not docs:
                return
            idx = BM25InvertedIndex(
                tokenizer=self._tokenizer,
                key_fields=cfg.dedupe_key_fields,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_max_doc_chars,
            )
            idx.build(docs)
            if idx.is_built():
                self._global_bm25[name] = idx
                logger.info(f"âœ… Global BM25 index built for '{name}' (docs={len(docs)})")

        _build("law", law_docs)
        _build("rule", rule_docs)
        _build("case", case_docs)

    def _hybrid_fuse_per_source(self, source: str, query: str, dense_docs: List[Document]) -> List[Document]:
        """Choose hybrid strategy per source (candidate-level vs global BM25)."""
        cfg = self.config
        mode = (cfg.sparse_mode or "auto").lower()

        use_global = False
        if mode == "global":
            use_global = True
        elif mode == "auto":
            use_global = source in self._global_bm25 and self._global_bm25[source].is_built()

        if not use_global:
            return self._dense_sparse_fuse(query, dense_docs)

        if source == "law":
            sk = cfg.sparse_k_law or (cfg.k_law * max(1, cfg.search_multiplier))
        elif source == "rule":
            sk = cfg.sparse_k_rule or (cfg.k_rule * max(1, cfg.search_multiplier))
        else:
            sk = cfg.sparse_k_case or max(cfg.case_candidate_k, cfg.k_case * max(1, cfg.search_multiplier))

        sparse_pairs = self._global_bm25[source].search(query, top_k=int(sk))
        sparse_docs: List[Document] = []
        for rank, (d, score) in enumerate(sparse_pairs, start=1):
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__bm25_score"] = float(score)
            d.metadata["__bm25_rank"] = int(rank)
            sparse_docs.append(d)

        dense_docs = self._attach_source(dense_docs, source)
        sparse_docs = self._attach_source(sparse_docs, source)

        merged = _dedupe_docs(list(dense_docs) + list(sparse_docs), cfg.dedupe_key_fields)
        if len(merged) <= 1:
            return merged

        def _key(d: Document) -> str:
            md = d.metadata or {}
            for f in cfg.dedupe_key_fields:
                v = md.get(f)
                if v:
                    return f"{f}:{v}"
            return f"content:{hash(d.page_content)}"

        dense_rank_map: Dict[str, int] = {}
        sparse_rank_map: Dict[str, int] = {}

        for i, d in enumerate(dense_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__dense_rank", i))
            dense_rank_map[k] = min(dense_rank_map.get(k, r), r)

        for i, d in enumerate(sparse_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__bm25_rank", i))
            sparse_rank_map[k] = min(sparse_rank_map.get(k, r), r)

        max_dense = max(dense_rank_map.values()) if dense_rank_map else 1000
        max_sparse = max(sparse_rank_map.values()) if sparse_rank_map else 1000
        fill_dense = max_dense + 1000
        fill_sparse = max_sparse + 1000

        dense_ranks: List[int] = []
        sparse_ranks: List[int] = []
        for d in merged:
            k = _key(d)
            dense_ranks.append(int(dense_rank_map.get(k, fill_dense)))
            sparse_ranks.append(int(sparse_rank_map.get(k, fill_sparse)))

        fused = _rank_fusion(
            dense_ranks,
            sparse_ranks,
            mode=cfg.hybrid_fusion,
            w_dense=cfg.hybrid_dense_weight,
            w_sparse=cfg.hybrid_sparse_weight,
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(len(merged)), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, idx in enumerate(order, start=1):
            d = merged[idx]
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__hybrid_score"] = float(fused[idx])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _search_dense_candidates(self, store: PineconeVectorStore, query: str, k: int) -> List[Document]:
        """Dense retrieval via PineconeVectorStore."""
        try:
            pairs = store.similarity_search_with_score(query, k=k)  # type: ignore[attr-defined]
            docs: List[Document] = []
            for rank, (doc, score) in enumerate(pairs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_score"] = float(score)
                doc.metadata["__dense_rank"] = int(rank)
                docs.append(doc)
            return docs
        except Exception:
            docs = store.similarity_search(query, k=k)
            for rank, doc in enumerate(docs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_rank"] = int(rank)
            return docs

    def _dense_sparse_fuse(self, query: str, docs: List[Document]) -> List[Document]:
        """Dense candidatesë¥¼ BM25ë¡œ ìž¬ì •ë ¬ í›„, rank fusionìœ¼ë¡œ ê²°í•©."""
        cfg = self.config
        if not cfg.enable_bm25:
            return docs

        docs = _dedupe_docs(docs, cfg.dedupe_key_fields)
        if len(docs) <= 1:
            return docs

        dense_ranks: List[int] = []
        for i, d in enumerate(docs, start=1):
            if d.metadata is None:
                d.metadata = {}
            dense_ranks.append(int(d.metadata.get("__dense_rank", i)))

        bm25_scores = _compute_bm25_scores(
            query,
            docs,
            tokenizer=self._tokenizer,
            algorithm=cfg.bm25_algorithm,
            k1=cfg.bm25_k1,
            b=cfg.bm25_b,
            max_doc_chars=cfg.bm25_max_doc_chars,
        )
        order_sparse = sorted(range(len(docs)), key=lambda i: bm25_scores[i], reverse=True)
        sparse_ranks = [0] * len(docs)
        for r, idx in enumerate(order_sparse, start=1):
            sparse_ranks[idx] = r

        for i, d in enumerate(docs):
            d.metadata["__bm25_score"] = float(bm25_scores[i])
            d.metadata["__bm25_rank"] = int(sparse_ranks[i])

        fused = _rank_fusion(
            dense_ranks,
            sparse_ranks,
            mode=cfg.hybrid_fusion,
            w_dense=cfg.hybrid_dense_weight,
            w_sparse=cfg.hybrid_sparse_weight,
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(len(docs)), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, i in enumerate(order, start=1):
            d = docs[i]
            d.metadata["__hybrid_score"] = float(fused[i])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _rerank(self, query: str, docs: List[Document]) -> Optional[List[Tuple[int, float]]]:
        """Cohere rerank ì‹¤í–‰. ì‹¤íŒ¨/ë¹„í™œì„± ì‹œ None."""
        if not self._cohere_client:
            return None

        cfg = self.config
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]
        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            return [(r.index, float(r.relevance_score)) for r in rerank_results.results]
        except Exception as e:
            logger.warning(f"âš ï¸ Rerank ì‹¤íŒ¨ (skip): {e}")
            return None

    def _cap_for_rerank(self, law: List[Document], rule: List[Document], case: List[Document]) -> List[Document]:
        """rerank ìž…ë ¥ ë¬¸ì„œ ìˆ˜ ì œí•œ: law/rule ìš°ì„ , caseëŠ” ë‚¨ëŠ” ìŠ¬ë¡¯ë§Œ."""
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)

        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[: cfg.rerank_max_documents]
        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]

    # ----------------------------
    # Retrieval: triple index + hybrid + optional rerank + 2-stage case expansion
    # ----------------------------
    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        cfg = self.config
        mult = cfg.search_multiplier

        logger.info(f"ðŸ” [Hybrid Retrieval] query='{query}'")

        docs_law = self._attach_source(
            self._search_dense_candidates(self.law_store, query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self._search_dense_candidates(self.rule_store, query, k=cfg.k_rule * mult),
            "rule",
        )
        docs_case_chunks = self._attach_source(
            self._search_dense_candidates(self.case_store, query, k=cfg.case_candidate_k),
            "case",
        )

        # candidate-level BM25 fusion per index
        docs_law = self._hybrid_fuse_per_source("law", query, docs_law)
        docs_rule = self._hybrid_fuse_per_source("rule", query, docs_rule)
        docs_case_chunks = self._hybrid_fuse_per_source("case", query, docs_case_chunks)

        combined_for_rerank = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)

        ranked = self._rerank(query, combined_for_rerank) if cfg.enable_rerank else None
        if ranked:
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]
            selected_docs = [combined_for_rerank[i] for (i, _s) in filtered]
            logger.info(f"ðŸ“Œ Rerank selected={len(selected_docs)} (threshold={cfg.rerank_threshold})")
        else:
            selected_docs = combined_for_rerank

        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)

        law_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "rule"]
        case_ranked_chunks = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "case"]

        final_law = law_ranked[: cfg.k_law]
        final_rule = rule_ranked[: cfg.k_rule]

        top_n = cfg.case_expand_top_n if cfg.case_expand_top_n is not None else cfg.k_case
        seen_case_no: set[str] = set()
        chosen_case_docs: List[Document] = []
        for d in case_ranked_chunks:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no or str(case_no) in seen_case_no:
                continue
            seen_case_no.add(str(case_no))
            chosen_case_docs.append(d)
            if len(chosen_case_docs) >= top_n:
                break

        expanded_cases: List[Document] = []
        for d in chosen_case_docs:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                expanded_cases.append(d)
                continue

            title = (d.metadata or {}).get("title") or (d.metadata or {}).get("case_name") or str(case_no)
            md = dict(d.metadata or {})
            md["__expanded"] = True
            expanded_cases.append(
                Document(
                    page_content=f"[íŒë¡€ ì „ë¬¸: {title}]\n{full_text}",
                    metadata=md,
                )
            )

        final_case = expanded_cases[: cfg.k_case]

        final_docs = final_law + final_rule + final_case
        final_docs = sorted(final_docs, key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99))
        return final_docs

    # ----------------------------
    # Context formatting
    # ----------------------------
    @staticmethod
    def format_context_with_hierarchy(docs: List[Document]) -> str:
        section_1_law: List[str] = []
        section_2_rule: List[str] = []
        section_3_case: List[str] = []

        for doc in docs:
            md = doc.metadata or {}
            p = _safe_int(md.get("priority", 99), 99)
            src = md.get("src_title", md.get("__source_index", "ìžë£Œ"))
            title = md.get("title", "")
            content = doc.page_content or ""

            entry = f"[{src}] {title}\n{content}".strip()

            if p in (1, 2, 4, 5):
                section_1_law.append(entry)
            elif p in (3, 6, 7, 8, 11):
                section_2_rule.append(entry)
            else:
                section_3_case.append(entry)

        parts: List[str] = []
        if section_1_law:
            parts.append("## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" + "\n\n".join(section_1_law))
        if section_2_rule:
            parts.append("## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" + "\n\n".join(section_2_rule))
        if section_3_case:
            parts.append("## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" + "\n\n".join(section_3_case))

        return "\n\n".join(parts).strip()

    # ----------------------------
    # Answer generation
    # ----------------------------
    def generate_answer(self, user_input: str, *, skip_normalization: bool = False) -> str:
        normalized_query = user_input if skip_normalization else self.normalize_query(user_input)
        if not skip_normalization:
            logger.info(f"ðŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")

        docs = self.triple_hybrid_retrieval(normalized_query)
        if not docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        context = self.format_context_with_hierarchy(docs)

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", SYSTEM_PROMPT),
                ("human", "{question}"),
            ]
        )
        chain = prompt | self._generation_llm | StrOutputParser()

        logger.info("ðŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            return str(chain.invoke({"context": context, "question": normalized_query})).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


def create_pipeline(**kwargs: Any) -> RAGPipeline:
    """Convenience helper."""
    return RAGPipeline(**kwargs)


__all__ = [
    "RAGConfig",
    "RAGPipeline",
    "create_pipeline",
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
]
