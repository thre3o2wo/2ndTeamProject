{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ê·œì¹™ íŒŒì‹± í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ë¶\n",
    "## ì£¼íƒì„ëŒ€ì°¨ RAG - ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™/ëŒ€ë²•ì›ê·œì¹™ íŒŒì‹±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. í•„ìš” íŒ¨í‚¤ì§€ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ì¹´í…Œê³ ë¦¬ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¹´í…Œê³ ë¦¬ ì •ì˜ ì™„ë£Œ\n",
      "ì´ 9ê°œ ì¹´í…Œê³ ë¦¬\n"
     ]
    }
   ],
   "source": [
    "def get_law_category():\n",
    "    \"\"\"ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì¹´í…Œê³ ë¦¬-í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì‚¬ì „\"\"\"\n",
    "    return {\n",
    "        'ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥': {\n",
    "            'ë³´ì¦ê¸ˆ': 3, 'ëŒ€í•­ë ¥': 3, 'ìš°ì„ ë³€ì œê¶Œ': 3, 'ìµœìš°ì„ ë³€ì œ': 3,\n",
    "            'ë³´ì¦ê¸ˆë°˜í™˜': 3, 'ì „ì„¸ê¸ˆ': 2, 'ì†Œì•¡ì„ì°¨ì¸': 2, 'í™•ì •ì¼ì': 2,\n",
    "            'ì „ì…ì‹ ê³ ': 2, 'ì ìœ ': 2, 'ì„ì°¨ê¶Œë“±ê¸°': 3, 'ë°°ë‹¹': 2,\n",
    "            'ë°˜í™˜ë³´ì¦': 2, 'HUG': 1, 'ë³´ì¦ë³´í—˜': 2\n",
    "        },\n",
    "        'ê³„ì•½ê°±ì‹ ': {\n",
    "            'ê³„ì•½ê°±ì‹ ': 3, 'ê°±ì‹ ìš”êµ¬': 3, 'ê°±ì‹ ê±°ì ˆ': 3, 'ë¬µì‹œì ê°±ì‹ ': 3,\n",
    "            'ê³„ì•½ì—°ì¥': 2, 'ì‹¤ê±°ì£¼': 3, 'ì¡´ì†ê¸°ê°„': 2, '2ë…„': 1,\n",
    "            'ì¬ê³„ì•½': 2, 'ê°±ì‹ ì²­êµ¬': 2, 'ê±°ì ˆì‚¬ìœ ': 2, 'ë³µë¹„': 1\n",
    "        },\n",
    "        'ê³„ì•½í•´ì§€': {\n",
    "            'ê³„ì•½í•´ì§€': 3, 'í•´ì§€í†µê³ ': 3, 'ì¤‘ë„í•´ì§€': 3, 'ê¸°ê°„ë§Œë£Œ': 2,\n",
    "            'ê³„ì•½ì¢…ë£Œ': 2, 'í‡´ê±°': 2, 'ì´ì‚¬': 1, 'ëª…ë„': 2,\n",
    "            'í•©ì˜í•´ì§€': 2, 'í†µì§€': 1, '3ê°œì›”': 2, 'ì¦‰ì‹œí•´ì§€': 3\n",
    "        },\n",
    "        'ì„ëŒ€ë£Œ_ì¦ê°': {\n",
    "            'ì°¨ì„': 3, 'ì›”ì„¸': 2, 'ì¦ì•¡': 3, 'ê°ì•¡': 2, 'ì¸ìƒ': 2,\n",
    "            '5í¼ì„¼íŠ¸': 3, '5%': 3, '20ë¶„ì˜ 1': 2, 'ìƒí•œ': 2,\n",
    "            'ì „í™˜ìœ¨': 2, 'ì›”ì°¨ì„': 2, 'ê²½ì œì‚¬ì •': 1, 'ë¶€ë‹´': 1\n",
    "        },\n",
    "        'ìˆ˜ì„ _ì›ìƒíšŒë³µ': {\n",
    "            'ìˆ˜ì„ ': 3, 'ìˆ˜ë¦¬': 3, 'ì›ìƒíšŒë³µ': 3, 'íŒŒì†': 2, 'í›¼ì†': 2,\n",
    "            'ëˆ„ìˆ˜': 2, 'ê³°íŒ¡ì´': 2, 'ë³´ì¼ëŸ¬': 2, 'í•„ìš”ë¹„': 3, 'ìœ ìµë¹„': 3,\n",
    "            'ë¹„ìš©ìƒí™˜': 2, 'ë³´ì¡´í–‰ìœ„': 2, 'ê´€ë¦¬ë¹„': 1\n",
    "        },\n",
    "        'ê¶Œë¦¬_ë¦¬ìŠ¤í¬': {\n",
    "            'ì „ì„¸ì‚¬ê¸°': 3, 'ê¹¡í†µì „ì„¸': 3, 'ì‹ íƒ': 3, 'ê·¼ì €ë‹¹': 3, 'ì €ë‹¹ê¶Œ': 3,\n",
    "            'ì„ ìˆœìœ„': 3, 'ê°€ì••ë¥˜': 3, 'ì••ë¥˜': 3, 'êµ­ì„¸': 2, 'ì§€ë°©ì„¸': 2,\n",
    "            'ì²´ë‚©': 3, 'ë‚©ì„¸ì¦ëª…': 2, 'ìœ„ë°˜ê±´ì¶•ë¬¼': 2, 'ë¶ˆë²•ê±´ì¶•ë¬¼': 2,\n",
    "            'íŠ¹ì•½': 2, 'ë…ì†Œì¡°í•­': 2, 'ê°•í–‰ê·œì •': 3, 'íš¨ë ¥ì´ ì—†ë‹¤': 3,\n",
    "            'ë¬´íš¨': 3, 'ë¶ˆë¦¬í•œ ì•½ì •': 3, 'í¸ë©´ì  ê°•í–‰ê·œì •': 2\n",
    "        },\n",
    "        'í–‰ì •ì ˆì°¨': {\n",
    "            'í™•ì •ì¼ìë¶€ì—¬': 3, 'ë™ì£¼ë¯¼ì„¼í„°': 2, 'ë“±ê¸°ì†Œ': 2, 'ì¸í„°ë„·ë“±ê¸°ì†Œ': 3,\n",
    "            'ìˆ˜ìˆ˜ë£Œ': 1, 'ì—´ëŒ': 2, 'ì œê³µìš”ì²­': 2, 'ì´í•´ê´€ê³„ì¸': 2,\n",
    "            'ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ': 2, 'ì „ìê³„ì•½': 2, 'ì‹ ë¶„ì¦': 1\n",
    "        },\n",
    "        'ë¶„ìŸí•´ê²°': {\n",
    "            'ë¶„ìŸì¡°ì •': 3, 'ì¡°ì •ìœ„ì›íšŒ': 2, 'ì§€ê¸‰ëª…ë ¹': 3, 'ì†Œì†¡': 3,\n",
    "            'íŒê²°': 2, 'ì§‘í–‰ê¶Œì›': 2, 'ê²½ë§¤': 3, 'ê³µë§¤': 2,\n",
    "            'ë‚´ìš©ì¦ëª…': 2, 'ì†í•´ë°°ìƒ': 3, 'ì§€ì—°ì´ì': 2\n",
    "        },\n",
    "        'ì„ì°¨ê¶Œ_ìŠ¹ê³„': {\n",
    "            'ì„ì°¨ê¶ŒìŠ¹ê³„': 3, 'ìŠ¹ê³„': 3, 'ì‚¬ë§': 3, 'ìƒì†': 3, 'ìƒì†ì¸': 3,\n",
    "            'ì‚¬ì‹¤í˜¼': 3, 'ë°°ìš°ì': 2, 'ê°€ì •ê³µë™ìƒí™œ': 3, '2ì´Œ': 2,\n",
    "            'ê³µë™ìƒì†': 2, 'ë°˜í™˜ì²­êµ¬ê¶Œ': 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"âœ… ì¹´í…Œê³ ë¦¬ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"ì´ {len(get_law_category())}ê°œ ì¹´í…Œê³ ë¦¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ê²°ê³¼: ['ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥']\n"
     ]
    }
   ],
   "source": [
    "def categorize_content(content, top_k=None):\n",
    "    \"\"\"ë‚´ìš© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ - ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ë°˜í™˜\"\"\"\n",
    "    category_keywords = get_law_category()\n",
    "    category_scores = {}\n",
    "    \n",
    "    for category, weighted_keywords in category_keywords.items():\n",
    "        score = 0\n",
    "        for keyword, weight in weighted_keywords.items():\n",
    "            count = content.count(keyword)\n",
    "            score += count * weight\n",
    "        if score > 0:\n",
    "            category_scores[category] = score\n",
    "    \n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    all_categories = [category[0] for category in sorted_categories]\n",
    "    \n",
    "    if not all_categories:\n",
    "        all_categories = [\"ê¸°íƒ€\"]\n",
    "    \n",
    "    if top_k is not None:\n",
    "        return all_categories[:top_k]\n",
    "    else:\n",
    "        return all_categories\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_text = \"ë³´ì¦ê¸ˆ ë°˜í™˜ê³¼ ìš°ì„ ë³€ì œê¶Œì— ëŒ€í•œ ë‚´ìš©ì…ë‹ˆë‹¤. í™•ì •ì¼ìë¥¼ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "print(\"í…ŒìŠ¤íŠ¸ ê²°ê³¼:\", categorize_content(test_text, top_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\n",
      "Raw ë””ë ‰í† ë¦¬: c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\n",
      "CSV ì €ì¥ ê²½ë¡œ: c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\processed\\csv\\rule.csv\n"
     ]
    }
   ],
   "source": [
    "# í˜„ì¬ ë…¸íŠ¸ë¶ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •\n",
    "BASE_DIR = Path.cwd().parents[1]  # ë…¸íŠ¸ë¶ì´ src/ í•˜ìœ„ì— ìˆë‹¤ê³  ê°€ì •\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "CSV_DIR = PROCESSED_DIR / \"csv\"\n",
    "LOG_DIR = PROCESSED_DIR / \"log\"\n",
    "\n",
    "RULE_RAW_DIR = RAW_DIR / \"rule\"\n",
    "RULE_CSV_PATH = CSV_DIR / \"rule.csv\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RULE_RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"Raw ë””ë ‰í† ë¦¬: {RULE_RAW_DIR}\")\n",
    "print(f\"CSV ì €ì¥ ê²½ë¡œ: {RULE_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. íŒŒì‹± í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì‹± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_PATTERN = re.compile(r\"(ì œ\\s*\\d+ì¡°(?:ì˜\\d+)?)\")\n",
    "\n",
    "def extract_article_title(text):\n",
    "    \"\"\"ì¡°ë¬¸ì—ì„œ ì œëª© ì¶”ì¶œ (ê´„í˜¸ ì•ˆì˜ ë‚´ìš©)\"\"\"\n",
    "    match = re.search(r'\\(([^)]+)\\)', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def parse_rule_docx(file_path, src_title, priority, effective_date):\n",
    "    \"\"\"ê·œì¹™ DOCX íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í‚¹\"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
    "    full_text = \"\\n\".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "    \n",
    "    # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬\n",
    "    if not full_text or len(full_text) < 10:\n",
    "        print(f\"âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    \n",
    "    rows = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if not chunk.strip() or len(chunk.strip()) < 10:  # ë¹ˆ ì²­í¬ ì œì™¸\n",
    "            continue\n",
    "            \n",
    "        # ì¡°ë¬¸ ë²ˆí˜¸ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë°œê²¬ëœ ê²ƒ)\n",
    "        article_match = ARTICLE_PATTERN.search(chunk)\n",
    "        article = article_match.group(1).replace(\" \", \"\") if article_match else \"\"\n",
    "        \n",
    "        categories = categorize_content(chunk, top_k=3)\n",
    "        \n",
    "        rows.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"chunk_id\": f\"chunk_{idx:03d}\",\n",
    "            \"text\": chunk.strip(),\n",
    "            \"src_title\": src_title,\n",
    "            \"article\": article,\n",
    "            \"title\": extract_article_title(chunk) if article else \"\",\n",
    "            \"category\": \", \".join(categories),\n",
    "            \"priority\": priority,\n",
    "            \"effective_date\": effective_date,\n",
    "            \"source\": str(file_path),\n",
    "            \"parsed_at\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"âœ… íŒŒì‹± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. íŒŒì¼ ê²€ìƒ‰ ë° íŒŒì‹± ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ê·œì¹™ DOCX íŒŒì¼ íŒŒì‹± ì‹œì‘\n",
      "============================================================\n",
      "\n",
      "ğŸ“‚ ë°œê²¬ëœ DOCX íŒŒì¼: 1ê°œ\n",
      "   - ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™(ëŒ€ë²•ì›ê·œì¹™)(ì œ02986í˜¸)(20210610).docx\n"
     ]
    }
   ],
   "source": [
    "# DOCX íŒŒì¼ ì°¾ê¸°\n",
    "docx_files = list(RULE_RAW_DIR.glob(\"*.docx\"))\n",
    "docx_files = [f for f in docx_files if not f.name.startswith('~$')]  # ì„ì‹œ íŒŒì¼ ì œì™¸\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ê·œì¹™ DOCX íŒŒì¼ íŒŒì‹± ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not docx_files:\n",
    "    print(f\"\\nâš ï¸  ê·œì¹™ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ“ ë‹¤ìŒ ê²½ë¡œì— DOCX íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:\")\n",
    "    print(f\"   {RULE_RAW_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“‚ ë°œê²¬ëœ DOCX íŒŒì¼: {len(docx_files)}ê°œ\")\n",
    "    for f in docx_files:\n",
    "        print(f\"   - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ íŒŒì‹± ì¤‘: ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™(ëŒ€ë²•ì›ê·œì¹™)(ì œ02986í˜¸)(20210610).docx\n",
      "   âœ… 4 ì¡°ë¬¸ íŒŒì‹± ì™„ë£Œ\n",
      "\n",
      "============================================================\n",
      "íŒŒì‹± ì™„ë£Œ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì‹± ì‹¤í–‰\n",
    "all_dfs = []\n",
    "\n",
    "for docx_file in docx_files:\n",
    "    print(f\"\\nğŸ”„ íŒŒì‹± ì¤‘: {docx_file.name}\")\n",
    "    \n",
    "    filename = docx_file.stem\n",
    "    \n",
    "    # íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„\n",
    "    if \"ì‹œí–‰ë ¹\" in filename:\n",
    "        match = re.search(r'(\\d+)_(\\d{8})', filename)\n",
    "        if match:\n",
    "            law_no = match.group(1)\n",
    "            date = match.group(2)\n",
    "            src_title = f\"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹)(ì œ{law_no}í˜¸)({date})\"\n",
    "            effective_date = date\n",
    "        else:\n",
    "            src_title = \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹)\"\n",
    "            effective_date = \"2026-01-02\"\n",
    "        priority = 2\n",
    "        \n",
    "    elif \"ì‹œí–‰ê·œì¹™\" in filename:\n",
    "        src_title = \"ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ê·œì¹™\"\n",
    "        priority = 3\n",
    "        effective_date = \"2026-01-02\"\n",
    "\n",
    "    elif \"ë²•ë¬´ë¶€ë ¹\" in filename:\n",
    "        match = re.search(r'(\\d+)_(\\d{8})', filename)\n",
    "        if match:\n",
    "            law_no = match.group(1)\n",
    "            date = match.group(2)\n",
    "            src_title = f\"ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œìƒì˜ í™•ì •ì¼ì ë¶€ì—¬ ë° ì„ëŒ€ì°¨ ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™(ë²•ë¬´ë¶€ë ¹)(ì œ{law_no}í˜¸)({date})\"\n",
    "            effective_date = date\n",
    "        else:\n",
    "            src_title = \"í™•ì •ì¼ì ê´€ë ¨ ê·œì¹™(ë²•ë¬´ë¶€ë ¹)\"\n",
    "            effective_date = \"2022-02-07\"\n",
    "        priority = 3\n",
    "        \n",
    "    elif \"í™•ì •ì¼ì\" in filename or \"ëŒ€ë²•ì›ê·œì¹™\" in filename:\n",
    "        match = re.search(r'(\\d+)_(\\d{8})', filename)\n",
    "        if match:\n",
    "            law_no = match.group(1)\n",
    "            date = match.group(2)\n",
    "            src_title = f\"í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™(ì œ{law_no}í˜¸)({date})\"\n",
    "            effective_date = date\n",
    "        else:\n",
    "            src_title = \"í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™\"\n",
    "            effective_date = \"2021-06-10\"\n",
    "        priority = 3\n",
    "        \n",
    "    else:\n",
    "        src_title = filename\n",
    "        priority = 3\n",
    "        effective_date = \"2026-01-02\"\n",
    "    \n",
    "    df = parse_rule_docx(\n",
    "        file_path=docx_file,\n",
    "        src_title=src_title,\n",
    "        priority=priority,\n",
    "        effective_date=effective_date\n",
    "    )\n",
    "    \n",
    "    if not df.empty:\n",
    "        all_dfs.append(df)\n",
    "        print(f\"   âœ… {len(df)} ì¡°ë¬¸ íŒŒì‹± ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ íŒŒì‹±ëœ ì¡°ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"íŒŒì‹± ì™„ë£Œ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ê²°ê³¼ ì €ì¥ ë° í†µê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rule.csv ìƒì„± ì™„ë£Œ!\n",
      "ğŸ“Š ì´ 4 ì¡°ë¬¸ ì €ì¥\n",
      "ğŸ“ ì €ì¥ ê²½ë¡œ: c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\processed\\csv\\rule.csv\n"
     ]
    }
   ],
   "source": [
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(RULE_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"âœ… rule.csv ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“Š ì´ {len(final_df)} ì¡°ë¬¸ ì €ì¥\")\n",
    "    print(f\"ğŸ“ ì €ì¥ ê²½ë¡œ: {RULE_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"âŒ íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ ê·œì¹™ë³„ í†µê³„:\n",
      "   - í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™: 4ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ê·œì¹™ë³„ í†µê³„\n",
    "if all_dfs:\n",
    "    print(\"\\nğŸ“ˆ ê·œì¹™ë³„ í†µê³„:\")\n",
    "    stats = final_df.groupby('src_title').size()\n",
    "    for law, count in stats.items():\n",
    "        print(f\"   - {law}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\n",
      "   - í–‰ì •ì ˆì°¨: 4ê°œ\n",
      "   - ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥: 4ê°œ\n",
      "   - ì„ëŒ€ë£Œ_ì¦ê°: 2ê°œ\n",
      "   - ê¶Œë¦¬_ë¦¬ìŠ¤í¬: 1ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n",
    "if all_dfs:\n",
    "    print(\"\\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\")\n",
    "    all_categories = []\n",
    "    for cats in final_df['category']:\n",
    "        all_categories.extend([c.strip() for c in cats.split(',')])\n",
    "    \n",
    "    category_counts = Counter(all_categories)\n",
    "    for cat, count in category_counts.most_common():\n",
    "        print(f\"   - {cat}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ CSV ë¯¸ë¦¬ë³´ê¸° (ì²« 5ê°œ):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>src_title</th>\n",
       "      <th>article</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>priority</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>source</th>\n",
       "      <th>parsed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59c1c89f-3181-4871-88c2-dfd2bd496ab5</td>\n",
       "      <td>chunk_000</td>\n",
       "      <td>ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™\\n[ì‹œí–‰ 2021. 6....</td>\n",
       "      <td>í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™</td>\n",
       "      <td>ì œ1ì¡°</td>\n",
       "      <td>ë¶€ë™ì‚°ë“±ê¸°ê³¼</td>\n",
       "      <td>í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...</td>\n",
       "      <td>2026-01-21T12:40:52.830837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e4656595-1684-4caf-b2a4-836704a631dd</td>\n",
       "      <td>chunk_001</td>\n",
       "      <td>â‘¢ ì œ2í•­ì œ2í˜¸ì˜ ìê²©ìëŒ€ë¦¬ì¸ì€ã€Œë¶€ë™ì‚°ë“±ê¸°ê·œì¹™ã€ì œ68ì¡°ì— ë”°ë¥¸ ì‚¬ìš©ì(ì´ìš©)ë“±ë¡ì •ë³´...</td>\n",
       "      <td>í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™</td>\n",
       "      <td>ì œ68ì¡°</td>\n",
       "      <td>ì´ìš©</td>\n",
       "      <td>ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥, í–‰ì •ì ˆì°¨, ì„ëŒ€ë£Œ_ì¦ê°</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...</td>\n",
       "      <td>2026-01-21T12:40:52.830837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63ebe5c1-1ed3-4764-a875-4ab83028edd3</td>\n",
       "      <td>chunk_002</td>\n",
       "      <td>1. í•´ë‹¹ ì£¼íƒì˜ ì„ëŒ€ì¸ã†ì„ì°¨ì¸\\n2. í•´ë‹¹ ì£¼íƒì˜ ì†Œìœ ì\\n3. í•´ë‹¹ ì£¼íƒ ë˜ëŠ” ...</td>\n",
       "      <td>í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™</td>\n",
       "      <td>ì œ3ì¡°ì˜2</td>\n",
       "      <td>í™•ì •ì¼ì ì •ë³´ì œê³µì˜ ìš”ì²­ë°©ë²•</td>\n",
       "      <td>í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥, ì„ëŒ€ë£Œ_ì¦ê°</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...</td>\n",
       "      <td>2026-01-21T12:40:52.830837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dc124770-b8a2-4991-9485-552df4e7a228</td>\n",
       "      <td>chunk_003</td>\n",
       "      <td>1. ã€Œêµ­ë¯¼ê¸°ì´ˆìƒí™œ ë³´ì¥ë²•ã€ì œ2ì¡°ì œ2í˜¸ì— ë”°ë¥¸ ìˆ˜ê¸‰ì\\n2. ã€Œë…ë¦½ìœ ê³µìì˜ˆìš°ì— ê´€í•œ...</td>\n",
       "      <td>í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™</td>\n",
       "      <td>ì œ2ì¡°</td>\n",
       "      <td>ì„ ìˆœìœ„ìë§Œ í•´ë‹¹ëœë‹¤</td>\n",
       "      <td>ê¶Œë¦¬_ë¦¬ìŠ¤í¬, í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...</td>\n",
       "      <td>2026-01-21T12:40:52.830837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id   chunk_id  \\\n",
       "0  59c1c89f-3181-4871-88c2-dfd2bd496ab5  chunk_000   \n",
       "1  e4656595-1684-4caf-b2a4-836704a631dd  chunk_001   \n",
       "2  63ebe5c1-1ed3-4764-a875-4ab83028edd3  chunk_002   \n",
       "3  dc124770-b8a2-4991-9485-552df4e7a228  chunk_003   \n",
       "\n",
       "                                                text   src_title article  \\\n",
       "0  ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™\\n[ì‹œí–‰ 2021. 6....  í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™     ì œ1ì¡°   \n",
       "1  â‘¢ ì œ2í•­ì œ2í˜¸ì˜ ìê²©ìëŒ€ë¦¬ì¸ì€ã€Œë¶€ë™ì‚°ë“±ê¸°ê·œì¹™ã€ì œ68ì¡°ì— ë”°ë¥¸ ì‚¬ìš©ì(ì´ìš©)ë“±ë¡ì •ë³´...  í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™    ì œ68ì¡°   \n",
       "2  1. í•´ë‹¹ ì£¼íƒì˜ ì„ëŒ€ì¸ã†ì„ì°¨ì¸\\n2. í•´ë‹¹ ì£¼íƒì˜ ì†Œìœ ì\\n3. í•´ë‹¹ ì£¼íƒ ë˜ëŠ” ...  í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™   ì œ3ì¡°ì˜2   \n",
       "3  1. ã€Œêµ­ë¯¼ê¸°ì´ˆìƒí™œ ë³´ì¥ë²•ã€ì œ2ì¡°ì œ2í˜¸ì— ë”°ë¥¸ ìˆ˜ê¸‰ì\\n2. ã€Œë…ë¦½ìœ ê³µìì˜ˆìš°ì— ê´€í•œ...  í™•ì •ì¼ì ëŒ€ë²•ì›ê·œì¹™     ì œ2ì¡°   \n",
       "\n",
       "             title               category  priority effective_date  \\\n",
       "0           ë¶€ë™ì‚°ë“±ê¸°ê³¼          í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥         3     2021-06-10   \n",
       "1               ì´ìš©  ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥, í–‰ì •ì ˆì°¨, ì„ëŒ€ë£Œ_ì¦ê°         3     2021-06-10   \n",
       "2  í™•ì •ì¼ì ì •ë³´ì œê³µì˜ ìš”ì²­ë°©ë²•  í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥, ì„ëŒ€ë£Œ_ì¦ê°         3     2021-06-10   \n",
       "3       ì„ ìˆœìœ„ìë§Œ í•´ë‹¹ëœë‹¤  ê¶Œë¦¬_ë¦¬ìŠ¤í¬, í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥         3     2021-06-10   \n",
       "\n",
       "                                              source  \\\n",
       "0  c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...   \n",
       "1  c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...   \n",
       "2  c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...   \n",
       "3  c:\\project-AI2\\ì‹¤í”„ë¡œì íŠ¸\\data\\raw\\rule\\ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ ...   \n",
       "\n",
       "                    parsed_at  \n",
       "0  2026-01-21T12:40:52.830837  \n",
       "1  2026-01-21T12:40:52.830837  \n",
       "2  2026-01-21T12:40:52.830837  \n",
       "3  2026-01-21T12:40:52.830837  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì²« 5ê°œ í–‰ í™•ì¸\n",
    "if all_dfs:\n",
    "    print(\"\\nğŸ“‹ CSV ë¯¸ë¦¬ë³´ê¸° (ì²« 5ê°œ):\")\n",
    "    display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” íŠ¹ì • ì¡°ë¬¸ ìƒ˜í”Œ:\n",
      "ì¡°ë¬¸: ì œ1ì¡°\n",
      "ì œëª©: ë¶€ë™ì‚°ë“±ê¸°ê³¼\n",
      "ì¹´í…Œê³ ë¦¬: í–‰ì •ì ˆì°¨, ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥\n",
      "ìš°ì„ ìˆœìœ„: 3\n",
      "ë³¸ë¬¸(ì• 200ì): ì£¼íƒì„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ì ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™\n",
      "[ì‹œí–‰ 2021. 6. 10.] [ëŒ€ë²•ì›ê·œì¹™ ì œ2986í˜¸, 2021. 5. 27., íƒ€ë²•ê°œì •]\n",
      "ë²•ì›í–‰ì •ì²˜(ë¶€ë™ì‚°ë“±ê¸°ê³¼) 02-3480-1394\n",
      "ì œ1ì¡°(ëª©ì ) ì´ ê·œì¹™ì€ ì§€ë°©ë²•ì› ë° ê·¸ ì§€ì›ê³¼ ë“±ê¸°ì†Œê°€ã€Œì£¼íƒì„ëŒ€ì°¨ ë³´í˜¸ë²•ã€(ì´í•˜ â€œë²•â€ì´ë¼ í•œë‹¤)ì´ ì •í•˜ê³  ìˆëŠ” í™•ì •ì¼ì ë¶€ì—¬ ë“±ì˜ ì—…ë¬´ë¥¼ ì²˜ë¦¬í•¨ì— ìˆì–´ ...\n"
     ]
    }
   ],
   "source": [
    "# íŠ¹ì • ì¡°ë¬¸ í™•ì¸\n",
    "if all_dfs:\n",
    "    print(\"\\nğŸ” íŠ¹ì • ì¡°ë¬¸ ìƒ˜í”Œ:\")\n",
    "    sample = final_df[final_df['article'].str.contains('ì œ1ì¡°', na=False)].iloc[0]\n",
    "    print(f\"ì¡°ë¬¸: {sample['article']}\")\n",
    "    print(f\"ì œëª©: {sample['title']}\")\n",
    "    print(f\"ì¹´í…Œê³ ë¦¬: {sample['category']}\")\n",
    "    print(f\"ìš°ì„ ìˆœìœ„: {sample['priority']}\")\n",
    "    print(f\"ë³¸ë¬¸(ì• 200ì): {sample['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ì»¬ëŸ¼ ì •ë³´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ ë°ì´í„°í”„ë ˆì„ ì •ë³´:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              4 non-null      object\n",
      " 1   chunk_id        4 non-null      object\n",
      " 2   text            4 non-null      object\n",
      " 3   src_title       4 non-null      object\n",
      " 4   article         4 non-null      object\n",
      " 5   title           4 non-null      object\n",
      " 6   category        4 non-null      object\n",
      " 7   priority        4 non-null      int64 \n",
      " 8   effective_date  4 non-null      object\n",
      " 9   source          4 non-null      object\n",
      " 10  parsed_at       4 non-null      object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 480.0+ bytes\n",
      "None\n",
      "\n",
      "ì»¬ëŸ¼ ëª©ë¡:\n",
      "['id', 'chunk_id', 'text', 'src_title', 'article', 'title', 'category', 'priority', 'effective_date', 'source', 'parsed_at']\n"
     ]
    }
   ],
   "source": [
    "if all_dfs:\n",
    "    print(\"\\nğŸ“‹ ë°ì´í„°í”„ë ˆì„ ì •ë³´:\")\n",
    "    print(final_df.info())\n",
    "    print(\"\\nì»¬ëŸ¼ ëª©ë¡:\")\n",
    "    print(final_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
