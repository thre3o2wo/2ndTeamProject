"""
ì£¼íƒì„ëŒ€ì°¨ ì§ˆë¬¸ ìë™ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
Django ë·°ì—ì„œ í™œìš© ê°€ëŠ¥

ì‚¬ìš©ë²•:
1. Django views.pyì—ì„œ import
2. process_batch_questions() í˜¸ì¶œ
3. ê²°ê³¼ë¥¼ í…œí”Œë¦¿ì— ì „ë‹¬
"""

import os
from pathlib import Path
from dotenv import load_dotenv
import pandas as pd
from datetime import datetime

from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from pinecone import Pinecone


# =========================
# ì„¤ì •
# =========================
BASE_DIR = Path(__file__).resolve().parent
env_path = BASE_DIR / ".env"
load_dotenv(env_path)

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

LAW_INDEX_NAME = "housing-law-index"
RULE_INDEX_NAME = "housing-rule-index"
CASE_INDEX_NAME = "housing-case-index"


# =========================
# ì§ˆë¬¸ ëª©ë¡ ë°ì´í„°
# =========================
QUESTION_DATASET = [
    {
        "article": "ì œ3ì¡°",
        "category": "ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥",
        "question": "ì „ì…ì‹ ê³ ë‘ í™•ì •ì¼ìëŠ” í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ì— ë­ê°€ ì–´ë–»ê²Œ ì í˜€ ìˆëŠ”ì§€ê¹Œì§€ í™•ì¸í•´ì•¼ í•˜ë‚˜ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ4ì¡°", "ì‹œí–‰ë ¹ ì œ6ì¡°", "ì‹œí–‰ë ¹ ì œ5ì¡°"]
    },
    {
        "article": "ì œ4ì¡°",
        "category": "ê³„ì•½ê°±ì‹ ",
        "question": "ê³„ì•½ì„œì— 1ë…„ë§Œ ì‚´ê¸°ë¡œ ì¨ ìˆëŠ”ë°, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ë‚˜ê°€ì•¼ í•˜ëŠ” ê±´ê°€ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ5ì¡°",
        "category": "ê³„ì•½í•´ì§€",
        "question": "ì œê°€ ì ê¹ í•´ì™¸ì— ë‚˜ê°€ëŠ”ë°, ê·¸ë™ì•ˆ ì¹œêµ¬ê°€ ëŒ€ì‹  ì‚´ì•„ë„ ëœë‹¤ê³  ì§‘ì£¼ì¸ì´ë‘ ë§ë¡œë§Œ ì–˜ê¸°í–ˆì–´ìš”. ë¬¸ì œ ë  ìˆ˜ ìˆë‚˜ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ6ì¡°",
        "category": "ê³„ì•½ê°±ì‹ ",
        "question": "ê³„ì•½ ëë‚  ë•Œê¹Œì§€ ì§‘ì£¼ì¸ì´ ì•„ë¬´ ë§ ì•ˆ í–ˆëŠ”ë°, ì´ê±° ìë™ìœ¼ë¡œ ì—°ì¥ëœ ê±´ê°€ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ6ì¡°ì˜2",
        "category": "ê³„ì•½í•´ì§€",
        "question": "ê³„ì•½ì´ ê·¸ëƒ¥ ì—°ì¥ëœ ì¤„ ëª¨ë¥´ê³  ì‚´ê³  ìˆì—ˆëŠ”ë°ìš”. ì œê°€ ê°‘ìê¸° ì´ì‚¬ ê°€ì•¼ í•˜ë©´ ì–¸ì œê¹Œì§€ ì‚´ì•„ì•¼ í•˜ë‚˜ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ6ì¡°ì˜3",
        "category": "ê³„ì•½ê°±ì‹ ",
        "question": "ì§‘ì£¼ì¸ì´ ì‹¤ê±°ì£¼í•œë‹¤ê³  ê°±ì‹  ê±°ì ˆí–ˆëŠ”ë°ìš”. ë‚˜ì¤‘ì— ì‹¤ì œë¡œ ì•ˆ ì‚´ë©´, ì œê°€ ê·¸ ê¸°ë¡ ê°™ì€ ê±¸ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ5ì¡°"]
    },
    {
        "article": "ì œ7ì¡°",
        "category": "ì„ëŒ€ë£Œ_ì¦ê°",
        "question": "ì¬ê³„ì•½í•˜ë©´ì„œ ì›”ì„¸ë¥¼ í•œ ë²ˆì— ë§ì´ ì˜¬ë¦¬ìê³  í•˜ëŠ”ë°, ì§‘ì£¼ì¸ì´ ì •í•˜ëŠ” ëŒ€ë¡œ ë”°ë¼ì•¼ í•˜ë‚˜ìš”? ê·¸ë¦¬ê³  ì›”ì„¸ë¥¼ ì˜¬ë¦° ì§€ 6ê°œì›”ë°–ì— ì•ˆ ëëŠ”ë° ë˜ ì˜¬ë¦¬ìê³  í•´ìš”.",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ8ì¡°"]
    },
    {
        "article": "ì œ7ì¡°ì˜2",
        "category": "ì„ëŒ€ë£Œ_ì¦ê°",
        "question": "ë³´ì¦ê¸ˆ ì¤„ì—¬ì£¼ëŠ” ëŒ€ì‹  ì›”ì„¸ë¡œ ë°”ê¾¸ìëŠ”ë°, ì›”ì„¸ë¥¼ ë„ˆë¬´ ë§ì´ ë°›ìœ¼ë ¤ê³  í•´ìš”. ê¸°ì¤€ ê°™ì€ ê²Œ ìˆë‚˜ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ9ì¡°"]
    },
    {
        "article": "ì œ8ì¡°",
        "category": "ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥",
        "question": "ì§‘ì´ ê²½ë§¤ë¡œ ë„˜ì–´ê°„ë‹¤ê³  ë“¤ì—ˆì–´ìš”. ì €ëŠ” ë³´ì¦ê¸ˆì´ í¬ì§„ ì•Šì€ë°, ì¼ë¶€ë¼ë„ ë¨¼ì € ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ10ì¡°", "ì‹œí–‰ë ¹ ì œ11ì¡°"]
    },
    {
        "article": "ì œ10ì¡°",
        "category": "ê¶Œë¦¬_ë¦¬ìŠ¤í¬",
        "question": "ê³„ì•½ì„œì— 'ì§‘ì£¼ì¸ì´ ì›í•˜ë©´ ì–¸ì œë“  ë‚˜ê°€ì•¼ í•œë‹¤'ê³  ì¨ ìˆëŠ”ë°, ì œê°€ ì‚¬ì¸í–ˆìœ¼ë©´ ë¬´ì¡°ê±´ ì§€ì¼œì•¼ í•˜ë‚˜ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ10ì¡°ì˜2",
        "category": "ì„ëŒ€ë£Œ_ì¦ê°",
        "question": "ì¬ê³„ì•½í•˜ë©´ì„œ ì›”ì„¸ë¥¼ ë§ì´ ì˜¬ë ¸ëŠ”ë°, ë‚˜ì¤‘ì— ë³´ë‹ˆê¹Œ ë²•ì—ì„œ ì •í•œ ë¹„ìœ¨ë³´ë‹¤ ë” ë‚¸ ê²ƒ ê°™ì•„ìš”. ì´ë¯¸ ë‚¸ ëˆë„ ëŒë ¤ë‹¬ë¼ê³  í•  ìˆ˜ ìˆë‚˜ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ11ì¡°",
        "category": "ê³„ì•½í•´ì§€",
        "question": "ëª‡ ë‹¬ë§Œ ì‚´ ê±°ë¼ê³  í•´ì„œ ê³„ì•½í–ˆëŠ”ë°, ì§‘ì£¼ì¸ì´ ì´ê±´ ê·¸ëƒ¥ ì ê¹ ì“°ëŠ” ê±°ë¼ ë²• ì ìš© ì•ˆ ëœë‹¤ê³  í•˜ë„¤ìš”. ì§„ì§œ ê·¸ëŸ° ê±´ê°€ìš”?",
        "expected_rules": []
    },
    {
        "article": "ì œ14ì¡°",
        "category": "ë¶„ìŸí•´ê²°",
        "question": "ì§‘ì£¼ì¸ì´ë‘ ì›”ì„¸ë‘ ë³´ì¦ê¸ˆ ë¬¸ì œë¡œ ê³„ì† ì‹¸ìš°ëŠ”ë°, ë²•ì› ë§ê³  ì¤‘ê°„ì—ì„œ ì¡°ì •í•´ì£¼ëŠ” ë°ëŠ” ì—†ì–´ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ22ì¡°"]
    },
    {
        "article": "ì œ21ì¡°",
        "category": "í–‰ì •ì ˆì°¨",
        "question": "ì œê°€ ì§€ê¸ˆ ë‹¤ë¥¸ ì§€ì—­ìœ¼ë¡œ ì´ì‚¬ ì™”ëŠ”ë°ìš”. ì¡°ì • ì‹ ì²­ì€ ì§€ê¸ˆ ì‚¬ëŠ” ê³³ì—ì„œ í•˜ë©´ ë˜ë‚˜ìš”, ì›ë˜ ì§‘ ìˆëŠ” ë°ì„œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ30ì¡°", "ì‹œí–‰ë ¹ ì œ33ì¡°"]
    },
    {
        "article": "ì œ22ì¡°",
        "category": "í–‰ì •ì ˆì°¨",
        "question": "ì¡°ì • ì‹ ì²­í•˜ê³  ë‚˜ë©´ ì§‘ì£¼ì¸í•œí…Œ ë°”ë¡œ ì—°ë½ ê°€ë‚˜ìš”? ëª°ë˜ ì§„í–‰ë˜ëŠ” ê±´ ì•„ë‹ˆì£ ?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ32ì¡°"]
    },
    {
        "article": "ì œ27ì¡°",
        "category": "ë¶„ìŸí•´ê²°",
        "question": "ì¡°ì •ì—ì„œ í•©ì˜í–ˆëŠ”ë° ì§‘ì£¼ì¸ì´ ë˜ ì•ˆ ì§€ì¼œìš”. ì´ê±° ê·¸ëƒ¥ ì•½ì†ì´ë¼ ê°•ì œë¡œ ëª» ë°›ëŠ” ê±´ê°€ìš”?",
        "expected_rules": ["ì‹œí–‰ë ¹ ì œ34ì¡°", "ì‹œí–‰ë ¹ ì œ35ì¡°"]
    },
    {
        "article": "ì œ30ì¡°",
        "category": "í–‰ì •ì ˆì°¨",
        "question": "ì§‘ì£¼ì¸ì´ ê·¸ëƒ¥ ìê¸° ê³„ì•½ì„œ ì–‘ì‹ ì“°ìê³  í•˜ëŠ”ë°ìš”. í‘œì¤€ê³„ì•½ì„œ ê¼­ ì¨ì•¼ í•˜ëŠ” ê±° ì•„ë‹ˆì—ìš”?",
        "expected_rules": []
    }
]


# =========================
# RAG ì—”ì§„ ì´ˆê¸°í™”
# =========================
class HousingRAG:
    def __init__(self):
        """RAG ì—”ì§„ ì´ˆê¸°í™”"""
        # ì„ë² ë”©
        self.embedding = UpstageEmbeddings(
            model="solar-embedding-1-large-passage",
            api_key=UPSTAGE_API_KEY
        )
        
        # Pinecone
        pc = Pinecone(api_key=PINECONE_API_KEY)
        
        # 3ê°œ VectorStore
        self.law_vectorstore = PineconeVectorStore(
            index_name=LAW_INDEX_NAME,
            embedding=self.embedding
        )
        
        self.rule_vectorstore = PineconeVectorStore(
            index_name=RULE_INDEX_NAME,
            embedding=self.embedding
        )
        
        self.case_vectorstore = PineconeVectorStore(
            index_name=CASE_INDEX_NAME,
            embedding=self.embedding
        )
        
        # LLM
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.2,
            openai_api_key=OPENAI_API_KEY
        )
        
        # í”„ë¡¬í”„íŠ¸
        self.prompt_template = PromptTemplate(
            template="""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì£¼íƒì„ëŒ€ì°¨ ì „ë¬¸ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤.

### ë‹µë³€ ì‘ì„± ê·œì¹™:
1. **ì§ˆë¬¸ì˜ í•µì‹¬ì— ë¨¼ì € ì§ì ‘ ë‹µë³€**í•˜ì„¸ìš” (ì˜ˆ: "ë„¤, ë‚˜ê°€ì…”ì•¼ í•©ë‹ˆë‹¤" ë˜ëŠ” "ì•„ë‹ˆìš”, ì•ˆ ë‚˜ê°€ì…”ë„ ë©ë‹ˆë‹¤")
2. **ì´ìœ ë¥¼ ì‰½ê³  ê°„ê²°í•˜ê²Œ** ì„¤ëª…í•˜ì„¸ìš”
3. **ê·¼ê±° ë²•ë ¹ì„ ìì—°ìŠ¤ëŸ½ê²Œ** ì–¸ê¸‰í•˜ì„¸ìš” (ì¡°ë¬¸ ë²ˆí˜¸ëŠ” ê´„í˜¸ ì•ˆì—)
4. **êµ¬ì²´ì ì¸ í–‰ë™ ë°©ë²•**ì„ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•˜ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ë¶ˆí•„ìš”í•œ ë²•ë¥  ìš©ì–´ ì§€ì–‘)
6. ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•œ ë‚´ìš©ì€ **ëª…í™•íˆ ê°•ì¡°**í•˜ì„¸ìš”

### ê²€ìƒ‰ëœ ë²•ë ¹ ë° ì‚¬ë¡€:
{context}

### ì§ˆë¬¸:
{question}

### ë‹µë³€ (í•µì‹¬ ë‹µ â†’ ì´ìœ  â†’ ê·¼ê±° â†’ ì‹¤í–‰ ë°©ë²• ìˆœì„œë¡œ, ì¹œì ˆí•˜ê²Œ):
""",
            input_variables=["context", "question"]
        )
    
    def rerank_by_priority(self, documents):
        """Priority ê¸°ë°˜ ì¬ì •ë ¬"""
        return sorted(documents, key=lambda doc: doc.metadata.get('priority', 99))
    
    def query(self, question, k_per_index=7, top_n=15):
        """
        ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k_per_index: ê° ì¸ë±ìŠ¤ì—ì„œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 7ê°œë¡œ ì¦ê°€)
            top_n: ìµœì¢… ì„ íƒí•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 15ê°œë¡œ ì¦ê°€)
        
        Returns:
            dict: {
                'answer': AI ë‹µë³€,
                'sources': ì°¸ê³  ë²•ë ¹ ë¦¬ìŠ¤íŠ¸,
                'retrieved_docs': ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜
            }
        """
        try:
            # 1. 3ê°œ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
            law_docs = self.law_vectorstore.similarity_search(question, k=k_per_index)
            rule_docs = self.rule_vectorstore.similarity_search(question, k=k_per_index)
            case_docs = self.case_vectorstore.similarity_search(question, k=k_per_index)
            
            # 2. í†µí•© ë° Rerank
            all_docs = law_docs + rule_docs + case_docs
            reranked_docs = self.rerank_by_priority(all_docs)
            top_docs = reranked_docs[:top_n]
            
            # 3. Context ìƒì„±
            context_parts = []
            for i, doc in enumerate(top_docs, 1):
                meta = doc.metadata
                law_name = meta.get('law_name', meta.get('src_title', 'Unknown'))
                article = meta.get('article', '')
                content = doc.page_content[:500]  # 300 â†’ 500ìë¡œ ì¦ê°€
                
                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {law_name} {article}\në‚´ìš©: {content}...\n"
                )
            
            context = "\n".join(context_parts)
            
            # 4. LLM ë‹µë³€ ìƒì„±
            prompt = self.prompt_template.format(context=context, question=question)
            answer = self.llm.invoke(prompt).content
            
            # 5. ì°¸ê³  ë²•ë ¹ ì •ë¦¬
            sources = []
            for doc in top_docs:
                meta = doc.metadata
                sources.append({
                    'law_name': meta.get('law_name', meta.get('src_title', '?')),
                    'article': meta.get('article', ''),
                    'priority': int(meta.get('priority', 99))   # â¬…ï¸ í•µì‹¬
                })

            sources = sorted(sources, key=lambda x: x['priority'])
            
            return {
                'answer': answer,
                'sources': sources,
                'retrieved_docs': len(all_docs)
            }
        
        except Exception as e:
            return {
                'answer': f"âŒ ì˜¤ë¥˜: {str(e)}",
                'sources': [],
                'retrieved_docs': 0
            }


# =========================
# ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
# =========================
def process_batch_questions(questions=None, save_csv=True):
    """
    ì§ˆë¬¸ ëª©ë¡ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
    
    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: QUESTION_DATASET)
        save_csv: ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
    
    Returns:
        pd.DataFrame: ì²˜ë¦¬ ê²°ê³¼
    """
    if questions is None:
        questions = QUESTION_DATASET
    
    # RAG ì—”ì§„ ì´ˆê¸°í™”
    print("ğŸ”§ RAG ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
    rag = HousingRAG()
    print("âœ… ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    # ê²°ê³¼ ì €ì¥
    results = []
    
    for i, q_data in enumerate(questions, 1):
        question = q_data['question']
        print(f"[{i}/{len(questions)}] ì²˜ë¦¬ ì¤‘: {question[:50]}...")
        
        # RAG ì§ˆì˜
        result = rag.query(question)
        
        # ê²°ê³¼ ê¸°ë¡
        results.append({
            'article': q_data.get('article', ''),
            'category': q_data.get('category', ''),
            'question': question,
            'answer': result['answer'],
            'retrieved_docs': result['retrieved_docs'],
            'top_source': result['sources'][0]['law_name'] if result['sources'] else '',
            'top_article': result['sources'][0]['article'] if result['sources'] else '',
            'timestamp': datetime.now().isoformat()
        })
        
        print(f"   âœ… ì™„ë£Œ (ê²€ìƒ‰: {result['retrieved_docs']}ê°œ)\n")
    
    # DataFrame ë³€í™˜
    df = pd.DataFrame(results)
    
    # CSV ì €ì¥
    if save_csv:
        output_path = BASE_DIR / "data" / "processed" / "batch_results.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    return df


# =========================
# Django ë·°ìš© í•¨ìˆ˜
# =========================
def get_answer_for_question(question, category=None):
    """
    ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (Django ë·°ì—ì„œ ì‚¬ìš©)
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        category: ì¹´í…Œê³ ë¦¬ íŒíŠ¸ (ì˜µì…˜)
    
    Returns:
        dict: {
            'question': ì§ˆë¬¸,
            'answer': ë‹µë³€,
            'sources': ì°¸ê³  ë²•ë ¹,
            'category': ì¶”ë¡ ëœ ì¹´í…Œê³ ë¦¬
        }
    """
    rag = HousingRAG()
    result = rag.query(question)
    
    return {
        'question': question,
        'answer': result['answer'],
        'sources': result['sources'][:5],  # ìƒìœ„ 5ê°œë§Œ
        'category': category or 'ì¼ë°˜'
    }


# =========================
# ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©)
# =========================
if __name__ == "__main__":
    print("=" * 70)
    print("ì£¼íƒì„ëŒ€ì°¨ ì§ˆë¬¸ ìë™ ì²˜ë¦¬ ì‹œì‘")
    print("=" * 70)
    print()
    
    # ë°°ì¹˜ ì²˜ë¦¬
    df_results = process_batch_questions()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    print(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(df_results)}")
    print(f"í‰ê·  ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {df_results['retrieved_docs'].mean():.1f}")
    print("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    print(df_results['category'].value_counts())
    
    print("\n" + "=" * 70)
    print("âœ… ì™„ë£Œ")
    print("=" * 70)