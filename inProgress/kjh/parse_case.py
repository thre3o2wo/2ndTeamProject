"""
ì‚¬ë¡€ì§‘(PDF) íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥
íŒë¡€, ì „ì„¸í”¼í•´ ì‚¬ë¡€ì§‘ ë“±ì„ ì²˜ë¦¬

ë©”íƒ€ë°ì´í„° ì „ëµ:
- law: ë²•ë ¹ëª…, ì¡°í•­ ë²ˆí˜¸ ì¤‘ì‹¬
- case: ì‚¬ê±´ ë²ˆí˜¸, íŒê²° ìš”ì§€, í‚¤ì›Œë“œ ì¤‘ì‹¬
"""

from pathlib import Path
import pandas as pd
import PyPDF2
import re
import uuid
from datetime import datetime

# =========================
# Path ì„¤ì •
# =========================
BASE_DIR = Path(__file__).resolve().parents[2]

DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
CSV_DIR = PROCESSED_DIR / "csv"
LOG_DIR = PROCESSED_DIR / "log"

CASE_RAW_DIR = RAW_DIR / "case"

CASE_CSV_PATH = CSV_DIR / "case.csv"

# ë””ë ‰í† ë¦¬ ìƒì„±
CSV_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)
CASE_RAW_DIR.mkdir(parents=True, exist_ok=True)


def extract_text_from_pdf(pdf_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text
    except Exception as e:
        print(f"   âŒ PDF ì½ê¸° ì‹¤íŒ¨: {e}")
        return ""


def parse_case_pdf(file_path, case_type="ì‚¬ë¡€ì§‘"):
    """
    ì‚¬ë¡€ì§‘ PDF íŒŒì‹±
    
    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        case_type: "ì‚¬ë¡€ì§‘" ë˜ëŠ” "íŒë¡€ì§‘"
    
    Returns:
        pd.DataFrame: ì‚¬ë¡€ ì •ë³´ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„
    """
    
    text = extract_text_from_pdf(file_path)
    
    if not text:
        return pd.DataFrame()
    
    # ì‚¬ë¡€ ë²ˆí˜¸ íŒ¨í„´: "ì‚¬ë¡€ 1", "Case 1", "ã€ì‚¬ë¡€1ã€‘" ë“±
    case_pattern = re.compile(r"(?:ì‚¬ë¡€|CASE|ã€ì‚¬ë¡€)\s*(\d+).*?(?=ì‚¬ë¡€|CASE|ã€ì‚¬ë¡€|\Z)", re.DOTALL | re.IGNORECASE)
    
    # íŒë¡€ ë²ˆí˜¸ íŒ¨í„´: "ëŒ€ë²•ì› 2020ë‹¤12345"
    precedent_pattern = re.compile(r"(ëŒ€ë²•ì›|ì„œìš¸ê³ ë“±ë²•ì›|ì„œìš¸ì¤‘ì•™ì§€ë°©ë²•ì›)\s*(\d{4}[ê°€-í£]\d+)")
    
    rows = []
    
    # ì‚¬ë¡€ì§‘ì¸ ê²½ìš°
    if "ì‚¬ë¡€" in case_type:
        matches = case_pattern.finditer(text)
        
        for match in matches:
            case_num = match.group(1)
            case_content = match.group(0)
            
            # ì‚¬ë¡€ ë‚´ìš©ì„ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬ (1000ìì”©)
            chunks = [case_content[i:i+1000] for i in range(0, len(case_content), 1000)]
            
            for i, chunk in enumerate(chunks):
                rows.append({
                    "id": str(uuid.uuid4()),
                    "content": chunk.strip(),
                    "case_family": "ì£¼íƒì„ëŒ€ì°¨",
                    "case_type": case_type,
                    "case_number": f"ì‚¬ë¡€{case_num}-{i+1}" if len(chunks) > 1 else f"ì‚¬ë¡€{case_num}",
                    "keywords": extract_keywords(chunk),
                    "source_file": Path(file_path).name,
                    "parsed_at": datetime.now().isoformat()
                })
    
    # íŒë¡€ì§‘ì¸ ê²½ìš°
    else:
        # íŒë¡€ ë²ˆí˜¸ë¡œ ë¶„ë¦¬
        precedent_matches = precedent_pattern.finditer(text)
        
        for match in precedent_matches:
            court = match.group(1)
            case_no = match.group(2)
            
            # íŒë¡€ ë²ˆí˜¸ ìœ„ì¹˜ë¶€í„° ë‹¤ìŒ íŒë¡€ê¹Œì§€ ì¶”ì¶œ
            start = match.start()
            next_match = precedent_pattern.search(text, start + 1)
            end = next_match.start() if next_match else len(text)
            
            precedent_content = text[start:end]
            
            rows.append({
                "id": str(uuid.uuid4()),
                "content": precedent_content.strip()[:2000],  # ì²˜ìŒ 2000ì
                "case_family": "ì£¼íƒì„ëŒ€ì°¨",
                "case_type": "íŒë¡€",
                "case_number": f"{court} {case_no}",
                "keywords": extract_keywords(precedent_content),
                "source_file": Path(file_path).name,
                "parsed_at": datetime.now().isoformat()
            })
    
    return pd.DataFrame(rows)


def extract_keywords(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
    ì„ëŒ€ì°¨ ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keywords = [
        "ë³´ì¦ê¸ˆ", "ì›”ì„¸", "ì „ì„¸", "ëŒ€í•­ë ¥", "ìš°ì„ ë³€ì œê¶Œ",
        "í™•ì •ì¼ì", "ì„ì°¨ê¶Œë“±ê¸°", "ê³„ì•½ê°±ì‹ ", "ì„ëŒ€ì°¨",
        "ìˆ˜ì„ ì˜ë¬´", "ì›ìƒíšŒë³µ", "ì†í•´ë°°ìƒ", "ëª…ë„"
    ]
    
    found = [kw for kw in keywords if kw in text]
    return ", ".join(found) if found else "ê¸°íƒ€"


def log_parsing_result(log_path, case_name, row_count, status="SUCCESS"):
    """íŒŒì‹± ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(f"[{datetime.now().isoformat()}] {status} | {case_name} | {row_count} rows\n")


# =========================
# ì‹¤í–‰ë¶€
# =========================
if __name__ == "__main__":
    
    print("=" * 60)
    print("ì‚¬ë¡€ì§‘ PDF íŒŒì¼ íŒŒì‹± ì‹œì‘")
    print("=" * 60)
    
    # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = list(CASE_RAW_DIR.glob("*.pdf"))
    
    if not pdf_files:
        print(f"\nâš ï¸  ì‚¬ë¡€ì§‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ë‹¤ìŒ ê²½ë¡œì— PDF íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:")
        print(f"   {CASE_RAW_DIR}")
        print(f"\nâœ… í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸:")
        print(f"   ì¡´ì¬ ì—¬ë¶€: {CASE_RAW_DIR.exists()}")
        if CASE_RAW_DIR.exists():
            all_files = list(CASE_RAW_DIR.glob("*"))
            if all_files:
                print(f"   ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")
                for f in all_files[:5]:
                    print(f"      - {f.name}")
            else:
                print(f"   ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        print(f"\nğŸ“ ì˜ˆì‹œ íŒŒì¼ëª…:")
        print(f"   - 2025ì „ì„¸í”¼í•´ì§€ì›ì‚¬ë¡€ì§‘.pdf")
        print(f"   - ì „ì„¸í”¼í•´ë²•ë¥ ìƒë‹´ì‚¬ë¡€ì§‘.pdf")
        print(f"   - ì£¼íƒì„ëŒ€ì°¨_íŒë¡€ëª¨ìŒ.pdf")
        
        # ìƒ˜í”Œ CSV ìƒì„±
        print(f"\nğŸ“ ìƒ˜í”Œ CSV íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        sample_df = pd.DataFrame([
            {
                "id": str(uuid.uuid4()),
                "content": "ì„ì°¨ì¸ Aì”¨ëŠ” ì „ì„¸ ê³„ì•½ ì¢…ë£Œ í›„ ë³´ì¦ê¸ˆì„ ëŒë ¤ë°›ì§€ ëª»í•˜ëŠ” ìƒí™©ì— ì²˜í–ˆë‹¤...",
                "case_family": "ì£¼íƒì„ëŒ€ì°¨",
                "case_type": "ì‚¬ë¡€ì§‘",
                "case_number": "ì‚¬ë¡€1",
                "keywords": "ë³´ì¦ê¸ˆ, ì „ì„¸, ëª…ë„",
                "source_file": "sample.pdf",
                "parsed_at": datetime.now().isoformat()
            }
        ])
        sample_df.to_csv(CASE_CSV_PATH, index=False, encoding="utf-8-sig")
        print(f"âœ… ìƒ˜í”Œ CSV ì €ì¥: {CASE_CSV_PATH}")
        
    else:
        # ë°œê²¬ëœ ëª¨ë“  PDF íŒŒì¼ íŒŒì‹±
        print(f"\nğŸ“‚ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        for f in pdf_files:
            print(f"   - {f.name}")
        
        all_dfs = []
        
        for pdf_file in pdf_files:
            print(f"\nğŸ”„ íŒŒì‹± ì¤‘: {pdf_file.name}")
            
            # íŒŒì¼ëª…ì—ì„œ ì‚¬ë¡€ì§‘ ìœ í˜• íŒë‹¨
            filename = pdf_file.stem.lower()
            
            if "íŒë¡€" in filename:
                case_type = "íŒë¡€ì§‘"
            else:
                case_type = "ì‚¬ë¡€ì§‘"
            
            df = parse_case_pdf(pdf_file, case_type)
            
            if not df.empty:
                all_dfs.append(df)
                print(f"   âœ… {len(df)} ì‚¬ë¡€ íŒŒì‹± ì™„ë£Œ")
            else:
                print(f"   âš ï¸ íŒŒì‹±ëœ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
        if all_dfs:
            final_df = pd.concat(all_dfs, ignore_index=True)
            final_df.to_csv(CASE_CSV_PATH, index=False, encoding="utf-8-sig")
            
            log_parsing_result(
                LOG_DIR / "parsing_log.txt",
                "ì‚¬ë¡€ì§‘ ì „ì²´",
                len(final_df)
            )
            
            print(f"\nâœ… case.csv ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ {len(final_df)} ì‚¬ë¡€ ì €ì¥")
            print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {CASE_CSV_PATH}")
            
            # ì‚¬ë¡€ ìœ í˜•ë³„ í†µê³„
            print(f"\nğŸ“ˆ ì‚¬ë¡€ ìœ í˜•ë³„ í†µê³„:")
            stats = final_df.groupby('case_type').size()
            for case_type, count in stats.items():
                print(f"   - {case_type}: {count}ê°œ")
            
            # í‚¤ì›Œë“œë³„ í†µê³„
            print(f"\nğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
            all_keywords = []
            for kws in final_df['keywords']:
                all_keywords.extend(kws.split(', '))
            from collections import Counter
            keyword_counts = Counter(all_keywords)
            for kw, count in keyword_counts.most_common(10):
                print(f"   - {kw}: {count}íšŒ")
        else:
            print("\nâŒ íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)
    print("íŒŒì‹± ì™„ë£Œ")
    print("=" * 60)