{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5abd75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:90% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
       "div.text_cell_render.rendered_html{font-size:12pt;}\n",
       "div.output {font-size:12pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:12pt;}\n",
       "div.prompt {min-width:70px;}}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
       "table.dataframe{font-size:12px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:90% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:12pt;}\n",
    "div.text_cell_render.rendered_html{font-size:12pt;}\n",
    "div.output {font-size:12pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:12pt;}\n",
    "div.prompt {min-width:70px;}}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:12pt;padding:5px;}\n",
    "table.dataframe{font-size:12px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49900b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-docx datasets ragas\n",
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0f688",
   "metadata": {},
   "source": [
    "# RAG MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda360c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì‹±ëœ ë¬¸í•­ ìˆ˜: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:00:54,846 - chatbot_app.modules.rag_module - INFO - ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...\n",
      "2026-01-30 17:00:57,601 - chatbot_app.modules.rag_module - INFO - âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "2026-01-30 17:00:57,656 - chatbot_app.modules.rag_module - INFO - â„¹ï¸ SimpleTokenizer ì‚¬ìš© (BM25)\n",
      "2026-01-30 17:00:58,754 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:00:58,779 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì£¼ë¯¼ë“±ë¡Â·í™•ì •ì¼ì í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”? (ì£¼ë¯¼ë“±ë¡Â·í™•ì •ì¼ì í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”?)  \n",
      "\n",
      "â€» [ìš©ì–´ ì‚¬ì „]ì— 'ì „ì…ì‹ ê³ ' â†’ 'ì£¼ë¯¼ë“±ë¡' ë§¤í•‘ë§Œ ì¡´ì¬í•˜ë©°, 'í™•ì •ì¼ìë¶€'ëŠ” ë³„ë„ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ì›ë¬¸ì„ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
      "2026-01-30 17:00:58,780 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì£¼ë¯¼ë“±ë¡Â·í™•ì •ì¼ì í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”? (ì£¼ë¯¼ë“±ë¡Â·í™•ì •ì¼ì í–ˆëŠ”ë°, í™•ì •ì¼ìë¶€ ë‚´ìš©ê¹Œì§€ ì¤‘ìš”í•œê°€ìš”?)  \n",
      "\n",
      "â€» [ìš©ì–´ ì‚¬ì „]ì— 'ì „ì…ì‹ ê³ ' â†’ 'ì£¼ë¯¼ë“±ë¡' ë§¤í•‘ë§Œ ì¡´ì¬í•˜ë©°, 'í™•ì •ì¼ìë¶€'ëŠ” ë³„ë„ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ì›ë¬¸ì„ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.'\n",
      "2026-01-30 17:00:59,226 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:01,241 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:03,394 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:06,417 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:06,424 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=60 (threshold=0.2)\n",
      "2026-01-30 17:01:06,870 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:07,600 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:08,741 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:09,008 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:01:32,804 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:33,336 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:33,336 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ê³„ì•½ì¦ì„œ(ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ)ì— 1ë…„ì´ë¼ê³  ì¨ ìˆìœ¼ë©´, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ì£¼íƒì˜ì¸ë„(í‡´ê±°)í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-01-30 17:01:33,336 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ê³„ì•½ì¦ì„œ(ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ)ì— 1ë…„ì´ë¼ê³  ì¨ ìˆìœ¼ë©´, 1ë…„ ì§€ë‚˜ë©´ ë¬´ì¡°ê±´ ì£¼íƒì˜ì¸ë„(í‡´ê±°)í•´ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-01-30 17:01:33,948 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:34,907 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:35,461 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:36,957 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:36,967 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=9 (threshold=0.2)\n",
      "2026-01-30 17:01:37,414 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:38,101 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:39,022 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:39,314 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:01:52,869 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:53,354 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:53,354 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ê°€ ì•„ë¬´ ë§ ì•ˆ í–ˆëŠ”ë°, ê³„ì•½ì´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ëœ ê±´ê°€ìš”?\n",
      "2026-01-30 17:01:53,354 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì„ëŒ€ì¸(ì§‘ì£¼ì¸)ê°€ ì•„ë¬´ ë§ ì•ˆ í–ˆëŠ”ë°, ê³„ì•½ì´ ë¬µì‹œì ê°±ì‹ (ìë™ì—°ì¥)ëœ ê±´ê°€ìš”?'\n",
      "2026-01-30 17:01:53,801 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:54,552 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:55,427 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:56,674 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:56,686 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=18 (threshold=0.2)\n",
      "2026-01-30 17:01:57,257 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:57,991 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:58,670 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:01:58,923 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:02:08,348 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:08,972 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:08,976 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ë¬µì‹œì ê°±ì‹ (ë¬µì‹œì ê°±ì‹ )ìœ¼ë¡œ ì—°ì¥ëœ ì¤„ ëª¨ë¥´ê³  ì‚´ì•˜ëŠ”ë°, ì£¼íƒì˜ì¸ë„(ì£¼íƒì˜ì¸ë„) ê°€ë ¤ë©´ ì–¸ì œê¹Œì§€ ì‚´ì•„ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-01-30 17:02:08,977 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ë¬µì‹œì ê°±ì‹ (ë¬µì‹œì ê°±ì‹ )ìœ¼ë¡œ ì—°ì¥ëœ ì¤„ ëª¨ë¥´ê³  ì‚´ì•˜ëŠ”ë°, ì£¼íƒì˜ì¸ë„(ì£¼íƒì˜ì¸ë„) ê°€ë ¤ë©´ ì–¸ì œê¹Œì§€ ì‚´ì•„ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-01-30 17:02:09,444 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:10,491 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:11,140 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:12,356 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:12,367 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=13 (threshold=0.2)\n",
      "2026-01-30 17:02:12,788 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:14,302 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:14,996 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:15,243 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:02:37,886 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:38,516 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:38,518 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì°¨ì„(ì¦ì•¡)ì„ 6ê°œì›” ì „ì— í–ˆëŠ”ë° ë˜ ì°¨ì„ì¦ì•¡í•˜ìê³  í•©ë‹ˆë‹¤. ë”°ë¼ì•¼ í•˜ë‚˜ìš”?\n",
      "2026-01-30 17:02:38,518 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì°¨ì„(ì¦ì•¡)ì„ 6ê°œì›” ì „ì— í–ˆëŠ”ë° ë˜ ì°¨ì„ì¦ì•¡í•˜ìê³  í•©ë‹ˆë‹¤. ë”°ë¼ì•¼ í•˜ë‚˜ìš”?'\n",
      "2026-01-30 17:02:39,017 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:39,954 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:40,626 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:41,878 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:41,892 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=18 (threshold=0.2)\n",
      "2026-01-30 17:02:42,410 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:43,492 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:44,421 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:44,679 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:02:54,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¦ 5/10 ì €ì¥ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:02:54,892 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:54,895 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ê²½ê¸°ë„ì— ì‚¬ëŠ”ë° ì„ì°¨ì£¼íƒì´ ê²½ë§¤ì ˆì°¨ë¡œ ë„˜ì–´ê°€ë©´, ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ ì¼ë¶€ë¼ë„ ìš°ì„ ë³€ì œê¶Œìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\n",
      "2026-01-30 17:02:54,896 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ê²½ê¸°ë„ì— ì‚¬ëŠ”ë° ì„ì°¨ì£¼íƒì´ ê²½ë§¤ì ˆì°¨ë¡œ ë„˜ì–´ê°€ë©´, ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ ì¼ë¶€ë¼ë„ ìš°ì„ ë³€ì œê¶Œìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?'\n",
      "2026-01-30 17:02:55,338 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:56,216 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:57,117 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:58,160 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:58,182 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=45 (threshold=0.2)\n",
      "2026-01-30 17:02:58,595 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:59,246 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:02:59,930 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:00,187 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:03:14,558 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:15,967 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:15,982 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì¡°ì • ì‹ ì²­ì€ ì–´ë””ì— í•˜ë‚˜ìš”? ë§ë¡œ í•´ë„ ë˜ë‚˜ìš”? ëˆì€ ê¼­ ë‚´ì•¼ í•˜ë‚˜ìš”?  \n",
      "â†’ ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ(ì¡°ì •ìœ„) ì‹ ì²­ì€ ì–´ë””ì— í•˜ë‚˜ìš”? ë§ë¡œ í•´ë„ ë˜ë‚˜ìš”? ëˆ(ì¤‘ê°œë³´ìˆ˜)ì€ ê¼­ ë‚´ì•¼ í•˜ë‚˜ìš”?  \n",
      "\n",
      "[ìˆ˜ì • í›„ ìµœì¢… ì¶œë ¥]  \n",
      "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ(ì¡°ì •ìœ„) ì‹ ì²­ì€ ì–´ë””ì— í•˜ë‚˜ìš”? ë§ë¡œ í•´ë„ ë˜ë‚˜ìš”? ëˆ(ì¤‘ê°œë³´ìˆ˜)ì€ ê¼­ ë‚´ì•¼ í•˜ë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : 'ëˆ'ì€ ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œëœ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë¬¸ë§¥ìƒ 'ì¤‘ê°œë³´ìˆ˜'ì™€ ì—°ê´€ë  ìˆ˜ ìˆì–´ ê´„í˜¸ ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
      "2026-01-30 17:03:15,982 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì¡°ì • ì‹ ì²­ì€ ì–´ë””ì— í•˜ë‚˜ìš”? ë§ë¡œ í•´ë„ ë˜ë‚˜ìš”? ëˆì€ ê¼­ ë‚´ì•¼ í•˜ë‚˜ìš”?  \n",
      "â†’ ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ(ì¡°ì •ìœ„) ì‹ ì²­ì€ ì–´ë””ì— í•˜ë‚˜ìš”? ë§ë¡œ í•´ë„ ë˜ë‚˜ìš”? ëˆ(ì¤‘ê°œë³´ìˆ˜)ì€ ê¼­ ë‚´ì•¼ í•˜ë‚˜ìš”?  \n",
      "\n",
      "[ìˆ˜ì • í›„ ìµœì¢… ì¶œë ¥]  \n",
      "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ(ì¡°ì •ìœ„) ì‹ ì²­ì€ ì–´ë””ì— í•˜ë‚˜ìš”? ë§ë¡œ í•´ë„ ë˜ë‚˜ìš”? ëˆ(ì¤‘ê°œë³´ìˆ˜)ì€ ê¼­ ë‚´ì•¼ í•˜ë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : 'ëˆ'ì€ ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œëœ ë§¤í•‘ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ë³€ê²½í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë¬¸ë§¥ìƒ 'ì¤‘ê°œë³´ìˆ˜'ì™€ ì—°ê´€ë  ìˆ˜ ìˆì–´ ê´„í˜¸ ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.'\n",
      "2026-01-30 17:03:16,628 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:17,507 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:18,333 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:19,920 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:19,940 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=60 (threshold=0.2)\n",
      "2026-01-30 17:03:20,511 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:21,202 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:22,039 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:22,286 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:03:33,664 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:34,788 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:34,793 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ê²½ë§¤ì ˆì°¨(ê¶Œë¦¬ë¦¬ìŠ¤í¬)ë¡œ ì„ì°¨ì£¼íƒì´ ë„˜ì–´ê°”ëŠ”ë°, êµ­ì„¸ ì²´ë‚©ìœ¼ë¡œ ê°€ì••ë¥˜(ìš°ì„ ë³€ì œê¶Œ)ê°€ ë¨¼ì € ê±¸ë ¤ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì„¸ê¸ˆ(ìš°ì„ ë³€ì œê¶Œ)ê³¼ ì œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ìš°ì„ ë³€ì œê¶Œ) ì¤‘ ë­ê°€ ìµœìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ìœ¼ë¡œ ë¨¼ì € ê°€ì ¸ê°€ë‚˜ìš”?\n",
      "2026-01-30 17:03:34,793 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ê²½ë§¤ì ˆì°¨(ê¶Œë¦¬ë¦¬ìŠ¤í¬)ë¡œ ì„ì°¨ì£¼íƒì´ ë„˜ì–´ê°”ëŠ”ë°, êµ­ì„¸ ì²´ë‚©ìœ¼ë¡œ ê°€ì••ë¥˜(ìš°ì„ ë³€ì œê¶Œ)ê°€ ë¨¼ì € ê±¸ë ¤ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì„¸ê¸ˆ(ìš°ì„ ë³€ì œê¶Œ)ê³¼ ì œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ìš°ì„ ë³€ì œê¶Œ) ì¤‘ ë­ê°€ ìµœìš°ì„ ë³€ì œê¶Œ(ìš°ì„ ë³€ì œê¶Œ)ìœ¼ë¡œ ë¨¼ì € ê°€ì ¸ê°€ë‚˜ìš”?'\n",
      "2026-01-30 17:03:35,264 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:36,427 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:37,339 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:38,491 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:38,510 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=42 (threshold=0.2)\n",
      "2026-01-30 17:03:38,928 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:39,891 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:40,723 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:40,997 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:03:49,973 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:50,873 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:50,873 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì „ì„¸(ì „ì„¸) ê³„ì•½í•˜ë ¤ëŠ” ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ì— ì„ ìˆœìœ„ ê·¼ì €ë‹¹ê¶Œ(ê·¼ì €ë‹¹ê¶Œ)ì€ ì—†ëŠ”ë°, ë‚˜ì¤‘ì— ë³´ë‹ˆ ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ êµ­ì„¸(êµ­ì„¸)ë¥¼ ì²´ë‚©(ì²´ë‚©)í•œ ìƒíƒœì˜€ìŠµë‹ˆë‹¤. ì•„ì§ ì••ë¥˜ë“±ê¸°(ì••ë¥˜ë“±ê¸°)ëŠ” ì—†ì—ˆëŠ”ë°, ì´ ê²½ìš°ì—ë„ ì œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì´ ìœ„í—˜í•œê°€ìš”?\n",
      "2026-01-30 17:03:50,889 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì „ì„¸(ì „ì„¸) ê³„ì•½í•˜ë ¤ëŠ” ì„ì°¨ì£¼íƒ(ì„ì°¨ì£¼íƒ)ì— ì„ ìˆœìœ„ ê·¼ì €ë‹¹ê¶Œ(ê·¼ì €ë‹¹ê¶Œ)ì€ ì—†ëŠ”ë°, ë‚˜ì¤‘ì— ë³´ë‹ˆ ì„ëŒ€ì¸(ì„ëŒ€ì¸)ì´ êµ­ì„¸(êµ­ì„¸)ë¥¼ ì²´ë‚©(ì²´ë‚©)í•œ ìƒíƒœì˜€ìŠµë‹ˆë‹¤. ì•„ì§ ì••ë¥˜ë“±ê¸°(ì••ë¥˜ë“±ê¸°)ëŠ” ì—†ì—ˆëŠ”ë°, ì´ ê²½ìš°ì—ë„ ì œ ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ(ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ)ì´ ìœ„í—˜í•œê°€ìš”?'\n",
      "2026-01-30 17:03:51,570 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:52,479 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:53,280 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:54,466 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:54,484 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=60 (threshold=0.2)\n",
      "2026-01-30 17:03:54,898 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:55,836 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:56,585 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:03:56,836 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:04:09,163 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:10,823 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:10,823 - chatbot_app.modules.rag_module - INFO - ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: ì „ì„¸í”¼í•´(ê¶Œë¦¬ë¦¬ìŠ¤í¬)ë¡œ ì•„ì§ ê³µì‹ ê²°ì • ì „ì¸ë° ê²½ë§¤ì ˆì°¨(ê²½ë§¤) ë§¤ê°ê¸°ì¼ì´ ì¡í˜”ìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ë„ ê²½ë§¤ì ˆì°¨(ê²½ë§¤)ë¥¼ ë©ˆì¶œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?  \n",
      "\n",
      "ë³€ê²½ëœ ì§ˆë¬¸:  \n",
      "ì „ì„¸í”¼í•´(ê¶Œë¦¬ë¦¬ìŠ¤í¬)ë¡œ ì•„ì§ ê³µì‹ ê²°ì • ì „ì¸ë° ê²½ë§¤ì ˆì°¨(ê²½ë§¤) ë§¤ê°ê¸°ì¼ì´ ì¡í˜”ìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ë„ ê²½ë§¤ì ˆì°¨(ê²½ë§¤)ë¥¼ ë©ˆì¶œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : 'ì „ì„¸ì‚¬ê¸°'ëŠ” ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œë˜ì§€ ì•Šì•„ ì›ë¬¸ì„ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì¶”ê°€ ë§¤í•‘ ìš”ì²­ ë°”ëë‹ˆë‹¤.\n",
      "2026-01-30 17:04:10,823 - chatbot_app.modules.rag_module - INFO - ğŸ” [Hybrid Retrieval] query='ì „ì„¸í”¼í•´(ê¶Œë¦¬ë¦¬ìŠ¤í¬)ë¡œ ì•„ì§ ê³µì‹ ê²°ì • ì „ì¸ë° ê²½ë§¤ì ˆì°¨(ê²½ë§¤) ë§¤ê°ê¸°ì¼ì´ ì¡í˜”ìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ë„ ê²½ë§¤ì ˆì°¨(ê²½ë§¤)ë¥¼ ë©ˆì¶œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?  \n",
      "\n",
      "ë³€ê²½ëœ ì§ˆë¬¸:  \n",
      "ì „ì„¸í”¼í•´(ê¶Œë¦¬ë¦¬ìŠ¤í¬)ë¡œ ì•„ì§ ê³µì‹ ê²°ì • ì „ì¸ë° ê²½ë§¤ì ˆì°¨(ê²½ë§¤) ë§¤ê°ê¸°ì¼ì´ ì¡í˜”ìŠµë‹ˆë‹¤. ì´ ê²½ìš°ì—ë„ ê²½ë§¤ì ˆì°¨(ê²½ë§¤)ë¥¼ ë©ˆì¶œ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?  \n",
      "\n",
      "â€» ì°¸ê³ : 'ì „ì„¸ì‚¬ê¸°'ëŠ” ìš©ì–´ ì‚¬ì „ì— ëª…ì‹œë˜ì§€ ì•Šì•„ ì›ë¬¸ì„ ìœ ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì¶”ê°€ ë§¤í•‘ ìš”ì²­ ë°”ëë‹ˆë‹¤.'\n",
      "2026-01-30 17:04:11,273 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:12,193 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:12,880 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:14,160 - httpx - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:04:14,225 - chatbot_app.modules.rag_module - INFO - ğŸ“Œ Rerank selected=60 (threshold=0.2)\n",
      "2026-01-30 17:04:14,685 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:15,382 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:16,112 - httpx - INFO - HTTP Request: POST https://api.upstage.ai/v1/solar/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:16,352 - chatbot_app.modules.rag_module - INFO - ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...\n",
      "2026-01-30 17:04:40,944 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¦ 10/10 ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "âœ… RAG ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: ./data/RAGAS/rag_run_cache_k5-5-3_m2_furrf_w0.60-0.40_tr0.35_rrf60_bm25okapi-k11.50-b0.75-mdc3000_rerankT.jsonl\n",
      "âœ… RUN_TAG: k5-5-3_m2_furrf_w0.60-0.40_tr0.35_rrf60_bm25okapi-k11.50-b0.75-mdc3000_rerankT\n",
      "CPU times: total: 5.66 s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# RAG ëª¨ë“ˆ import\n",
    "MODULES_PATH = os.path.join(os.getcwd(), \"chatbot_app\", \"modules\")\n",
    "sys.path.append(MODULES_PATH)\n",
    "sys.path.append(os.getcwd())\n",
    "from chatbot_app.modules.rag_module import create_pipeline, RAGConfig\n",
    "\n",
    "\n",
    "def make_run_tag(cfg: RAGConfig) -> str:\n",
    "    \"\"\"íŒŒì¼ëª…ì— 'íŠœë‹í•œ ê°’'ì´ ë³´ì´ë„ë¡ íƒœê·¸ë¥¼ ì¶©ë¶„íˆ í¬í•¨\"\"\"\n",
    "    rerank_flag = \"T\" if getattr(cfg, \"enable_rerank\", False) else \"F\"\n",
    "\n",
    "    # (ì„ íƒ) ì¡´ì¬í•  ìˆ˜ë„/ì—†ì„ ìˆ˜ë„ ìˆëŠ” í•„ë“œë“¤ì€ getattrë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼\n",
    "    fusion = getattr(cfg, \"hybrid_fusion\", \"rrf\")\n",
    "    dw = getattr(cfg, \"hybrid_dense_weight\", None)\n",
    "    sw = getattr(cfg, \"hybrid_sparse_weight\", None)\n",
    "    tr = getattr(cfg, \"hybrid_sparse_title_ratio\", None)\n",
    "    rrf_k = getattr(cfg, \"rrf_k\", None)\n",
    "\n",
    "    bm25_alg = getattr(cfg, \"bm25_algorithm\", None)\n",
    "    bm25_k1 = getattr(cfg, \"bm25_k1\", None)\n",
    "    bm25_b = getattr(cfg, \"bm25_b\", None)\n",
    "    bm25_mdc = getattr(cfg, \"bm25_max_doc_chars\", None)\n",
    "\n",
    "    # ì†Œìˆ˜ì /ê¸´ ê°’ë“¤ ë³´ê¸° ì¢‹ê²Œ ì¶•ì•½\n",
    "    def ffloat(x, nd=2):\n",
    "        if x is None:\n",
    "            return \"NA\"\n",
    "        try:\n",
    "            return f\"{float(x):.{nd}f}\"\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "\n",
    "    tag = (\n",
    "        f\"k{cfg.k_law}-{cfg.k_rule}-{cfg.k_case}\"\n",
    "        f\"_m{cfg.search_multiplier}\"\n",
    "        f\"_fu{fusion}\"\n",
    "        f\"_w{ffloat(dw)}-{ffloat(sw)}\"\n",
    "        f\"_tr{ffloat(tr)}\"\n",
    "        f\"_rrf{rrf_k if rrf_k is not None else 'NA'}\"\n",
    "        f\"_bm25{bm25_alg if bm25_alg is not None else 'NA'}\"\n",
    "        f\"-k1{ffloat(bm25_k1)}-b{ffloat(bm25_b)}-mdc{bm25_mdc if bm25_mdc is not None else 'NA'}\"\n",
    "        f\"_rerank{rerank_flag}\"\n",
    "    )\n",
    "    # íŒŒì¼ëª… ì•ˆì „ ì²˜ë¦¬(ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)\n",
    "    tag = re.sub(r\"[^0-9A-Za-z_\\-\\.]+\", \"\", tag)\n",
    "    return tag\n",
    "\n",
    "\n",
    "DOC_PATH = \"./data/RAGAS/RAGAS_ì§ˆë¬¸_10ê°œ_ì •ë¦¬ë³¸.docx\"\n",
    "\n",
    "\n",
    "def parse_ragas_docx(doc_path: str) -> List[Dict[str, Any]]:\n",
    "    doc = DocxDocument(doc_path)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]\n",
    "    text = \"\\n\".join(paras)\n",
    "\n",
    "    blocks = re.split(r\"\\n(?=\\d+\\.\\s)\", text)\n",
    "\n",
    "    items = []\n",
    "    for b in blocks:\n",
    "        b = b.strip()\n",
    "        if not re.match(r\"^\\d+\\.\\s\", b):\n",
    "            continue\n",
    "\n",
    "        parts = b.split(\"âœ”ï¸ ëª¨ë²”ë‹µì•ˆ\", 1)\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "\n",
    "        q_part = re.sub(r\"^\\d+\\.\\s*\", \"\", parts[0].strip())\n",
    "        a_part = parts[1].strip()\n",
    "\n",
    "        items.append({\"question\": q_part.strip(), \"ground_truth\": a_part.strip()})\n",
    "\n",
    "    if len(items) == 0:\n",
    "        raise ValueError(\"docxì—ì„œ ë¬¸í•­ì„ 1ê°œë„ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ í¬ë§·ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    return items\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 0) í™˜ê²½ë³€ìˆ˜ ì²´í¬\n",
    "    required_env = [\"PINECONE_API_KEY\", \"UPSTAGE_API_KEY\", \"OPENAI_API_KEY\"]\n",
    "    missing = [k for k in required_env if not os.getenv(k)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤: {missing}\")\n",
    "\n",
    "    # 1) ì§ˆë¬¸ ë¡œë“œ\n",
    "    items = parse_ragas_docx(DOC_PATH)\n",
    "    print(f\"âœ… íŒŒì‹±ëœ ë¬¸í•­ ìˆ˜: {len(items)}\")\n",
    "\n",
    "    # 2) íŒŒì´í”„ë¼ì¸ ìƒì„± (â­ íŠœë‹ í¬ì¸íŠ¸: ì•„ë˜ ê°’ë“¤ì„ ë°”ê¿”ê°€ë©° ì‹¤í—˜)\n",
    "    # - NOTE: ì—¬ê¸°ì„œ ë„˜ê¸°ëŠ” ê°’ì´ 'ìµœì¢… ì ìš©'ì…ë‹ˆë‹¤.\n",
    "    #         rag_module.py ì•ˆì—ì„œ ê¸°ë³¸ê°’ì„ ë°”ê¿”ë„, ì—¬ê¸°ì„œ k_law=5ë¡œ ì£¼ë©´ '5'ê°€ ì ìš©ë©ë‹ˆë‹¤.\n",
    "    cfg = RAGConfig(\n",
    "        # (1) Retrieval ì–‘/í›„ë³´í’€\n",
    "        k_law=5, # 5\n",
    "        k_rule=5, # 5\n",
    "        k_case=3, # 3\n",
    "        search_multiplier=2, # 2\n",
    "\n",
    "        # (2) Rerank (retrieval íŠœë‹ë§Œ ë³´ë©´ False ê¶Œì¥)\n",
    "        enable_rerank=True,\n",
    "\n",
    "        # (3) Hybrid / RRF (rag_module.pyì˜ RAGConfigì— í•´ë‹¹ í•„ë“œê°€ ìˆì„ ë•Œë§Œ ë™ì‘)\n",
    "        hybrid_fusion=\"rrf\",\n",
    "        hybrid_dense_weight=0.6, # 0.6\n",
    "        hybrid_sparse_weight=0.4, # 0.4\n",
    "        hybrid_sparse_title_ratio=0.35, # 0.35\n",
    "        rrf_k=60, # 60\n",
    "\n",
    "        # (4) BM25 (rag_module.pyì˜ RAGConfigì— í•´ë‹¹ í•„ë“œê°€ ìˆì„ ë•Œë§Œ ë™ì‘)\n",
    "        bm25_algorithm=\"okapi\",   # okapi / plus\n",
    "        bm25_k1=1.5, # 1.5\n",
    "        bm25_b=0.75, # 0.75\n",
    "        bm25_max_doc_chars=3000,\n",
    "        enable_bm25_title=True,\n",
    "    )\n",
    "\n",
    "    RUN_TAG = make_run_tag(cfg)\n",
    "    OUT_JSONL = f\"./data/RAGAS/rag_run_cache_{RUN_TAG}.jsonl\"\n",
    "\n",
    "    pipeline = create_pipeline(config=cfg)\n",
    "\n",
    "    # 3) ì¶œë ¥ í´ë” ë³´ì¥\n",
    "    os.makedirs(os.path.dirname(OUT_JSONL), exist_ok=True)\n",
    "\n",
    "    # 4) ìºì‹œ íŒŒì¼ë¡œ ì €ì¥ (jsonl: í•œ ì¤„ì— í•œ ìƒ˜í”Œ)\n",
    "    #    - ê° rowì— run_configë¥¼ ê°™ì´ ë„£ì–´ì„œ, íŒŒì¼ ë‚´ë¶€ë§Œ ë´ë„ ì–´ë–¤ íŠœë‹ì¸ì§€ ë°”ë¡œ ì•Œ ìˆ˜ ìˆê²Œ í•¨\n",
    "    #    - (ì£¼ì˜) RAGAS ì…ë ¥ìœ¼ë¡œ ì“¸ ë•Œ run_config ì»¬ëŸ¼ì€ ë¬´ì‹œë˜ê±°ë‚˜ ì œê±°í•´ë„ ë¨\n",
    "    try:\n",
    "        cfg_dict = cfg.model_dump()  # pydantic v2\n",
    "    except Exception:\n",
    "        try:\n",
    "            cfg_dict = cfg.dict()     # pydantic v1\n",
    "        except Exception:\n",
    "            cfg_dict = {k: getattr(cfg, k) for k in dir(cfg) if not k.startswith(\"_\")}\n",
    "\n",
    "    with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ex in enumerate(items, start=1):\n",
    "            q = ex[\"question\"]\n",
    "            gt = ex[\"ground_truth\"]\n",
    "\n",
    "            trace = pipeline.answer_with_trace(q, skip_normalization=False)\n",
    "            answer = trace.get(\"answer\", \"\") or \"\"\n",
    "            docs = trace.get(\"docs\", []) or []\n",
    "\n",
    "            contexts = []\n",
    "            for d in docs:\n",
    "                try:\n",
    "                    contexts.append(d.page_content)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            row = {\n",
    "                \"id\": i,\n",
    "                \"question\": q,\n",
    "                \"answer\": answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"reference\": gt,               # RAGASê°€ ìš”êµ¬í•˜ë˜ reference\n",
    "                \"ground_truths\": [gt],         # ë‹¤ë¥¸ ì§€í‘œìš©ìœ¼ë¡œë„ ìœ ì§€\n",
    "                \"normalized_query\": trace.get(\"normalized_query\", \"\"),\n",
    "\n",
    "                # âœ… íŠœë‹ ì¶”ì ìš©(íŒŒì¼ ë‚´ë¶€ì—ë„ ë‚¨ê¹€)\n",
    "                \"run_tag\": RUN_TAG,\n",
    "                \"run_config\": cfg_dict,\n",
    "            }\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                print(f\"â€¦ {i}/{len(items)} ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "    print(f\"\\nâœ… RAG ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {OUT_JSONL}\")\n",
    "    print(\"âœ… RUN_TAG:\", RUN_TAG)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c022555",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32885b9c",
   "metadata": {},
   "source": [
    "## í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e95a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ragas\n",
    "# print(\"RAGAS version:\", ragas.__version__)\n",
    "\n",
    "# print(\"\\nğŸ” Testing ContextRecall import methods:\")\n",
    "\n",
    "# # ë°©ë²• 1: collections (ê¶Œì¥)\n",
    "# try:\n",
    "#     from ragas.metrics.collections import ContextRecall\n",
    "#     print(\"âœ… collections import works:\", ContextRecall)\n",
    "# except Exception as e:\n",
    "#     print(\"âŒ collections import failed:\", e)\n",
    "\n",
    "# # ë°©ë²• 2: ragas.metrics (deprecated ê°€ëŠ¥)\n",
    "# try:\n",
    "#     from ragas.metrics import ContextRecall\n",
    "#     print(\"âœ… ragas.metrics import works (deprecated likely):\", ContextRecall)\n",
    "# except Exception as e:\n",
    "#     print(\"âŒ ragas.metrics import failed:\", e)\n",
    "\n",
    "# # ë°©ë²• 3: ë‚´ë¶€ ê²½ë¡œ(ë¹„ê¶Œì¥)\n",
    "# try:\n",
    "#     from ragas.metrics._context_recall import ContextRecall as _ContextRecall\n",
    "#     print(\"âœ… internal import works (not recommended):\", _ContextRecall)\n",
    "# except Exception as e:\n",
    "#     print(\"âŒ internal import failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3312162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import ragas\n",
    "# print(f\"RAGAS version: {ragas.__version__}\")\n",
    "\n",
    "# # ì–´ë–¤ import ë°©ë²•ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸\n",
    "# print(\"\\nğŸ” Testing import methods:\")\n",
    "\n",
    "# # ë°©ë²• 1: ë‚´ë¶€ í´ë˜ìŠ¤ ì§ì ‘ import (ê°€ì¥ í™•ì‹¤)\n",
    "# try:\n",
    "#     from ragas.metrics._answer_relevance import AnswerRelevancy\n",
    "#     print(\"âœ… Method 1 works: from ragas.metrics._answer_relevance import AnswerRelevancy\")\n",
    "#     print(f\"   Type: {type(AnswerRelevancy)}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Method 1 failed: {e}\")\n",
    "\n",
    "# # ë°©ë²• 2: ragas.metricsì—ì„œ ëŒ€ë¬¸ì í´ë˜ìŠ¤ import\n",
    "# try:\n",
    "#     from ragas.metrics import AnswerRelevancy\n",
    "#     print(\"âœ… Method 2 works: from ragas.metrics import AnswerRelevancy\")\n",
    "#     print(f\"   Type: {type(AnswerRelevancy)}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Method 2 failed: {e}\")\n",
    "\n",
    "# # ë°©ë²• 3: deprecated ë°©ë²•\n",
    "# try:\n",
    "#     from ragas.metrics import answer_relevancy\n",
    "#     print(f\"âœ… Method 3: from ragas.metrics import answer_relevancy\")\n",
    "#     print(f\"   Type: {type(answer_relevancy)}\")\n",
    "#     # ì´ê²Œ í´ë˜ìŠ¤ì¸ì§€ í™•ì¸\n",
    "#     if hasattr(answer_relevancy, '__call__'):\n",
    "#         print(\"   â†’ This is callable (good!)\")\n",
    "#     else:\n",
    "#         print(\"   â†’ This is a module (bad!)\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Method 3 failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390c1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ragas.metrics.collections as c\n",
    "# print([name for name in dir(c) if \"Context\" in name and \"Precision\" in name or \"Recall\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67fdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.metrics.collections import ContextRecall\n",
    "# print(\"ContextRecall symbol =\", ContextRecall)\n",
    "# print(\"type(ContextRecall) =\", type(ContextRecall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3da37",
   "metadata": {},
   "source": [
    "## ê³µí†µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651e7af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ragas_llm ready: <class 'ragas.llms.base.InstructorLLM'>\n",
      "âœ… ragas_embeddings ready: <class 'langchain_openai.embeddings.base.OpenAIEmbeddings'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:04:43,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding test successful! Dimension: 1536\n",
      "âœ… latest cache: ./data/RAGAS\\rag_run_cache_k5-5-3_m2_furrf_w0.60-0.40_tr0.35_rrf60_bm25okapi-k11.50-b0.75-mdc3000_rerankT.jsonl\n",
      "âœ… Normalized rows: 10\n",
      "CPU times: total: 1.44 s\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import gc  # ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from ragas.llms import llm_factory\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "ragas_llm = llm_factory(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    provider=\"openai\",\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "ragas_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"âœ… ragas_llm ready:\", type(ragas_llm))\n",
    "print(\"âœ… ragas_embeddings ready:\", type(ragas_embeddings))\n",
    "\n",
    "# âœ… embedding í…ŒìŠ¤íŠ¸\n",
    "try:\n",
    "    test_vec = ragas_embeddings.embed_query(\"í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"âœ… Embedding test successful! Dimension: {len(test_vec)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Embedding test failed: {e}\")\n",
    "\n",
    "\n",
    "# ìµœì‹  ìºì‹œ jsonl ì°¾ê¸°\n",
    "def find_latest_cache(pattern=\"./data/RAGAS/rag_run_cache_*.jsonl\") -> str:\n",
    "    candidates = glob.glob(pattern)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pattern}\")\n",
    "    return max(candidates, key=os.path.getmtime)\n",
    "\n",
    "CACHE_JSONL = find_latest_cache()\n",
    "print(\"âœ… latest cache:\", CACHE_JSONL)\n",
    "\n",
    "# jsonl ë¡œë“œ\n",
    "def load_jsonl(path: str):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_rows = load_jsonl(CACHE_JSONL)\n",
    "\n",
    "# ì»¬ëŸ¼ëª… í‘œì¤€í™” + ë°ì´í„° ê²€ì¦ ê°•í™”\n",
    "def normalize_row_schema(rows):\n",
    "    out = []\n",
    "    for i, r in enumerate(rows):\n",
    "        rr = dict(r)\n",
    "\n",
    "        # question/user_input í†µì¼\n",
    "        if \"question\" not in rr and \"user_input\" in rr:\n",
    "            rr[\"question\"] = rr[\"user_input\"]\n",
    "        \n",
    "        # âœ… questionì´ ë¹„ì–´ìˆê±°ë‚˜ Noneì´ë©´ ìŠ¤í‚µ\n",
    "        if not rr.get(\"question\"):\n",
    "            print(f\"âš ï¸ Warning: Row {i} has empty question, skipping\")\n",
    "            continue\n",
    "\n",
    "        # âœ… answerê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •\n",
    "        if not rr.get(\"answer\"):\n",
    "            print(f\"âš ï¸ Warning: Row {i} has empty answer, using default\")\n",
    "            rr[\"answer\"] = \"ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "        # contexts í†µì¼\n",
    "        if \"contexts\" not in rr and \"retrieved_contexts\" in rr:\n",
    "            rr[\"contexts\"] = rr[\"retrieved_contexts\"]\n",
    "        \n",
    "        # âœ… contextsê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”\n",
    "        if \"contexts\" not in rr:\n",
    "            rr[\"contexts\"] = []\n",
    "        \n",
    "        # âœ… contextsê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë³€í™˜\n",
    "        if not isinstance(rr.get(\"contexts\"), list):\n",
    "            rr[\"contexts\"] = [str(rr[\"contexts\"])] if rr[\"contexts\"] else []\n",
    "\n",
    "        # reference í†µì¼\n",
    "        if \"reference\" not in rr:\n",
    "            gts = rr.get(\"ground_truths\", None)\n",
    "            if isinstance(gts, list) and len(gts) > 0:\n",
    "                rr[\"reference\"] = gts[0]\n",
    "            elif \"ground_truth\" in rr:\n",
    "                rr[\"reference\"] = rr[\"ground_truth\"]\n",
    "            else:\n",
    "                rr[\"reference\"] = \"\"  # âœ… ê¸°ë³¸ê°’ ì„¤ì •\n",
    "\n",
    "        out.append(rr)\n",
    "    return out\n",
    "\n",
    "rows = normalize_row_schema(raw_rows)\n",
    "print(f\"âœ… Normalized rows: {len(rows)}\")\n",
    "\n",
    "# âœ… ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€\n",
    "def validate_data(rows, required_fields):\n",
    "    \"\"\"ë°ì´í„° ê²€ì¦ ë° ë¬¸ì œ ë¦¬í¬íŠ¸\"\"\"\n",
    "    issues = []\n",
    "    for i, r in enumerate(rows):\n",
    "        for field in required_fields:\n",
    "            if field not in r:\n",
    "                issues.append(f\"Row {i}: missing field '{field}'\")\n",
    "            elif r[field] is None:\n",
    "                issues.append(f\"Row {i}: field '{field}' is None\")\n",
    "            elif field in [\"question\", \"answer\"] and not str(r[field]).strip():\n",
    "                issues.append(f\"Row {i}: field '{field}' is empty\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"âš ï¸ Data validation issues:\")\n",
    "        for issue in issues[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
    "            print(f\"  - {issue}\")\n",
    "        if len(issues) > 10:\n",
    "            print(f\"  ... and {len(issues) - 10} more issues\")\n",
    "    else:\n",
    "        print(\"âœ… Data validation passed\")\n",
    "    \n",
    "    return len(issues) == 0\n",
    "\n",
    "# âœ… ì»¨í…ìŠ¤íŠ¸ í´ë¦½ ê°œì„ \n",
    "def clip_rows(rows, *, max_ctx=3, max_chars=600, clip_reference_chars=None, clip_answer_chars=None):\n",
    "    clipped = []\n",
    "    for r in rows:\n",
    "        rr = dict(r)\n",
    "\n",
    "        ctx = rr.get(\"contexts\", []) or []\n",
    "        # âœ… None ê°’ í•„í„°ë§ ì¶”ê°€\n",
    "        ctx = [c for c in ctx if c is not None]\n",
    "        rr[\"contexts\"] = [str(c)[:max_chars] for c in ctx[:max_ctx]]\n",
    "\n",
    "        if clip_reference_chars is not None and rr.get(\"reference\"):\n",
    "            rr[\"reference\"] = str(rr[\"reference\"])[:clip_reference_chars]\n",
    "\n",
    "        if clip_answer_chars is not None and rr.get(\"answer\"):\n",
    "            rr[\"answer\"] = str(rr[\"answer\"])[:clip_answer_chars]\n",
    "\n",
    "        clipped.append(rr)\n",
    "    return clipped\n",
    "\n",
    "# âœ… ì €ì¥ í´ë”\n",
    "OUT_DIR = \"./data/RAGAS\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def save_result(result, metric_name: str) -> str:\n",
    "    \"\"\"\n",
    "    result: ragas evaluate result\n",
    "    metric_name: íŒŒì¼ëª…ì— ë°•ì„ ì´ë¦„\n",
    "    \"\"\"\n",
    "    df = result.to_pandas()\n",
    "    \n",
    "    # âœ… ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“Š {metric_name} Results:\")\n",
    "    print(f\"  Mean: {df[metric_name].mean():.4f}\")\n",
    "    print(f\"  Std: {df[metric_name].std():.4f}\")\n",
    "    print(f\"  Min: {df[metric_name].min():.4f}\")\n",
    "    print(f\"  Max: {df[metric_name].max():.4f}\")\n",
    "    print(f\"  NaN count: {df[metric_name].isna().sum()}\")\n",
    "    \n",
    "    out_path = os.path.join(OUT_DIR, f\"ragas_result_{metric_name}.csv\")\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"âœ… saved: {out_path}\\n\")\n",
    "    return out_path\n",
    "\n",
    "# âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜\n",
    "def cleanup_memory():\n",
    "    \"\"\"ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f864ed6",
   "metadata": {},
   "source": [
    "## context_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a9f51e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Validating data for context_precision...\n",
      "âœ… Data validation passed\n",
      "â–¶ context_precision evaluate start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038f0bb1a6964389a454f2b0e64e13f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0426bd49e43946d89aad161b3b3dd6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:04:46,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:49,235 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:52,020 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:54,616 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:04:58,437 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:02,047 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:04,583 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:06,641 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:08,876 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:12,720 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:14,924 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:17,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:20,085 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:24,105 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:28,375 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:30,822 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:33,417 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:36,835 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:39,733 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:42,405 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:45,367 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:48,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:51,504 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:55,550 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:05:58,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:01,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:04,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:06,527 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:08,904 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:11,278 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… context_precision done: {'context_precision': 1.0000}\n",
      "\n",
      "ğŸ“Š context_precision Results:\n",
      "  Mean: 1.0000\n",
      "  Std: 0.0000\n",
      "  Min: 1.0000\n",
      "  Max: 1.0000\n",
      "  NaN count: 0\n",
      "âœ… saved: ./data/RAGAS\\ragas_result_context_precision.csv\n",
      "\n",
      "ğŸ§¹ Memory cleaned\n",
      "CPU times: total: 1.77 s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ragas import evaluate\n",
    "from ragas.metrics._context_precision import ContextPrecision\n",
    "\n",
    "print(\"ğŸ“‹ Validating data for context_precision...\")\n",
    "validate_data(rows, required_fields=[\"question\", \"contexts\"])\n",
    "\n",
    "rows_cp = clip_rows(rows, max_ctx=3, max_chars=600)\n",
    "\n",
    "eval_ds_cp = Dataset.from_list(rows_cp)\n",
    "\n",
    "print(\"â–¶ context_precision evaluate start\")\n",
    "try:\n",
    "    res_cp = evaluate(\n",
    "        dataset=eval_ds_cp,\n",
    "        metrics=[ContextPrecision()],\n",
    "        llm=ragas_llm,\n",
    "        embeddings=ragas_embeddings,\n",
    "        show_progress=True,\n",
    "        raise_exceptions=True, \n",
    "        batch_size=1,\n",
    "    )\n",
    "    print(\"âœ… context_precision done:\", res_cp)\n",
    "    cp_csv = save_result(res_cp, \"context_precision\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67ad18",
   "metadata": {},
   "source": [
    "## context_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278d7bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Validating data for context_recall...\n",
      "âœ… Data validation passed\n",
      "â–¶ context_recall evaluate start (ìƒ˜í”Œ 10ê°œ)\n",
      "metric_cr = ContextRecall(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'user_input', 'reference', 'retrieved_contexts'}}, name='context_recall', llm=None, output_type=<MetricOutputType.CONTINUOUS: 'continuous'>, context_recall_prompt=ContextRecallClassificationPrompt(instruction=Given a context, and an answer, analyze each sentence in the answer and classify if the sentence can be attributed to the given context or not. Use only 'Yes' (1) or 'No' (0) as a binary classification. Output json with reason., examples=[(QCA(question='What can you tell me about albert Albert Einstein?', context=\"Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass-energy equivalence formula E = mc2, which arises from relativity theory, has been called 'the world's most famous equation'. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.\", answer='Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895.'), ContextRecallClassifications(classifications=[ContextRecallClassification(statement='Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.', reason='The date of birth of Einstein is mentioned clearly in the context.', attributed=1), ContextRecallClassification(statement='He received the 1921 Nobel Prize in Physics for his services to theoretical physics.', reason='The exact sentence is present in the given context.', attributed=1), ContextRecallClassification(statement='He published 4 papers in 1905.', reason='There is no mention about papers he wrote in the given context.', attributed=0), ContextRecallClassification(statement='Einstein moved to Switzerland in 1895.', reason='There is no supporting evidence for this in the given context.', attributed=0)]))], language=english), max_retries=1) | type = <class 'ragas.metrics._context_recall.ContextRecall'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd53dcef5564a268a57a0869c52cf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd99172456ee4087b4ef38b59882798a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:06:22,063 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:33,397 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:46,926 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:06:58,907 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:07:09,971 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:07:24,844 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:07:37,007 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:07:50,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:08:00,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-30 17:08:10,854 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… context_recall done: {'context_recall': 0.6476}\n",
      "\n",
      "ğŸ“Š context_recall Results:\n",
      "  Mean: 0.6476\n",
      "  Std: 0.3213\n",
      "  Min: 0.0000\n",
      "  Max: 0.9091\n",
      "  NaN count: 0\n",
      "âœ… saved: ./data/RAGAS\\ragas_result_context_recall.csv\n",
      "\n",
      "ğŸ§¹ Memory cleaned\n",
      "CPU times: total: 1.03 s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ragas import evaluate\n",
    "from ragas.metrics._context_recall import ContextRecall\n",
    "\n",
    "print(\"ğŸ“‹ Validating data for context_recall...\")\n",
    "validate_data(rows, required_fields=[\"question\", \"contexts\", \"reference\"])\n",
    "\n",
    "# ë„ˆë¬´ ë¬´ê±°ìš°ë©´ ìƒ˜í”Œë§Œ\n",
    "SAMPLE_SIZE = 10  \n",
    "rows_cr = clip_rows(\n",
    "    rows[:SAMPLE_SIZE],  \n",
    "    max_ctx=2,\n",
    "    max_chars=200,  \n",
    "    clip_reference_chars=300, \n",
    "    clip_answer_chars=300,  # optional\n",
    ")\n",
    "\n",
    "eval_ds_cr = Dataset.from_list(rows_cr)\n",
    "\n",
    "print(f\"â–¶ context_recall evaluate start (ìƒ˜í”Œ {SAMPLE_SIZE}ê°œ)\")\n",
    "try:\n",
    "    metric_cr = ContextRecall()  \n",
    "    print(\"metric_cr =\", metric_cr, \"| type =\", type(metric_cr))\n",
    "\n",
    "    res_cr = evaluate(\n",
    "        dataset=eval_ds_cr,\n",
    "        metrics=[metric_cr],          \n",
    "        llm=ragas_llm,                \n",
    "        embeddings=ragas_embeddings,  \n",
    "        show_progress=True,\n",
    "        raise_exceptions=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    print(\"âœ… context_recall done:\", res_cr)\n",
    "    cr_csv = save_result(res_cr, \"context_recall\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ccf34a",
   "metadata": {},
   "source": [
    "## answer_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47da567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics._answer_relevance import AnswerRelevancy\n",
    "\n",
    "# print(\"ğŸ“‹ Validating data for answer_relevancy...\")\n",
    "# validate_data(rows, required_fields=[\"question\", \"answer\", \"contexts\"])\n",
    "\n",
    "# rows_ar = clip_rows(rows, max_ctx=0, max_chars=0, clip_answer_chars=1000)\n",
    "# for r in rows_ar:\n",
    "#     r[\"contexts\"] = []\n",
    "\n",
    "# eval_ds_ar = Dataset.from_list(rows_ar)\n",
    "\n",
    "# print(\"\\nâ–¶ answer_relevancy evaluate start\")\n",
    "# try:\n",
    "#     res_ar = evaluate(\n",
    "#         dataset=eval_ds_ar,\n",
    "#         metrics=[AnswerRelevancy()],\n",
    "#         llm=ragas_llm,\n",
    "#         embeddings=ragas_embeddings,  # âœ… LangChain embeddings\n",
    "#         show_progress=True,\n",
    "#         raise_exceptions=True,\n",
    "#         batch_size=1,\n",
    "#     )\n",
    "#     print(\"âœ… answer_relevancy done:\", res_ar)\n",
    "#     ar_csv = save_result(res_ar, \"answer_relevancy\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Error: {str(e)}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "# cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a1bf9",
   "metadata": {},
   "source": [
    "## faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ed34dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics._faithfulness import Faithfulness\n",
    "\n",
    "# print(\"ğŸ“‹ Validating data for faithfulness...\")\n",
    "# validate_data(rows, required_fields=[\"question\", \"answer\", \"contexts\"])\n",
    "\n",
    "# SAMPLE_SIZE = 5  # âœ… 5ê°œë§Œ ì¶”ì²œ\n",
    "# rows_fa = clip_rows(\n",
    "#     rows[:SAMPLE_SIZE],\n",
    "#     max_ctx=2,\n",
    "#     max_chars=400,  # âœ… ëŠ˜ë¦¼\n",
    "#     clip_answer_chars=800  # âœ… ëŠ˜ë¦¼\n",
    "# )\n",
    "\n",
    "# eval_ds_fa = Dataset.from_list(rows_fa)\n",
    "\n",
    "# print(f\"â–¶ faithfulness evaluate start (ìƒ˜í”Œ {SAMPLE_SIZE}ê°œ)\")\n",
    "# try:\n",
    "#     res_fa = evaluate(\n",
    "#         dataset=eval_ds_fa,\n",
    "#         metrics=[Faithfulness()],\n",
    "#         llm=ragas_llm,\n",
    "#         embeddings=ragas_embeddings,\n",
    "#         show_progress=True,\n",
    "#         raise_exceptions=True,\n",
    "#         batch_size=1,\n",
    "#     )\n",
    "#     print(\"âœ… faithfulness done:\", res_fa)\n",
    "#     fa_csv = save_result(res_fa, \"faithfulness_sample\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Error: {str(e)}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "# cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d3de9",
   "metadata": {},
   "source": [
    "# ê²°ê³¼ í•©ì‚°\n",
    "## ì „ì²´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1dc214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # [ì…€ 6/6] ë§ˆì§€ë§‰: ê²°ê³¼ í•©ì‚°(ê°€ëŠ¥í•œ ì§€í‘œ ìë™ ë³‘í•©) + ì´ì  ê³„ì‚° + ì €ì¥\n",
    "# # =========================\n",
    "# import pandas as pd\n",
    "\n",
    "# # ê° ê²°ê³¼ CSVë¥¼ ë‹¤ì‹œ ë¡œë“œ\n",
    "# df_cp = pd.read_csv(cp_csv)  # contains context_precision\n",
    "# df_cr = pd.read_csv(cr_csv)  # contains context_recall\n",
    "# df_ar = pd.read_csv(ar_csv# =========================\n",
    "# # [ì…€ 6/6] ë§ˆì§€ë§‰: ê²°ê³¼ í•©ì‚°(ì¡´ì¬í•˜ëŠ” ì§€í‘œë§Œ ìë™ ë³‘í•©) + ì´ì  ê³„ì‚° + ì €ì¥\n",
    "# # - ì–´ë–¤ ì§€í‘œë¥¼ ì¼ë¶€ë§Œ ëŒë ¤ë„(ì˜ˆ: answer_relevancyë§Œ) ì—ëŸ¬ ì—†ì´ í•©ì‚°ë˜ë„ë¡ ìˆ˜ì •\n",
    "# # - ./data/RAGAS í´ë”ì˜ ragas_result_*.csv ë¥¼ ìë™ íƒìƒ‰\n",
    "# # =========================\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# # âœ… ì €ì¥ í´ë” (ê³µí†µë¶€ë¶„ì—ì„œ OUT_DIRë¥¼ ë§Œë“¤ì—ˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)\n",
    "# OUT_DIR = globals().get(\"OUT_DIR\", \"./data/RAGAS\")\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# # âœ… ìœ í‹¸: íŒŒì¼ ì¡´ì¬ ì²´í¬ + ë¡œë“œ\n",
    "# def safe_read_csv(path: str):\n",
    "#     if path and os.path.exists(path):\n",
    "#         return pd.read_csv(path)\n",
    "#     return None\n",
    "\n",
    "# # âœ… ìœ í‹¸: id ì—†ìœ¼ë©´ ìƒì„± (mergeìš©)\n",
    "# def ensure_id(df: pd.DataFrame):\n",
    "#     if df is None:\n",
    "#         return None\n",
    "#     if \"id\" not in df.columns:\n",
    "#         df = df.copy()\n",
    "#         df[\"id\"] = range(1, len(df) + 1)\n",
    "#     return df\n",
    "\n",
    "# # âœ… (ê¶Œì¥) ì´ì „ ì…€ì—ì„œ cp_csv/cr_csv/ar_csv/fa_csv ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "# cp_path = globals().get(\"cp_csv\", None)\n",
    "# cr_path = globals().get(\"cr_csv\", None)\n",
    "# ar_path = globals().get(\"ar_csv\", None)\n",
    "# fa_path = globals().get(\"fa_csv\", None)\n",
    "\n",
    "# # âœ… ì—†ìœ¼ë©´ í´ë”ì—ì„œ ìë™ íƒìƒ‰ (ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ)\n",
    "# def pick_latest(pattern: str):\n",
    "#     cands = glob.glob(os.path.join(OUT_DIR, pattern))\n",
    "#     if not cands:\n",
    "#         return None\n",
    "#     return max(cands, key=os.path.getmtime)\n",
    "\n",
    "# if cp_path is None:\n",
    "#     cp_path = pick_latest(\"ragas_result_context_precision*.csv\")\n",
    "# if cr_path is None:\n",
    "#     cr_path = pick_latest(\"ragas_result_context_recall*.csv\")\n",
    "# if ar_path is None:\n",
    "#     ar_path = pick_latest(\"ragas_result_answer_relevancy*.csv\")\n",
    "# if fa_path is None:\n",
    "#     fa_path = pick_latest(\"ragas_result_faithfulness*.csv\")\n",
    "\n",
    "# print(\"ğŸ” Using result files:\")\n",
    "# print(\" - context_precision:\", cp_path)\n",
    "# print(\" - context_recall   :\", cr_path)\n",
    "# print(\" - answer_relevancy :\", ar_path)\n",
    "# print(\" - faithfulness     :\", fa_path)\n",
    "\n",
    "# df_cp = ensure_id(safe_read_csv(cp_path))\n",
    "# df_cr = ensure_id(safe_read_csv(cr_path))\n",
    "# df_ar = ensure_id(safe_read_csv(ar_path))\n",
    "# df_fa = ensure_id(safe_read_csv(fa_path))\n",
    "\n",
    "# # âœ… ê¸°ì¤€ DF ë§Œë“¤ê¸°: \"ê°€ì¥ í™•ì‹¤íˆ ì¡´ì¬í•˜ëŠ” ê²ƒ\"ë¶€í„° ì‹œì‘\n",
    "# # ìš°ì„ ìˆœìœ„: cp > cr > ar > fa\n",
    "# df_all = None\n",
    "# for candidate in [df_cp, df_cr, df_ar, df_fa]:\n",
    "#     if candidate is not None:\n",
    "#         df_all = candidate.copy()\n",
    "#         break\n",
    "\n",
    "# if df_all is None:\n",
    "#     raise FileNotFoundError(f\"ê²°ê³¼ CSVë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. OUT_DIR={OUT_DIR} ë‚´ ragas_result_*.csv ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# # âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì•ˆì „í•˜ê²Œ merge\n",
    "# def safe_merge(base: pd.DataFrame, other: pd.DataFrame, cols: list):\n",
    "#     if other is None:\n",
    "#         return base\n",
    "#     keep = [c for c in ([\"id\"] + cols) if c in other.columns]\n",
    "#     if \"id\" not in keep:\n",
    "#         return base\n",
    "#     return base.merge(other[keep], on=\"id\", how=\"left\")\n",
    "\n",
    "# df_all = safe_merge(df_all, df_cp, [\"context_precision\"])\n",
    "# df_all = safe_merge(df_all, df_cr, [\"context_recall\"])\n",
    "# df_all = safe_merge(df_all, df_ar, [\"answer_relevancy\"])\n",
    "# df_all = safe_merge(df_all, df_fa, [\"faithfulness\"])\n",
    "\n",
    "# # âœ… Retrieval score R (ê°€ëŠ¥í•  ë•Œë§Œ)\n",
    "# if all(c in df_all.columns for c in [\"context_recall\", \"context_precision\"]):\n",
    "#     df_all[\"R\"] = 0.6 * df_all[\"context_recall\"] + 0.4 * df_all[\"context_precision\"]\n",
    "\n",
    "# # âœ… Generation score G (ê°€ëŠ¥í•  ë•Œë§Œ)\n",
    "# # - faithfulnessê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì‚¬ìš©\n",
    "# # - ì—†ìœ¼ë©´ answer_relevancyë§Œ ì‚¬ìš©\n",
    "# if \"faithfulness\" in df_all.columns and \"answer_relevancy\" in df_all.columns:\n",
    "#     df_all[\"G\"] = 0.6 * df_all[\"faithfulness\"] + 0.4 * df_all[\"answer_relevancy\"]\n",
    "# elif \"answer_relevancy\" in df_all.columns:\n",
    "#     df_all[\"G\"] = df_all[\"answer_relevancy\"]\n",
    "\n",
    "# # âœ… Total (ê°€ëŠ¥í•œ êµ¬ì„±ìœ¼ë¡œ ìë™ ê³„ì‚°)\n",
    "# # ê¸°ë³¸ ê°€ì¤‘ì¹˜:\n",
    "# # - R(ê²€ìƒ‰ í’ˆì§ˆ) + G(ë‹µë³€ í’ˆì§ˆ)\n",
    "# # - Rì´ ì—†ìœ¼ë©´ Gë§Œ\n",
    "# # - Gë„ ì—†ìœ¼ë©´(ì´ë¡ ìƒ ì—†ìŒ) NaN\n",
    "# if \"R\" in df_all.columns and \"G\" in df_all.columns:\n",
    "#     df_all[\"Total\"] = 0.55 * df_all[\"R\"] + 0.45 * df_all[\"G\"]\n",
    "# elif \"G\" in df_all.columns:\n",
    "#     df_all[\"Total\"] = df_all[\"G\"]\n",
    "\n",
    "# # âœ… ì €ì¥\n",
    "# out_path_all = os.path.join(OUT_DIR, \"ragas_merged_all_scores.csv\")\n",
    "# df_all.to_csv(out_path_all, index=False, encoding=\"utf-8\")\n",
    "# print(\"âœ… merged saved:\", out_path_all)\n",
    "\n",
    "# # âœ… ìš”ì•½ ì¶œë ¥\n",
    "# summary_cols = [c for c in [\"context_precision\",\"context_recall\",\"answer_relevancy\",\"faithfulness\",\"R\",\"G\",\"Total\"] if c in df_all.columns]\n",
    "# print(\"\\nğŸ“Œ mean summary\")\n",
    "# print(df_all[summary_cols].mean(numeric_only=True))\n",
    "# )  # contains answer_relevancy\n",
    "# # faithfulnessëŠ” ìƒ˜í”Œ ê²°ê³¼ë¼ ì „ì²´ ë³‘í•©ì— ì§ì ‘ ì“°ê¸° ì• ë§¤ (ì›í•˜ë©´ idë¡œ merge ê°€ëŠ¥)\n",
    "# df_fa = pd.read_csv(fa_csv)  # faithfulness_sample\n",
    "\n",
    "# # ê³µí†µí‚¤: ë³´í†µ 'id'ê°€ ìˆìŒ. ì—†ìœ¼ë©´ indexë¡œ í•©ì¹¨.\n",
    "# def ensure_id(df):\n",
    "#     if \"id\" not in df.columns:\n",
    "#         df = df.copy()\n",
    "#         df[\"id\"] = range(1, len(df) + 1)\n",
    "#     return df\n",
    "\n",
    "# df_cp = ensure_id(df_cp)\n",
    "# df_cr = ensure_id(df_cr)\n",
    "# df_ar = ensure_id(df_ar)\n",
    "\n",
    "# # ë³‘í•© (id ê¸°ì¤€)\n",
    "# df_all = df_cp.merge(df_cr[[\"id\", \"context_recall\"]], on=\"id\", how=\"left\")\n",
    "# df_all = df_all.merge(df_ar[[\"id\", \"answer_relevancy\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# # Retrieval score (í•­ìƒ ê°€ëŠ¥)\n",
    "# if all(c in df_all.columns for c in [\"context_recall\", \"context_precision\"]):\n",
    "#     df_all[\"R\"] = 0.6 * df_all[\"context_recall\"] + 0.4 * df_all[\"context_precision\"]\n",
    "\n",
    "# # -----------------------------\n",
    "# # Case 1) faithfulness ìˆëŠ” ê²½ìš° (ìƒ˜í”Œì´ë©´ ì£¼ì˜)\n",
    "# # -----------------------------\n",
    "# # df_faëŠ” ìƒ˜í”Œ(SAMPLE_N)ë§Œ ë“¤ì–´ìˆì„ ìˆ˜ ìˆìŒ â†’ ì›í•˜ë©´ ì•„ë˜ì²˜ëŸ¼ idë¡œ ë³‘í•©\n",
    "# df_fa = ensure_id(df_fa)\n",
    "# if \"faithfulness\" in df_fa.columns:\n",
    "#     df_all = df_all.merge(df_fa[[\"id\", \"faithfulness\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Total ì ìˆ˜(2ë²„ì „)\n",
    "# # -----------------------------\n",
    "# # (A) faithfulnessê°€ ìˆëŠ” í–‰ë§Œ: G_with_faith / Total_with_faith\n",
    "# if all(c in df_all.columns for c in [\"faithfulness\", \"answer_relevancy\", \"R\"]):\n",
    "#     df_all[\"G_with_faith\"] = 0.6 * df_all[\"faithfulness\"] + 0.4 * df_all[\"answer_relevancy\"]\n",
    "#     df_all[\"Total_with_faith\"] = 0.55 * df_all[\"R\"] + 0.45 * df_all[\"G_with_faith\"]\n",
    "\n",
    "# # (B) faithfulness ì—†ëŠ” ê²½ìš°: answer_relevancy ë‹¨ë… + Retrieval ë¹„ì¤‘â†‘\n",
    "# # if all(c in df_all.columns for c in [\"answer_relevancy\", \"R\"]):\n",
    "# #     df_all[\"G_no_faith\"] = df_all[\"answer_relevancy\"]\n",
    "# #     df_all[\"Total_no_faith\"] = 0.65 * df_all[\"R\"] + 0.35 * df_all[\"G_no_faith\"]\n",
    "\n",
    "# # ì €ì¥\n",
    "# out_path_all = os.path.join(OUT_DIR, \"ragas_merged_all_scores.csv\")\n",
    "# df_all.to_csv(out_path_all, index=False, encoding=\"utf-8\")\n",
    "# print(\"âœ… merged saved:\", out_path_all)\n",
    "\n",
    "# # ìš”ì•½ ì¶œë ¥\n",
    "# summary_cols = [c for c in [\"context_precision\",\"context_recall\",\"answer_relevancy\",\"faithfulness\",\"R\",\"Total_with_faith\",\"Total_no_faith\"] if c in df_all.columns]\n",
    "# print(\"\\nğŸ“Œ mean summary\")\n",
    "# print(df_all[summary_cols].mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f95ce3e",
   "metadata": {},
   "source": [
    "## R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a04a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Per-question summary (recall/precision/R_i)\n",
      "       context_recall  context_precision        R_i\n",
      "count       10.000000       1.000000e+01  10.000000\n",
      "mean         0.647612       1.000000e+00   0.788567\n",
      "std          0.321305       1.170278e-16   0.192783\n",
      "min          0.000000       1.000000e+00   0.400000\n",
      "25%          0.496429       1.000000e+00   0.697857\n",
      "50%          0.775000       1.000000e+00   0.865000\n",
      "75%          0.889286       1.000000e+00   0.933571\n",
      "max          0.909091       1.000000e+00   0.945455\n",
      "\n",
      "ğŸ“Œ R_i quantiles\n",
      "0.00    0.400000\n",
      "0.10    0.520000\n",
      "0.25    0.697857\n",
      "0.50    0.865000\n",
      "0.75    0.933571\n",
      "0.90    0.945455\n",
      "1.00    0.945455\n",
      "Name: R_i, dtype: float64\n",
      "\n",
      "ğŸ“Š R_i histogram (10 bins)\n",
      "0.400 ~ 0.455 |  1 â–ˆ\n",
      "0.455 ~ 0.509 |  0 \n",
      "0.509 ~ 0.564 |  1 â–ˆ\n",
      "0.564 ~ 0.618 |  0 \n",
      "0.618 ~ 0.673 |  1 â–ˆ\n",
      "0.673 ~ 0.727 |  0 \n",
      "0.727 ~ 0.782 |  0 \n",
      "0.782 ~ 0.836 |  1 â–ˆ\n",
      "0.836 ~ 0.891 |  2 â–ˆâ–ˆ\n",
      "0.891 ~ 0.945 |  4 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ğŸ”» Lowest R_i (top 10)\n",
      "   id  context_recall  context_precision       R_i\n",
      "4   5        0.000000                1.0  0.400000\n",
      "6   7        0.222222                1.0  0.533333\n",
      "8   9        0.428571                1.0  0.657143\n",
      "7   8        0.700000                1.0  0.820000\n",
      "9  10        0.750000                1.0  0.850000\n",
      "0   1        0.800000                1.0  0.880000\n",
      "1   2        0.857143                1.0  0.914286\n",
      "3   4        0.900000                1.0  0.940000\n",
      "5   6        0.909091                1.0  0.945455\n",
      "2   3        0.909091                1.0  0.945455\n",
      "\n",
      "ğŸš¨ recall == 0 rows\n",
      "   id  context_recall  context_precision  R_i\n",
      "4   5             0.0                1.0  0.4\n",
      "\n",
      "ğŸ’¾ saved: ./data/RAGAS\\ragas_Ri_per_question.csv\n"
     ]
    }
   ],
   "source": [
    "# matplotlib ì—†ì´: ì§ˆë¬¸ë³„ recall/precision merge â†’ R_i ê³„ì‚° â†’ ë¶„í¬/ìš”ì•½/í•˜ìœ„ë¬¸í•­/recall=0 ì¶”ì¶œ + ì €ì¥\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"./data/RAGAS\"\n",
    "recall_csv = os.path.join(OUT_DIR, \"ragas_result_context_recall.csv\")\n",
    "prec_csv   = os.path.join(OUT_DIR, \"ragas_result_context_precision.csv\")\n",
    "\n",
    "w_recall = 0.6\n",
    "w_prec   = 0.4\n",
    "\n",
    "# 1) ë¡œë“œ\n",
    "df_r = pd.read_csv(recall_csv)\n",
    "df_p = pd.read_csv(prec_csv)\n",
    "\n",
    "# 2) id ë³´ì¥\n",
    "if \"id\" not in df_r.columns:\n",
    "    df_r = df_r.copy()\n",
    "    df_r[\"id\"] = range(1, len(df_r) + 1)\n",
    "\n",
    "if \"id\" not in df_p.columns:\n",
    "    df_p = df_p.copy()\n",
    "    df_p[\"id\"] = range(1, len(df_p) + 1)\n",
    "\n",
    "# 3) merge (í•„ìš” ì»¬ëŸ¼ë§Œ)\n",
    "keep_r = [c for c in [\"id\", \"question\", \"context_recall\"] if c in df_r.columns]\n",
    "keep_p = [c for c in [\"id\", \"question\", \"context_precision\"] if c in df_p.columns]\n",
    "\n",
    "df = df_r[keep_r].merge(df_p[keep_p], on=\"id\", how=\"inner\", suffixes=(\"_r\", \"_p\"))\n",
    "\n",
    "# question ì •ë¦¬\n",
    "if \"question_r\" in df.columns and \"question_p\" in df.columns:\n",
    "    df[\"question\"] = df[\"question_r\"].fillna(df[\"question_p\"])\n",
    "    df.drop(columns=[\"question_r\", \"question_p\"], inplace=True)\n",
    "elif \"question_r\" in df.columns:\n",
    "    df.rename(columns={\"question_r\": \"question\"}, inplace=True)\n",
    "elif \"question_p\" in df.columns:\n",
    "    df.rename(columns={\"question_p\": \"question\"}, inplace=True)\n",
    "\n",
    "# 4) R_i ê³„ì‚°\n",
    "if \"context_recall\" not in df.columns or \"context_precision\" not in df.columns:\n",
    "    raise ValueError(\"context_recall ë˜ëŠ” context_precision ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. CSV ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "df[\"R_i\"] = w_recall * df[\"context_recall\"] + w_prec * df[\"context_precision\"]\n",
    "\n",
    "# 5) ìš”ì•½ í†µê³„\n",
    "print(\"âœ… Per-question summary (recall/precision/R_i)\")\n",
    "print(df[[\"context_recall\", \"context_precision\", \"R_i\"]].describe())\n",
    "\n",
    "# 6) R_i ë¶„ìœ„ìˆ˜(ë¶„í¬ ëŠë‚Œ)\n",
    "qs = df[\"R_i\"].quantile([0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0])\n",
    "print(\"\\nğŸ“Œ R_i quantiles\")\n",
    "print(qs)\n",
    "\n",
    "# 7) í…ìŠ¤íŠ¸ íˆìŠ¤í† ê·¸ë¨(10 bins)\n",
    "bins = np.linspace(df[\"R_i\"].min(), df[\"R_i\"].max(), 11)\n",
    "counts, edges = np.histogram(df[\"R_i\"], bins=bins)\n",
    "\n",
    "print(\"\\nğŸ“Š R_i histogram (10 bins)\")\n",
    "for i in range(len(counts)):\n",
    "    lo = edges[i]\n",
    "    hi = edges[i+1]\n",
    "    bar = \"â–ˆ\" * counts[i]\n",
    "    print(f\"{lo:0.3f} ~ {hi:0.3f} | {counts[i]:2d} {bar}\")\n",
    "\n",
    "# 8) í•˜ìœ„ ë¬¸í•­\n",
    "cols_show = [\"id\"]\n",
    "if \"question\" in df.columns:\n",
    "    cols_show.append(\"question\")\n",
    "cols_show += [\"context_recall\", \"context_precision\", \"R_i\"]\n",
    "\n",
    "print(\"\\nğŸ”» Lowest R_i (top 10)\")\n",
    "print(df.sort_values(\"R_i\", ascending=True).head(10)[cols_show])\n",
    "\n",
    "# 9) recall=0 ë¬¸í•­\n",
    "print(\"\\nğŸš¨ recall == 0 rows\")\n",
    "df_fail = df[df[\"context_recall\"] == 0].copy()\n",
    "if df_fail.empty:\n",
    "    print(\"ì—†ìŒ\")\n",
    "else:\n",
    "    print(df_fail.sort_values(\"id\")[cols_show])\n",
    "\n",
    "# 10) ì €ì¥\n",
    "out_path = os.path.join(OUT_DIR, \"ragas_Ri_per_question.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(\"\\nğŸ’¾ saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92d7ba",
   "metadata": {},
   "source": [
    "## ì´ë¯¸ì§€ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5146a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… saved: ./data/RAGAS\\ragas_Ri_per_question.csv\n",
      "âœ… R_overall (mean of R_i): 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:08:12,906 - choreographer.browsers.chromium - INFO - Chromium init'ed with kwargs {}\n",
      "2026-01-30 17:08:12,923 - choreographer.browsers.chromium - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2026-01-30 17:08:12,925 - choreographer.utils._tmpfile - INFO - Temp directory created: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpfzs_88km.\n",
      "2026-01-30 17:08:12,926 - choreographer.browser_async - INFO - Opening browser.\n",
      "2026-01-30 17:08:12,928 - choreographer.utils._tmpfile - INFO - Temp directory created: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpwrmebgjm.\n",
      "2026-01-30 17:08:12,929 - choreographer.browsers.chromium - INFO - Temporary directory at: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpwrmebgjm\n",
      "2026-01-30 17:08:13,184 - kaleido.kaleido - INFO - Conforming 1 to file:///C:/Users/Admin/AppData/Local/Temp/tmpfzs_88km/index.html\n",
      "2026-01-30 17:08:13,185 - kaleido.kaleido - INFO - Waiting on all navigates\n",
      "2026-01-30 17:08:14,040 - kaleido.kaleido - INFO - All navigates done, putting them all in queue.\n",
      "2026-01-30 17:08:14,043 - kaleido.kaleido - INFO - Getting tab from queue (has 1)\n",
      "2026-01-30 17:08:14,044 - kaleido.kaleido - INFO - Got AB79\n",
      "2026-01-30 17:08:14,045 - kaleido._kaleido_tab - INFO - Processing R_i_per_Question_fixed_order_Q1__Q10.png\n",
      "2026-01-30 17:08:14,046 - kaleido._kaleido_tab - INFO - Sending big command for R_i_per_Question_fixed_order_Q1__Q10.png.\n",
      "2026-01-30 17:08:14,153 - kaleido._kaleido_tab - INFO - Sent big command for R_i_per_Question_fixed_order_Q1__Q10.png.\n",
      "2026-01-30 17:08:14,154 - kaleido.kaleido - INFO - Reloading tab AB79 before return.\n",
      "2026-01-30 17:08:14,238 - kaleido.kaleido - INFO - Putting tab AB79 back (queue size: 0).\n",
      "2026-01-30 17:08:14,239 - kaleido.kaleido - INFO - Waiting for all cleanups to finish.\n",
      "2026-01-30 17:08:14,240 - kaleido.kaleido - INFO - Exiting Kaleido\n",
      "2026-01-30 17:08:14,242 - choreographer.browser_async - INFO - Closing browser.\n",
      "2026-01-30 17:08:14,244 - choreographer.browser_async - INFO - Closing browser.\n",
      "2026-01-30 17:08:14,246 - choreographer.utils._tmpfile - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2026-01-30 17:08:14,247 - choreographer.utils._tmpfile - INFO - shutil.rmtree worked.\n",
      "2026-01-30 17:08:14,248 - kaleido.kaleido - INFO - Cancelling tasks.\n",
      "2026-01-30 17:08:14,249 - kaleido.kaleido - INFO - Exiting Kaleido/Choreo\n",
      "2026-01-30 17:08:14,360 - choreographer.utils._tmpfile - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2026-01-30 17:08:14,360 - choreographer.utils._tmpfile - INFO - shutil.rmtree worked.\n",
      "2026-01-30 17:08:14,360 - choreographer.utils._tmpfile - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2026-01-30 17:08:14,376 - choreographer.utils._tmpfile - INFO - shutil.rmtree worked.\n",
      "2026-01-30 17:08:14,376 - kaleido.kaleido - INFO - Cancelling tasks.\n",
      "2026-01-30 17:08:14,377 - kaleido.kaleido - INFO - Exiting Kaleido/Choreo\n",
      "2026-01-30 17:08:14,390 - choreographer.browsers.chromium - INFO - Chromium init'ed with kwargs {}\n",
      "2026-01-30 17:08:14,393 - choreographer.browsers.chromium - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2026-01-30 17:08:14,394 - choreographer.utils._tmpfile - INFO - Temp directory created: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpzw6_9xco.\n",
      "2026-01-30 17:08:14,396 - choreographer.browser_async - INFO - Opening browser.\n",
      "2026-01-30 17:08:14,398 - choreographer.utils._tmpfile - INFO - Temp directory created: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpaxhkrv7p.\n",
      "2026-01-30 17:08:14,399 - choreographer.browsers.chromium - INFO - Temporary directory at: C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpaxhkrv7p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ PNG saved: ./data/RAGAS\\R_i_top10_bar.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 17:08:14,629 - kaleido.kaleido - INFO - Conforming 1 to file:///C:/Users/Admin/AppData/Local/Temp/tmpzw6_9xco/index.html\n",
      "2026-01-30 17:08:14,630 - kaleido.kaleido - INFO - Waiting on all navigates\n",
      "2026-01-30 17:08:15,486 - kaleido.kaleido - INFO - All navigates done, putting them all in queue.\n",
      "2026-01-30 17:08:15,488 - kaleido.kaleido - INFO - Getting tab from queue (has 1)\n",
      "2026-01-30 17:08:15,489 - kaleido.kaleido - INFO - Got A292\n",
      "2026-01-30 17:08:15,490 - kaleido._kaleido_tab - INFO - Processing Per_question_metrics_table_2_decimals.png\n",
      "2026-01-30 17:08:15,491 - kaleido._kaleido_tab - INFO - Sending big command for Per_question_metrics_table_2_decimals.png.\n",
      "2026-01-30 17:08:15,553 - kaleido._kaleido_tab - INFO - Sent big command for Per_question_metrics_table_2_decimals.png.\n",
      "2026-01-30 17:08:15,555 - kaleido.kaleido - INFO - Reloading tab A292 before return.\n",
      "2026-01-30 17:08:15,647 - kaleido.kaleido - INFO - Putting tab A292 back (queue size: 0).\n",
      "2026-01-30 17:08:15,647 - kaleido.kaleido - INFO - Waiting for all cleanups to finish.\n",
      "2026-01-30 17:08:15,648 - kaleido.kaleido - INFO - Exiting Kaleido\n",
      "2026-01-30 17:08:15,648 - choreographer.browser_async - INFO - Closing browser.\n",
      "2026-01-30 17:08:15,651 - choreographer.browser_async - INFO - Closing browser.\n",
      "2026-01-30 17:08:15,653 - choreographer.utils._tmpfile - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2026-01-30 17:08:15,654 - choreographer.utils._tmpfile - INFO - shutil.rmtree worked.\n",
      "2026-01-30 17:08:15,655 - kaleido.kaleido - INFO - Cancelling tasks.\n",
      "2026-01-30 17:08:15,656 - kaleido.kaleido - INFO - Exiting Kaleido/Choreo\n",
      "2026-01-30 17:08:15,784 - choreographer.utils._tmpfile - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2026-01-30 17:08:15,785 - choreographer.utils._tmpfile - INFO - shutil.rmtree worked.\n",
      "2026-01-30 17:08:15,785 - choreographer.utils._tmpfile - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2026-01-30 17:08:15,786 - choreographer.utils._tmpfile - INFO - shutil.rmtree worked.\n",
      "2026-01-30 17:08:15,787 - kaleido.kaleido - INFO - Cancelling tasks.\n",
      "2026-01-30 17:08:15,787 - kaleido.kaleido - INFO - Exiting Kaleido/Choreo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ PNG saved: ./data/RAGAS\\R_i_top10_table.png\n",
      "âœ… done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "OUT_DIR = \"./data/RAGAS\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "recall_csv = os.path.join(OUT_DIR, \"ragas_result_context_recall.csv\")\n",
    "prec_csv   = os.path.join(OUT_DIR, \"ragas_result_context_precision.csv\")\n",
    "\n",
    "w_recall = 0.6\n",
    "w_prec   = 0.4\n",
    "\n",
    "# 1) load\n",
    "df_r = pd.read_csv(recall_csv)\n",
    "df_p = pd.read_csv(prec_csv)\n",
    "\n",
    "# 2) ensure id\n",
    "if \"id\" not in df_r.columns:\n",
    "    df_r = df_r.copy()\n",
    "    df_r[\"id\"] = range(1, len(df_r) + 1)\n",
    "if \"id\" not in df_p.columns:\n",
    "    df_p = df_p.copy()\n",
    "    df_p[\"id\"] = range(1, len(df_p) + 1)\n",
    "\n",
    "# 3) merge (í•„ìš” ì»¬ëŸ¼ë§Œ)\n",
    "keep_r = [c for c in [\"id\", \"question\", \"context_recall\"] if c in df_r.columns]\n",
    "keep_p = [c for c in [\"id\", \"question\", \"context_precision\"] if c in df_p.columns]\n",
    "\n",
    "df = df_r[keep_r].merge(df_p[keep_p], on=\"id\", how=\"inner\", suffixes=(\"_r\", \"_p\"))\n",
    "\n",
    "# question ì •ë¦¬\n",
    "if \"question_r\" in df.columns and \"question_p\" in df.columns:\n",
    "    df[\"question\"] = df[\"question_r\"].fillna(df[\"question_p\"])\n",
    "    df.drop(columns=[\"question_r\", \"question_p\"], inplace=True)\n",
    "elif \"question_r\" in df.columns:\n",
    "    df.rename(columns={\"question_r\": \"question\"}, inplace=True)\n",
    "elif \"question_p\" in df.columns:\n",
    "    df.rename(columns={\"question_p\": \"question\"}, inplace=True)\n",
    "\n",
    "# 4) compute R_i\n",
    "if \"context_recall\" not in df.columns or \"context_precision\" not in df.columns:\n",
    "    raise ValueError(\"CSVì— context_recall / context_precision ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "df[\"R_i\"] = w_recall * df[\"context_recall\"] + w_prec * df[\"context_precision\"]\n",
    "\n",
    "# 5) round to 2 decimals (CSV/ì´ë¯¸ì§€ ëª¨ë‘)\n",
    "for c in [\"context_recall\", \"context_precision\", \"R_i\"]:\n",
    "    df[c] = df[c].astype(float).round(2)\n",
    "\n",
    "# 6) overall R (mean of R_i) -> CSVì—ë§Œ\n",
    "R_overall = float(df[\"R_i\"].mean().round(2))\n",
    "\n",
    "# 7) save CSV (ìš”ì•½í–‰ 1ì¤„ ì¶”ê°€)\n",
    "out_csv = os.path.join(OUT_DIR, \"ragas_Ri_per_question.csv\")\n",
    "\n",
    "cols = [\"id\"]\n",
    "if \"question\" in df.columns:\n",
    "    cols.append(\"question\")\n",
    "cols += [\"context_recall\", \"context_precision\", \"R_i\"]\n",
    "\n",
    "df_out = df[cols].copy()\n",
    "\n",
    "summary_row = {c: \"\" for c in df_out.columns}\n",
    "summary_row[\"id\"] = \"OVERALL\"\n",
    "summary_row[\"R_i\"] = R_overall  # ì „ì²´ R ê°’ì€ ì—¬ê¸°ì„œë§Œ ì œê³µ\n",
    "df_out = pd.concat([df_out, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "df_out.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"âœ… saved:\", out_csv)\n",
    "print(\"âœ… R_overall (mean of R_i):\", R_overall)\n",
    "\n",
    "# ---------- Plotly -> PNG helper ----------\n",
    "def save_png(fig, filename: str):\n",
    "    path = os.path.join(OUT_DIR, filename)\n",
    "    fig.write_image(path, scale=2)  # kaleido í•„ìš”\n",
    "    print(\"ğŸ–¼ï¸ PNG saved:\", path)\n",
    "\n",
    "# 8) Bar PNG (ì§ˆë¬¸ ë²ˆí˜¸ ê³ ì •: id ì˜¤ë¦„ì°¨ìˆœ)\n",
    "\n",
    "def short(s, n=40):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    return s if len(s) <= n else s[:n] + \"â€¦\"\n",
    "\n",
    "df_plot = df.copy()\n",
    "\n",
    "# ğŸ”‘ í•µì‹¬: id ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì ˆëŒ€ R_ië¡œ ì •ë ¬í•˜ì§€ ì•ŠìŒ)\n",
    "df_plot = df_plot.sort_values(\"id\", ascending=True)\n",
    "\n",
    "# ë¼ë²¨ ìƒì„±\n",
    "if \"question\" in df_plot.columns:\n",
    "    df_plot[\"label\"] = df_plot.apply(\n",
    "        lambda r: f'{int(r[\"id\"])}. {short(r[\"question\"])}',\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    df_plot[\"label\"] = df_plot[\"id\"].astype(str)\n",
    "\n",
    "fig_bar = px.bar(\n",
    "    df_plot,\n",
    "    x=\"R_i\",\n",
    "    y=\"label\",\n",
    "    orientation=\"h\",\n",
    "    title=\"R_i per Question (fixed order: Q1 â†’ Q10)\",\n",
    "    text=\"R_i\",\n",
    ")\n",
    "\n",
    "# ìˆ«ì í¬ë§· ê³ ì • (ì†Œìˆ˜ 2ìë¦¬)\n",
    "fig_bar.update_traces(\n",
    "    texttemplate=\"%{text:.2f}\",\n",
    "    textposition=\"outside\"\n",
    ")\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    xaxis=dict(range=[0, 1.05]),\n",
    "    yaxis=dict(autorange=\"reversed\"),  # 1ë²ˆì´ ë§¨ ìœ„ë¡œ ì˜¤ê²Œ\n",
    "    margin=dict(l=10, r=10, t=60, b=10),\n",
    "    height=520\n",
    ")\n",
    "\n",
    "save_png(fig_bar, \"R_i_top10_bar.png\")\n",
    "\n",
    "\n",
    "# 9) Table PNG (10ê°œ ì „ì²´)\n",
    "table_df = df[cols].copy()\n",
    "# ìˆ«ì ì»¬ëŸ¼ì€ ë¬¸ìì—´ë¡œ 2ìë¦¬ ê³ ì • í‘œê¸°\n",
    "for c in [\"context_recall\", \"context_precision\", \"R_i\"]:\n",
    "    if c in table_df.columns:\n",
    "        table_df[c] = table_df[c].map(lambda x: f\"{float(x):.2f}\")\n",
    "\n",
    "fig_table = go.Figure(\n",
    "    data=[\n",
    "        go.Table(\n",
    "            header=dict(values=list(table_df.columns)),\n",
    "            cells=dict(values=[table_df[c].tolist() for c in table_df.columns]),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "fig_table.update_layout(title=\"Per-question metrics table (2 decimals)\", height=420, margin=dict(l=10, r=10, t=60, b=10))\n",
    "\n",
    "save_png(fig_table, \"R_i_top10_table.png\")\n",
    "\n",
    "print(\"âœ… done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ad24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm (ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
