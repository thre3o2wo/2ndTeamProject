"""
RAG ë²•ë ¹ ì¸ë±ìŠ¤ ì²˜ë¦¬ - ê°œì„ ëœ ë©”íƒ€ë°ì´í„° ë²„ì „ (v3)
================================================================================
ë³€ê²½ ë‚´ìš©:
- article: í•´ë‹¹ ì¡°ë¬¸ì˜ ì¡°í•­ë²ˆí˜¸ì™€ ì œëª©ë§Œ ì¶”ì¶œ (ex: "ì œ3ì¡°(ëŒ€í•­ë ¥ ë“±)")
- title: LLMì„ í†µí•œ ì‹¤ì§ˆì ì¸ ë‚´ìš© ìš”ì•½ (ìš”ì•½ë¬¸ ìƒì„±)
- text(page_content): articleì„ ì œì™¸í•œ ìˆœìˆ˜ ë³¸ë¬¸ë§Œ

ëª©ì : RAG ì¡°íšŒ ì‹œ "title"ê³¼ "text"ë¥¼ ëª¨ë‘ ì°¸ì¡°í•´ ì •í™•ë„ë¥¼ ë†’ì´ê³ ,
      LLM contextë¡œ "{src_title} - {article}. {text}" í˜•íƒœë¡œ ì œê³µ
================================================================================
"""

import os
import re
import time
import pandas as pd
from langchain_ollama import ChatOllama
from langchain_community.document_loaders import Docx2txtLoader
from langchain_core.documents import Document


# ============================================================================
# 1. ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì‚¬ì „ (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================================================
def get_law_category():
    """
    ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ í†µí•© ì¹´í…Œê³ ë¦¬-í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì‚¬ì „
    ë²•ë ¹, íŒë¡€, ìƒë‹´ ì‚¬ë¡€ë¥¼ ì•„ìš°ë¥´ëŠ” í¬ê´„ì  ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    return {
        # 1. ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥: ë³´ì¦ê¸ˆ ë³´í˜¸ ë° ìš°ì„ ë³€ì œê¶Œ (ìµœê³  ì¤‘ìš” ë¦¬ìŠ¤í¬)
        'ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥': {
            'ë³´ì¦ê¸ˆ': 3, 'ëŒ€í•­ë ¥': 3, 'ìš°ì„ ë³€ì œê¶Œ': 3, 'ìµœìš°ì„ ë³€ì œ': 3,
            'ë³´ì¦ê¸ˆë°˜í™˜': 3, 'ì „ì„¸ê¸ˆ': 2, 'ì†Œì•¡ì„ì°¨ì¸': 2, 'í™•ì •ì¼ì': 2,
            'ì „ì…ì‹ ê³ ': 2, 'ì ìœ ': 2, 'ì„ì°¨ê¶Œë“±ê¸°': 3, 'ë°°ë‹¹': 2,
            'ë°˜í™˜ë³´ì¦': 2, 'HUG': 1, 'ë³´ì¦ë³´í—˜': 2, 'ì „ì„¸ë³´ì¦ê¸ˆ': 2,
            'ìš°ì„ ë§¤ìˆ˜ê¶Œ': 3, 'í”¼í•´ìê²°ì •': 2, 'ê²½ë§¤ìœ ì˜ˆ': 3,
            'ì„ëŒ€ë³´ì¦ê¸ˆë³´ì¦': 3, 'ì¼ë¶€ë³´ì¦': 2, 'ë³´ì¦ë¯¸ê°€ì…': 3, 'ë™ì˜ì„œ': 2,
            'ì„ì°¨ê¶Œì´ì „': 2, 'ê¸ˆìœµê¸°ê´€ë‹´ë³´': 1, 'ì„ì°¨ê¶Œë“±ê¸°ëª…ë ¹': 3, 
            'ë“±ê¸°ì´‰íƒ': 2, 'ê¸°ì…ë“±ê¸°': 2, 'ì „ìí™•ì •ì¼ì': 3,
        },

        # 2. ê³„ì•½ê°±ì‹ _ì—°ì¥: ê°±ì‹ ìš”êµ¬ê¶Œ ë° ì‹¤ê±°ì£¼ ê±°ì ˆ (ë¹ˆë²ˆí•œ ë¶„ìŸ)
        'ê³„ì•½ê°±ì‹ ': {
            'ê³„ì•½ê°±ì‹ ': 3, 'ê°±ì‹ ìš”êµ¬': 3, 'ê°±ì‹ ê±°ì ˆ': 3, 'ë¬µì‹œì ê°±ì‹ ': 3,
            'ê³„ì•½ì—°ì¥': 2, 'ì‹¤ê±°ì£¼': 3, 'ì¡´ì†ê¸°ê°„': 2, '2ë…„': 1, '2+2': 2,
            'ì¬ê³„ì•½': 2, 'ê°±ì‹ ì²­êµ¬': 2, 'ê±°ì ˆì‚¬ìœ ': 2, 'ë³µë¹„': 1, 'ì§ê³„ì¡´ë¹„ì†': 2
        },

        # 3. ìˆ˜ì„ _ì›ìƒíšŒë³µ: ë¯¼ë²• ì œ615ì¡° ê¸°ë°˜ì˜ ìœ ì§€ë³´ìˆ˜ ë° ì²­ì†Œë¹„ ê³µì œ
        'ìˆ˜ì„ _ì›ìƒíšŒë³µ': {
            'ìˆ˜ì„ ': 3, 'ìˆ˜ë¦¬': 3, 'ì›ìƒíšŒë³µ': 3, 'ì›ìƒì— íšŒë³µ': 3, 'íŒŒì†': 2, 
            'í›¼ì†': 2, 'ëˆ„ìˆ˜': 3, 'ê³°íŒ¡ì´': 2, 'ë³´ì¼ëŸ¬': 2, 'í•„ìš”ë¹„': 3, 
            'ìœ ìµë¹„': 3, 'ë¹„ìš©ìƒí™˜': 2, 'ë³´ì¡´í–‰ìœ„': 2, 'ì²­ì†Œë¹„': 3, 
            'ì›ìƒë³µêµ¬': 2, 'í†µìƒì  ë§ˆë©¸': 3, 'ìì—°ì  ë§ˆë©¸': 2, 'ê³µì œ': 3,
            'ê³µìš©ë¶€ë¶„': 2, 'ë¶€ì†ì‹œì„¤': 2, 'êµì²´': 1,
        },

        # 4. ìƒí™œí™˜ê²½_íŠ¹ì•½: ì£¼ê±°ì˜ ì§ˆê³¼ ê´€ë ¨ëœ ì»¤ë®¤ë‹ˆí‹° ê°ˆë“± (ì‹ ì„¤)
        'ìƒí™œí™˜ê²½_íŠ¹ì•½': {
            'ì¸µê°„ì†ŒìŒ': 3, 'ë°˜ë ¤ë™ë¬¼': 3, 'ì• ì™„ë™ë¬¼': 3, 'í¡ì—°': 3, 'ë‹´ë°°': 2,
            'ì£¼ì°¨': 2, 'ê²°ë¡œ': 2, 'ë°©ìŒ': 2, 'ì•…ì·¨': 2, 'ê³µë™ìƒí™œ': 2, 
            'ê¸ˆì§€ì‚¬í•­': 2, 'ì…ì£¼ë¯¼ê°ˆë“±': 2, 'ë²½ì§€í›¼ì†': 2,
            'í‰ì˜¨ì„ í•´ì¹˜ëŠ” í–‰ìœ„': 3, 'ê´€ë¦¬ê·œì•½': 2
        },

        # 5. ê´€ë¦¬ë¹„_íˆ¬ëª…ì„±: ê´€ë¦¬ë¹„ ê¼¼ìˆ˜ ì¸ìƒ ë° ë‚´ì—­ ê³µê°œ (ìµœì‹  ì´ìŠˆ)
        'ê´€ë¦¬ë¹„_íˆ¬ëª…ì„±': {
            'ê´€ë¦¬ë¹„': 3, 'ê³µê³µìš”ê¸ˆ': 2, 'ì •ì•¡ê´€ë¦¬ë¹„': 3, 'ì„¸ë¶€ë‚´ì—­': 2,
            'ì¥ê¸°ìˆ˜ì„ ì¶©ë‹¹ê¸ˆ': 3, 'ìˆ˜ì„ ìœ ì§€ë¹„': 2, 'ê´€ë¦¬ë¹„ì¸ìƒ': 2, 'ì „ê¸°ë£Œ': 1,
            'ìˆ˜ë„ë£Œ': 1, 'ë¹„ìš© ì²­êµ¬': 1, 'ì‚¬ìš©ë£Œ': 2, 'ê´€ë¦¬ì£¼ì²´': 2, 'ë‚´ì—­ê³µê°œ': 2,
            'ìˆ˜ì„ ì ë¦½ê¸ˆ': 3, 'ê´€ë¦¬ë‹¨': 2, 'ë¶„ë‹´ê¸ˆ': 2
        },

        # 6. ì„ëŒ€ë£Œ_ì¦ê°: ì°¨ì„ ì¦ì•¡ ìƒí•œ(5%) ë° ì›”ì„¸ ì „í™˜ìœ¨
        'ì„ëŒ€ë£Œ_ì¦ê°': {
            'ì°¨ì„': 3, 'ì›”ì„¸': 2, 'ì¦ì•¡': 3, 'ê°ì•¡': 2, 'ì¸ìƒ': 2,
            '5í¼ì„¼íŠ¸': 3, '5%': 3, '20ë¶„ì˜ 1': 2, 'ìƒí•œ': 2,
            'ì „í™˜ìœ¨': 2, 'ì›”ì°¨ì„': 2, 'ê²½ì œì‚¬ì •': 1, 'ë¶€ë‹´': 1, 'ì—°ì²´': 3, '3ê¸°': 3,
            'ë³´ì¦ê¸ˆì „í™˜': 3, 'ì„ì°¨ì¸ë™ì˜': 2,
        },

        # 7. ê¶Œë¦¬_ì •ë³´ë¦¬ìŠ¤í¬: ì‹ íƒ ë¶€ë™ì‚°, ì „ì„¸ì‚¬ê¸°, ì„¸ê¸ˆ ì²´ë‚© (ì •ë³´ ë¹„ëŒ€ì¹­)
        'ê¶Œë¦¬_ì •ë³´ë¦¬ìŠ¤í¬': {
            'ì „ì„¸ì‚¬ê¸°': 3, 'ê¹¡í†µì „ì„¸': 3, 'ì‹ íƒ': 3, 'ìˆ˜íƒì': 2, 'ê·¼ì €ë‹¹': 3, 
            'ì €ë‹¹ê¶Œ': 3, 'ì„ ìˆœìœ„': 3, 'ê°€ì••ë¥˜': 3, 'ì••ë¥˜': 3, 'êµ­ì„¸': 2, 
            'ì§€ë°©ì„¸': 2, 'ì²´ë‚©': 3, 'ë‚©ì„¸ì¦ëª…': 2, 'ìœ„ë°˜ê±´ì¶•ë¬¼': 2, 'ë¶ˆë²•ê±´ì¶•ë¬¼': 2,
            'í™•ì •ì¼ìí˜„í™©': 3, 'ì¡°ì„¸ì±„ê¶Œ': 2, 'ë²•ì •ê¸°ì¼': 3, 'ë¯¸ë‚©êµ­ì„¸ì—´ëŒ': 2,
            'ì‚¬ìš©ìŠ¹ì¸': 2, 'ë¬´í—ˆê°€': 2, 'ê±°ì§“ê³„ì•½': 2,
            'ë¯¸ë‚©ì§€ë°©ì„¸ì—´ëŒ': 3, 'ì‚¬í•´í–‰ìœ„ì·¨ì†Œ': 2,
            'ë³¸ë“±ê¸°': 2, 'ì§ê¶Œë§ì†Œ': 2, 'ê°€ë“±ê¸°': 2,
            'ì •ë³´ì œê³µìš”ì²­': 3, 'ì—´ëŒì‹ ì²­': 2, 'ì´í•´ê´€ê³„ì¸ ì¦ëª…': 2,
        },

        # 8. íŠ¹ì•½_ìœ íš¨ì„±: ì£¼ì„ë²• ì œ10ì¡°(ê°•í–‰ê·œì •) ë° ë…ì†Œì¡°í•­ íƒì§€
        'íŠ¹ì•½_ìœ íš¨ì„±': {
            'íŠ¹ì•½': 3, 'íŠ¹ì•½ì‚¬í•­': 3, 'ê°•í–‰ê·œì •': 3, 'ì œ10ì¡°': 3,
            'íš¨ë ¥ì´ ì—†ë‹¤': 3, 'ë¬´íš¨': 3, 'ë¶ˆë¦¬í•œ ì•½ì •': 3, 'ì¼ë°©ì  í¬ê¸°': 2,
            'í¸ë©´ì  ê°•í–‰ê·œì •': 2, 'ë°°ì œ': 1, 'í‘œì¤€ì„ëŒ€ì°¨ê³„ì•½ì„œ': 3
        },

        # 9. ë°°ìƒ_ì±…ì„: ë¯¼ë²• ì œ390ì¡°/ì œ750ì¡° ê¸°ë°˜ì˜ ì†í•´ë°°ìƒ ë° ìœ„ì•½ê¸ˆ
        'ì†í•´ë°°ìƒ_ì±…ì„': {
            'ì†í•´ë°°ìƒ': 3, 'ì±„ë¬´ë¶ˆì´í–‰': 3, 'ë¶ˆë²•í–‰ìœ„': 3, 'ìœ„ì•½ê¸ˆ': 3,
            'ê³¼ì‹¤': 2, 'ë°°ìƒì•¡': 2, 'ì§€ì—°ì´ì': 2, 'ì´í–‰ì§€ì²´': 2, 'ìœ„ë°˜': 2
        },

        # 10. ê³„ì•½í•´ì§€_ëª…ë„: ê³„ì•½ ì¢…ë£Œ ì ˆì°¨ ë° í‡´ê±°/ì´ì‚¬ ê´€ë ¨
        'ê³„ì•½í•´ì§€_ëª…ë„': {
            'ê³„ì•½í•´ì§€': 3, 'í•´ì§€í†µê³ ': 3, 'ì¤‘ë„í•´ì§€': 3, 'ê¸°ê°„ë§Œë£Œ': 2,
            'ê³„ì•½ì¢…ë£Œ': 2, 'í‡´ê±°': 3, 'ì´ì‚¬': 1, 'ëª…ë„': 3, 'í•©ì˜í•´ì§€': 2, 
            'í†µì§€': 1, '3ê°œì›”': 2, 'ì¦‰ì‹œí•´ì§€': 3, 'ë‚´ìš©ì¦ëª…': 2,
            'ì„ëŒ€ì°¨ì‹ ê³ ': 3, 'ê±°ë˜ì‹ ê³ ': 3, 'ì‹ ê³ í•„ì¦': 2, 'ë“±ê¸°ì‚¬í•­': 2, 'ì •ì •ì‹ ì²­': 2,
        },

        # 11. í–‰ì •_ì ˆì°¨: í™•ì •ì¼ì ë¶€ì—¬ ë° ì „ìê³„ì•½ ì ˆì°¨
        'í–‰ì •ì ˆì°¨': {
            'í™•ì •ì¼ìë¶€ì—¬': 3, 'ë™ì£¼ë¯¼ì„¼í„°': 2, 'ë“±ê¸°ì†Œ': 2, 'ì¸í„°ë„·ë“±ê¸°ì†Œ': 3,
            'ìˆ˜ìˆ˜ë£Œ': 1, 'ì—´ëŒ': 2, 'ì œê³µìš”ì²­': 2, 'ì´í•´ê´€ê³„ì¸': 2,
            'ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ': 2, 'ì „ìê³„ì•½': 2, 'ì‹ ë¶„ì¦': 1
        },

        # 12. ë¶„ìŸí•´ê²°_ì ˆì°¨: ì¡°ì •ìœ„ì›íšŒ ë° ì†Œì†¡/ê²½ë§¤ ìµœí›„ ìˆ˜ë‹¨
        'ë¶„ìŸí•´ê²°': {
            'ë¶„ìŸì¡°ì •': 3, 'ì¡°ì •ìœ„ì›íšŒ': 3, 'ì§€ê¸‰ëª…ë ¹': 3, 'ì†Œì†¡': 3,
            'íŒê²°': 2, 'ì§‘í–‰ê¶Œì›': 2, 'ê²½ë§¤': 3, 'ê³µë§¤': 2, 'ì œì†Œì „í™”í•´': 3,
            'ì†¡ë‹¬': 2, 'ê°•ì œì§‘í–‰': 3, 'ë°°ë‹¹ìš”êµ¬': 3, 'ë§¤ìˆ˜ì‹ ì²­ë³´ì¦': 2,
            'ì¡°ì •ì‹ ì²­ê°í•˜': 2, 'ì¶œì„ìš”êµ¬': 1, 'ì„œë©´êµë¶€': 1,
            'ì†Œì•¡ì‚¬ê±´': 3, '3,000ë§Œì›': 3, 'ì´í–‰ê¶Œê³ ': 2, 'ê³µì‹œìµœê³ ': 1,
            'ì„ì°¨ê¶Œë“±ê¸°ëª…ë ¹ì‹ ì²­': 3, 'ê²°ì •ì†¡ë‹¬': 2, 'ì‹ ì²­ì„œê¸°ì¬ì‚¬í•­': 1,
        },

        # 13. ì„ì°¨ê¶Œ_ìŠ¹ê³„: ì œ9ì¡° íƒ€ê²ŸíŒ… ë° ì‚¬ì‹¤í˜¼ ê°€ì¡± ë³´í˜¸
        'ì„ì°¨ê¶Œ_ìŠ¹ê³„': {
            'ì„ì°¨ê¶ŒìŠ¹ê³„': 3, 'ìŠ¹ê³„': 3, 'ì‚¬ë§': 3, 'ìƒì†': 3, 'ìƒì†ì¸': 3,
            'ì‚¬ì‹¤í˜¼': 3, 'ë°°ìš°ì': 2, 'ê°€ì •ê³µë™ìƒí™œ': 3, '2ì´Œ': 2, 
            'ê³µë™ìƒì†': 2, 'ë°˜í™˜ì²­êµ¬ê¶Œ': 2
        }
    }


# ============================================================================
# 2. í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================================

def categorize_content(content, top_k=None):
    """
    ë‚´ìš© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ - ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ë°˜í™˜
    """
    category_keywords = get_law_category()
    category_scores = {}
    
    for category, weighted_keywords in category_keywords.items():
        score = 0
        for keyword, weight in weighted_keywords.items():
            count = content.count(keyword)
            score += count * weight
        if score > 0:
            category_scores[category] = score
    
    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
    all_categories = [category[0] for category in sorted_categories]
    
    if not all_categories:
        all_categories = ["ê¸°íƒ€"]
    
    return all_categories[:top_k] if top_k else all_categories


def split_by_article_safe(text):
    """
    ì¤„ë°”ê¿ˆ(\\n)ê³¼ í•¨ê»˜ ì‹œì‘í•˜ëŠ” 'ì œNì¡°'ë§Œ ì¸ì‹í•˜ì—¬ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    ë¬¸ì¥ ì¤‘ê°„ì— ë‚˜ì˜¤ëŠ” 'ì œNì¡°'ëŠ” ë¬´ì‹œí•©ë‹ˆë‹¤.
    """
    padded_text = "\n" + text.strip()
    pattern = r'(\nì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?)'  # 'ì œNì¡°' ë˜ëŠ” 'ì œNì¡°ì˜2' í˜•íƒœ
    parts = re.split(pattern, padded_text)
    
    chunks = []
    if parts and parts[0].strip():
        chunks.append(parts[0].strip())
        
    for i in range(1, len(parts), 2):
        if i + 1 < len(parts):
            chunks.append(f"{parts[i].strip()} {parts[i+1].strip()}")
    return chunks


# ============================================================================
# 3. [ì‹ ê·œ] ê°œì„ ëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ë“¤
# ============================================================================

def extract_article_header(content):
    """
    [ì‹ ê·œ] ì¡°ë¬¸ì˜ ì¡°í•­ë²ˆí˜¸ì™€ ì œëª©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì…ë ¥: "ì œ3ì¡° (ëŒ€í•­ë ¥ ë“±) â‘  ì„ëŒ€ì°¨ëŠ” ê·¸ ë“±ê¸°(ç™»è¨˜)ê°€ ì—†ëŠ” ê²½ìš°ì—ë„..."
    ì¶œë ¥: "ì œ3ì¡°(ëŒ€í•­ë ¥ ë“±)"
    
    ì…ë ¥: "ì œ1ì¡°ì˜2(ì†Œì•¡ì‚¬ê±´ì˜ ë²”ìœ„) ë²• ì œ2ì¡°ì œ1í•­ì— ë”°ë¥¸..."
    ì¶œë ¥: "ì œ1ì¡°ì˜2(ì†Œì•¡ì‚¬ê±´ì˜ ë²”ìœ„)"
    """
    content = content.strip()
    
    # íŒ¨í„´: ì œNì¡° ë˜ëŠ” ì œNì¡°ì˜M + (ì œëª©)
    # ì˜ˆ: "ì œ3ì¡°(ëŒ€í•­ë ¥ ë“±)", "ì œ1ì¡°ì˜2(ì†Œì•¡ì‚¬ê±´ì˜ ë²”ìœ„)"
    patterns = [
        # ì œNì¡°ì˜M (ì œëª©) í˜•íƒœ - ì œëª© ìˆëŠ” ê²½ìš°
        r'^(ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?)\s*\(([^)]+)\)',
        # ì œNì¡° (ì œëª©) í˜•íƒœ - ê´„í˜¸ ì•ì— ê³µë°±ì´ ìˆëŠ” ê²½ìš°
        r'^(ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?)\s+\(([^)]+)\)',
        # ì œNì¡°ì˜M í˜•íƒœ - ì œëª© ì—†ëŠ” ê²½ìš°
        r'^(ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?)',
    ]
    
    for pattern in patterns:
        match = re.match(pattern, content)
        if match:
            groups = match.groups()
            article_num = groups[0].replace(" ", "")  # ê³µë°± ì œê±°: "ì œ 3 ì¡°" -> "ì œ3ì¡°"
            
            if len(groups) >= 2 and groups[1]:
                # ì œëª©ì´ ìˆëŠ” ê²½ìš°
                title = groups[1].strip()
                return f"{article_num}({title})"
            else:
                # ì œëª©ì´ ì—†ëŠ” ê²½ìš°
                return article_num
    
    # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì²« ì¤„ì˜ ì•ë¶€ë¶„ ë°˜í™˜
    first_line = content.split('\n')[0][:50]
    return first_line


def extract_body_text(content):
    """
    [ì‹ ê·œ] ì¡°í•­ í—¤ë”(ì¡°í•­ë²ˆí˜¸ì™€ ì œëª©)ë¥¼ ì œì™¸í•œ ìˆœìˆ˜ ë³¸ë¬¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì…ë ¥: "ì œ3ì¡° (ëŒ€í•­ë ¥ ë“±) â‘  ì„ëŒ€ì°¨ëŠ” ê·¸ ë“±ê¸°(ç™»è¨˜)ê°€ ì—†ëŠ” ê²½ìš°ì—ë„..."
    ì¶œë ¥: "â‘  ì„ëŒ€ì°¨ëŠ” ê·¸ ë“±ê¸°(ç™»è¨˜)ê°€ ì—†ëŠ” ê²½ìš°ì—ë„..."
    
    ì…ë ¥: "ì œ618ì¡° (ì„ëŒ€ì°¨ì˜ ì˜ì˜) ì„ëŒ€ì°¨ëŠ” ë‹¹ì‚¬ì ì¼ë°©ì´ ìƒëŒ€ë°©ì—ê²Œ..."
    ì¶œë ¥: "ì„ëŒ€ì°¨ëŠ” ë‹¹ì‚¬ì ì¼ë°©ì´ ìƒëŒ€ë°©ì—ê²Œ..."
    """
    content = content.strip()
    
    # ì¡°í•­ í—¤ë” íŒ¨í„´ ì œê±°
    # íŒ¨í„´: ì œNì¡° ë˜ëŠ” ì œNì¡°ì˜M + ì„ íƒì  (ì œëª©)
    patterns = [
        # ì œNì¡°ì˜M (ì œëª©) í˜•íƒœ
        r'^ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*\([^)]+\)\s*',
        # ì œNì¡° (ì œëª©) í˜•íƒœ
        r'^ì œ\s*\d+\s*ì¡°\s+\([^)]+\)\s*',
        # ì œNì¡°ì˜M í˜•íƒœ (ì œëª© ì—†ìŒ)
        r'^ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*',
    ]
    
    for pattern in patterns:
        result = re.sub(pattern, '', content, count=1)
        if result != content:
            return result.strip()
    
    return content


def summarize_article_with_llm(content, article_header, llm=None):
    """
    [ì‹ ê·œ/ê°œì„ ] LLMì„ ì‚¬ìš©í•˜ì—¬ ì¡°ë¬¸ ë‚´ìš©ì„ ì‹¤ì§ˆì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    
    ê¸°ì¡´ ë¬¸ì œì : "ì œ618ì¡° ì„ëŒ€ì°¨ì˜ ì˜ì˜"ì²˜ëŸ¼ ì œëª© ë”°ê¸°ì— ê·¸ì¹¨
    ê°œì„  ëª©í‘œ: ì‹¤ì§ˆì ì¸ ë‚´ìš© ìš”ì•½ ìƒì„±
    
    ì˜ˆì‹œ:
    ì…ë ¥: "ì œ618ì¡° (ì„ëŒ€ì°¨ì˜ ì˜ì˜) ì„ëŒ€ì°¨ëŠ” ë‹¹ì‚¬ì ì¼ë°©ì´ ìƒëŒ€ë°©ì—ê²Œ ëª©ì ë¬¼ì„ ì‚¬ìš©, 
          ìˆ˜ìµí•˜ê²Œ í•  ê²ƒì„ ì•½ì •í•˜ê³  ìƒëŒ€ë°©ì´ ì´ì— ëŒ€í•˜ì—¬ ì°¨ì„ì„ ì§€ê¸‰í•  ê²ƒì„ 
          ì•½ì •í•¨ìœ¼ë¡œì¨ ê·¸ íš¨ë ¥ì´ ìƒê¸´ë‹¤."
    ì¶œë ¥: "ì„ëŒ€ì°¨ëŠ” í•œìª½ì´ ë¬¼ê±´ì„ ì‚¬ìš©Â·ìˆ˜ìµí•˜ê²Œ í•˜ê³ , ìƒëŒ€ë°©ì´ ê·¸ ëŒ€ê°€ë¡œ 
          ì°¨ì„ì„ ì§€ê¸‰í•˜ê¸°ë¡œ ì•½ì •í•˜ë©´ ì„±ë¦½í•œë‹¤."
    """
    if llm is None:
        llm = ChatOllama(
            model="exaone3.5:2.4b",
            temperature=0.2,
            num_predict=150,
        )
    
    # ë³¸ë¬¸ë§Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
    body_text = extract_body_text(content)
    
    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if len(body_text) < 30:
        return body_text
    
    prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë²•ë¥  ì¡°ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ **ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ** 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

[ê·œì¹™]
1. ì¡°ë¬¸ ë²ˆí˜¸ë‚˜ ì œëª©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: "ì œ3ì¡°", "ëŒ€í•­ë ¥ ë“±" ê¸ˆì§€)
2. í•µì‹¬ì ì¸ ë²•ì  íš¨ê³¼ë‚˜ ìš”ê±´ë§Œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
3. "~í•œë‹¤", "~ëœë‹¤" í˜•íƒœì˜ í‰ì„œë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ìš”ì•½ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª…ì´ë‚˜ ë¶€ì—° ê¸ˆì§€)

[ì¡°ë¬¸ ë²ˆí˜¸]: {article_header}
[ì¡°ë¬¸ ë‚´ìš©]: {body_text[:800]}

[ìš”ì•½]:"""

    try:
        ai_message = llm.invoke(prompt)
        summary = ai_message.content.strip()
        
        # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°
        prefixes_to_remove = [
            "ìš”ì•½:", "ìš”ì•½ :", "[ìš”ì•½]:", "[ìš”ì•½] :",
            "ë‹µë³€:", "ë‹µë³€ :", "ê²°ê³¼:", "ê²°ê³¼ :",
        ]
        for prefix in prefixes_to_remove:
            if summary.lower().startswith(prefix.lower()):
                summary = summary[len(prefix):].strip()
        
        # ì¡°ë¬¸ ë²ˆí˜¸ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œê±°
        summary = re.sub(r'^ì œ\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?\s*', '', summary)
        summary = re.sub(r'^\([^)]+\)\s*', '', summary)
        
        return summary if summary else body_text[:100]
        
    except Exception as e:
        print(f"   âš ï¸ LLM ìš”ì•½ ì‹¤íŒ¨: {str(e)[:50]}")
        return body_text[:100] + "..." if len(body_text) > 100 else body_text


# ============================================================================
# 4. [ì‹ ê·œ] ê°œì„ ëœ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def process_single_file_v3(file_path, priority, output_dir='./data/1-2_chunked/', use_llm_summary=True):
    """
    [ê°œì„ ëœ ë²„ì „] íŒŒì¼ í•˜ë‚˜ë¥¼ ì²˜ë¦¬í•˜ì—¬ Documents ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ê³  CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    ë©”íƒ€ë°ì´í„° ë³€ê²½ ì‚¬í•­:
    - article: í•´ë‹¹ ì¡°ë¬¸ì˜ ì¡°í•­ë²ˆí˜¸ì™€ ì œëª© (ex: "ì œ3ì¡°(ëŒ€í•­ë ¥ ë“±)")
    - title: LLMì„ í†µí•œ ì‹¤ì§ˆì ì¸ ë‚´ìš© ìš”ì•½
    - text(page_content): articleì„ ì œì™¸í•œ ìˆœìˆ˜ ë³¸ë¬¸
    
    Parameters:
    - file_path: docx íŒŒì¼ ê²½ë¡œ
    - priority: RAG ì°¸ì¡° ìš°ì„ ìˆœìœ„ (1~9)
    - output_dir: CSV ì €ì¥ ë””ë ‰í† ë¦¬
    - use_llm_summary: LLM ìš”ì•½ ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ë³¸ë¬¸ ì• 100ìë¡œ ëŒ€ì²´)
    """
    
    # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ê²½ë¡œ ì •ë³´ ì¶”ì¶œ
    if not os.path.exists(file_path):
        print(f"âŒ Error: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []

    file_name = os.path.basename(file_path)
    file_name_no_ext = os.path.splitext(file_name)[0] or "Unknown_Source"
    
    print(f"ğŸš€ ì²˜ë¦¬ ì‹œì‘: {file_name} (ìš°ì„ ìˆœìœ„: {priority})")

    # 2. ë¡œë“œ
    loader = Docx2txtLoader(file_path)
    docs = loader.load()
    origin_text = docs[0].page_content
    
    # 3. ì²­í‚¹
    legal_chunks = split_by_article_safe(origin_text)
    print(f"   - ì²­í‚¹ ì™„ë£Œ: {len(legal_chunks)}ê°œ ì¡°í•­")
    
    # 4. LLM ì´ˆê¸°í™” (ì¬ì‚¬ìš©)
    llm = None
    if use_llm_summary:
        try:
            llm = ChatOllama(model="exaone3.5:2.4b", temperature=0.2, num_predict=150)
        except Exception as e:
            print(f"   âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨, ë‹¨ìˆœ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´: {e}")
            use_llm_summary = False
    
    # 5. ë©”íƒ€ë°ì´í„° ì…íˆê¸°
    processed_documents = []
    
    for i, chunk in enumerate(legal_chunks):
        # (1) ì¹´í…Œê³ ë¦¬ ë¶„ì„
        categories = categorize_content(chunk, top_k=None)
        if not categories:
            categories = ["ê¸°íƒ€"]
        
        # (2) [ì‹ ê·œ] article: ì¡°í•­ë²ˆí˜¸ì™€ ì œëª©ë§Œ ì¶”ì¶œ
        article_header = extract_article_header(chunk)
        
        # (3) [ì‹ ê·œ] text: articleì„ ì œì™¸í•œ ìˆœìˆ˜ ë³¸ë¬¸
        body_text = extract_body_text(chunk)
        
        # (4) [ì‹ ê·œ] title: ì‹¤ì§ˆì ì¸ ë‚´ìš© ìš”ì•½
        if use_llm_summary and article_header.startswith("ì œ"):
            # ë²•ë¥  ì¡°ë¬¸ì¸ ê²½ìš°ì—ë§Œ LLM ìš”ì•½ ìˆ˜í–‰
            title_summary = summarize_article_with_llm(chunk, article_header, llm)
        else:
            # ì„œë¬¸ì´ë‚˜ ë¶€ì¹™ ë“±ì€ ë‹¨ìˆœ ì²˜ë¦¬
            title_summary = body_text[:100] + "..." if len(body_text) > 100 else body_text
        
        # (5) ë©”íƒ€ë°ì´í„° ì¡°ë¦½
        metadata = {
            "chunk_id": f"{file_name_no_ext}_{i:03d}",
            "priority": int(priority),
            "category": categories,
            "article": str(article_header).strip(),      # [ë³€ê²½] ì¡°í•­ë²ˆí˜¸+ì œëª©
            "title": str(title_summary).strip(),         # [ë³€ê²½] ì‹¤ì§ˆì  ìš”ì•½
            "source": str(file_name).strip(),
            "src_title": str(file_name_no_ext).strip(),
        }
        
        # (6) [ë³€ê²½] page_contentëŠ” article ì œì™¸í•œ ë³¸ë¬¸ë§Œ
        doc = Document(page_content=str(body_text).strip(), metadata=metadata)
        processed_documents.append(doc)
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ (10ê°œë§ˆë‹¤)
        if (i + 1) % 10 == 0:
            print(f"   - {i + 1}/{len(legal_chunks)} ì²˜ë¦¬ ì™„ë£Œ...")
        
    # 6. CSV ì €ì¥
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    df_data = []
    for doc in processed_documents:
        row = doc.metadata.copy()
        row['page_content'] = doc.page_content
        row['category'] = str(row['category'])  # ë¦¬ìŠ¤íŠ¸ -> ë¬¸ìì—´ ë³€í™˜ (CSVìš©)
        df_data.append(row)
        
    df = pd.DataFrame(df_data)
    csv_path = os.path.join(output_dir, f"{file_name_no_ext}_v3.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_path} ({len(legal_chunks)}ê°œ ì¡°í•­)")
    print("-" * 60)
    
    return processed_documents


# ============================================================================
# 5. [ì‹ ê·œ] ë‹¤ì¤‘ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def process_multiple_files_v3(target_files, output_dir='./data/1-2_chunked/', use_llm_summary=True):
    """
    ì—¬ëŸ¬ íŒŒì¼ì„ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Parameters:
    - target_files: [{"path": "...", "priority": N}, ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    - output_dir: CSV ì €ì¥ ë””ë ‰í† ë¦¬
    - use_llm_summary: LLM ìš”ì•½ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
    - all_docs: ëª¨ë“  ì²˜ë¦¬ëœ Document ë¦¬ìŠ¤íŠ¸
    """
    all_docs = []
    
    for item in target_files:
        docs = process_single_file_v3(
            file_path=item["path"],
            priority=item["priority"],
            output_dir=output_dir,
            use_llm_summary=use_llm_summary
        )
        all_docs.extend(docs)
    
    print(f"\nâœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì´ {len(all_docs)}ê°œì˜ ì¡°í•­ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_docs


# ============================================================================
# 6. Pinecone Upsert í•¨ìˆ˜
# ============================================================================

def upsert_to_pinecone(all_docs, index_name, pc_api_key, up_api_key):
    """
    ì²˜ë¦¬ëœ Documentsë¥¼ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    
    Parameters:
    - all_docs: Document ë¦¬ìŠ¤íŠ¸
    - index_name: "law-index-final" ë˜ëŠ” "rule-index-final"
    - pc_api_key: Pinecone API Key
    - up_api_key: Upstage API Key
    """
    from langchain_upstage import UpstageEmbeddings
    from langchain_pinecone import PineconeVectorStore
    from pinecone import Pinecone, ServerlessSpec
    
    print(f"\nğŸ”„ Pinecone '{index_name}' ì¸ë±ìŠ¤ì— ì—…ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“¦ ì´ {len(all_docs)}ê°œì˜ ë¬¸ì„œê°€ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.")
    
    # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    pc = Pinecone(api_key=pc_api_key)
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (Upstage Solar: 4096 ì°¨ì›)
    embedding = UpstageEmbeddings(
        model="solar-embedding-1-large-passage",
        api_key=up_api_key
    )
    
    # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
    if index_name not in pc.list_indexes().names():
        print(f"âš™ï¸ '{index_name}' ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤ (ì°¨ì›: 4096)...")
        pc.create_index(
            name=index_name,
            dimension=4096,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
        # ì¸ë±ìŠ¤ í™œì„±í™” ëŒ€ê¸°
        while not pc.describe_index(index_name).status['ready']:
            time.sleep(1)
        print("   - ì¸ë±ìŠ¤ ìƒì„± ë° í™œì„±í™” ì™„ë£Œ")
    else:
        print(f"â„¹ï¸ '{index_name}' ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    
    # ì—…ë¡œë“œ
    try:
        database = PineconeVectorStore.from_documents(
            documents=all_docs,
            embedding=embedding,
            index_name=index_name,
            pinecone_api_key=pc_api_key
        )
        print(f"\nğŸ‰ [ì„±ê³µ] ëª¨ë“  ë¬¸ì„œê°€ '{index_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return database
        
    except Exception as e:
        print(f"\nğŸ”¥ ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        print("ğŸ’¡ Tip: ë©”íƒ€ë°ì´í„°ì— ë¦¬ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ë¬¸ìì—´ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None


# ============================================================================
# 7. [ì‹ ê·œ] LLM Context í¬ë§·íŒ… í•¨ìˆ˜
# ============================================================================

def format_for_llm_context(docs):
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLM contextìš© ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    
    í˜•ì‹: "{src_title} - {article}. {text}"
    
    ì˜ˆì‹œ ì¶œë ¥:
    "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•(ë²•ë¥ )(ì œ21065í˜¸)(20260102) - ì œ3ì¡°(ëŒ€í•­ë ¥ ë“±). 
     â‘  ì„ëŒ€ì°¨ëŠ” ê·¸ ë“±ê¸°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì„ì°¨ì¸ì´ ì£¼íƒì˜ ì¸ë„ì™€ ì£¼ë¯¼ë“±ë¡ì„ 
     ë§ˆì¹œ ë•Œì—ëŠ” ê·¸ ë‹¤ìŒ ë‚ ë¶€í„° ì œì‚¼ìì— ëŒ€í•˜ì—¬ íš¨ë ¥ì´ ìƒê¸´ë‹¤..."
    """
    formatted_texts = []
    
    for i, doc in enumerate(docs):
        src_title = doc.metadata.get('src_title', 'ì¶œì²˜ ë¯¸ìƒ')
        article = doc.metadata.get('article', '')
        text = doc.page_content
        title = doc.metadata.get('title', '')
        priority = doc.metadata.get('priority', 9)
        
        # LLM Context í˜•ì‹
        context_text = f"[{i+1}] {src_title} - {article}\n{text}"
        formatted_texts.append(context_text)
    
    return "\n\n---\n\n".join(formatted_texts)


def format_for_display(docs):
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì í™”ë©´ í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    """
    display_items = []
    
    for i, doc in enumerate(docs):
        item = {
            "rank": i + 1,
            "src_title": doc.metadata.get('src_title', 'ì¶œì²˜ ë¯¸ìƒ'),
            "article": doc.metadata.get('article', ''),
            "title": doc.metadata.get('title', ''),
            "text_preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
            "priority": doc.metadata.get('priority', 9),
            "category": doc.metadata.get('category', ['ê¸°íƒ€'])
        }
        display_items.append(item)
    
    return display_items


# ============================================================================
# 8. í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("RAG ë²•ë ¹ ì¸ë±ìŠ¤ ì²˜ë¦¬ - ê°œì„ ëœ ë©”íƒ€ë°ì´í„° ë²„ì „ (v3)")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ìš© ì˜ˆì‹œ í…ìŠ¤íŠ¸
    test_content = """ì œ3ì¡° (ëŒ€í•­ë ¥ ë“±) â‘  ì„ëŒ€ì°¨ëŠ” ê·¸ ë“±ê¸°(ç™»è¨˜)ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì„ì°¨ì¸(è³ƒå€Ÿäºº)ì´ ì£¼íƒì˜ ì¸ë„(å¼•æ¸¡)ì™€ ì£¼ë¯¼ë“±ë¡ì„ ë§ˆì¹œ ë•Œì—ëŠ” ê·¸ ë‹¤ìŒ ë‚ ë¶€í„° ì œì‚¼ìì— ëŒ€í•˜ì—¬ íš¨ë ¥ì´ ìƒê¸´ë‹¤. ì´ ê²½ìš° ì „ì…ì‹ ê³ ë¥¼ í•œ ë•Œì— ì£¼ë¯¼ë“±ë¡ì´ ëœ ê²ƒìœ¼ë¡œ ë³¸ë‹¤.
â‘¡ ì£¼íƒë„ì‹œê¸°ê¸ˆì„ ì¬ì›ìœ¼ë¡œ í•˜ì—¬ ì €ì†Œë“ì¸µ ë¬´ì£¼íƒìì—ê²Œ ì£¼ê±°ìƒí™œ ì•ˆì •ì„ ëª©ì ìœ¼ë¡œ ì „ì„¸ì„ëŒ€ì£¼íƒì„ ì§€ì›í•˜ëŠ” ë²•ì¸ì´ ì£¼íƒì„ ì„ì°¨í•œ í›„ ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¥ ë˜ëŠ” ê·¸ ë²•ì¸ì´ ì„ ì •í•œ ì…ì£¼ìê°€ ê·¸ ì£¼íƒì„ ì¸ë„ë°›ê³  ì£¼ë¯¼ë“±ë¡ì„ ë§ˆì³¤ì„ ë•Œì—ëŠ” ì œ1í•­ì„ ì¤€ìš©í•œë‹¤."""

    print("\n[í…ŒìŠ¤íŠ¸] ê°œì„ ëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ")
    print("-" * 50)
    
    # article ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    article = extract_article_header(test_content)
    print(f"article: {article}")
    
    # body ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    body = extract_body_text(test_content)
    print(f"body (ì• 150ì): {body[:150]}...")
    
    print("\n[í…ŒìŠ¤íŠ¸] ì¶”ê°€ ì˜ˆì‹œ")
    print("-" * 50)
    
    # ë¯¼ë²• ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸
    test_minbeop = "ì œ618ì¡° (ì„ëŒ€ì°¨ì˜ ì˜ì˜) ì„ëŒ€ì°¨ëŠ” ë‹¹ì‚¬ì ì¼ë°©ì´ ìƒëŒ€ë°©ì—ê²Œ ëª©ì ë¬¼ì„ ì‚¬ìš©, ìˆ˜ìµí•˜ê²Œ í•  ê²ƒì„ ì•½ì •í•˜ê³  ìƒëŒ€ë°©ì´ ì´ì— ëŒ€í•˜ì—¬ ì°¨ì„ì„ ì§€ê¸‰í•  ê²ƒì„ ì•½ì •í•¨ìœ¼ë¡œì¨ ê·¸ íš¨ë ¥ì´ ìƒê¸´ë‹¤."
    print(f"ë¯¼ë²• article: {extract_article_header(test_minbeop)}")
    print(f"ë¯¼ë²• body: {extract_body_text(test_minbeop)}")
    
    # ì œNì¡°ì˜M ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸
    test_jo_ui = "ì œ1ì¡°ì˜2(ì†Œì•¡ì‚¬ê±´ì˜ ë²”ìœ„) ë²• ì œ2ì¡°ì œ1í•­ì— ë”°ë¥¸ ì†Œì•¡ì‚¬ê±´ì€ ì œì†Œí•œ ë•Œì˜ ì†Œì†¡ëª©ì ì˜ ê°’ì´ 3,000ë§Œì›ì„ ì´ˆê³¼í•˜ì§€ ì•„ë‹ˆí•˜ëŠ” ê¸ˆì „ ê¸°íƒ€ ëŒ€ì²´ë¬¼ì´ë‚˜ ìœ ê°€ì¦ê¶Œì˜ ì¼ì •í•œ ìˆ˜ëŸ‰ì˜ ì§€ê¸‰ì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ì œ1ì‹¬ì˜ ë¯¼ì‚¬ì‚¬ê±´ìœ¼ë¡œ í•œë‹¤."
    print(f"ì œNì¡°ì˜M article: {extract_article_header(test_jo_ui)}")
    print(f"ì œNì¡°ì˜M body: {extract_body_text(test_jo_ui)}")
    
    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ. ì‹¤ì œ ì‚¬ìš© ì‹œ process_single_file_v3() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
    print("=" * 70)
