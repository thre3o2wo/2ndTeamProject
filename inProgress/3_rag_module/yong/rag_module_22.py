"""
RAG LLM Pipeline for Django Web Application
ì£¼íƒì„ëŒ€ì°¨ RAG ì‹œìŠ¤í…œ - LLM íŠœë‹ ë° íŒŒì´í”„ë¼ì¸

ì´ ëª¨ë“ˆì€ Jupyter ë…¸íŠ¸ë¶ì˜ Section #2 (LLM Tuning)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
Django ë“± ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì¤€í™” (normalize_query)
2. 3ì¤‘ ì¸ë±ìŠ¤ í†µí•© ê²€ìƒ‰ (triple_hybrid_retrieval)
3. ìµœì¢… ë‹µë³€ ìƒì„± (generate_final_answer)

ì°¸ê³ : LLM ëª¨ë¸ì€ í•„ìš”ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import os
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.documents import Document

# Vector DB imports
from pinecone import Pinecone

# Reranking import
try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False
    print("âš ï¸ Warning: cohere library not installed. Reranking will be disabled.")


# ==========================================
# Section 2-1: Normalizing Query Input
# ==========================================

# ì£¼íƒì„ëŒ€ì°¨ ì±—ë´‡ ì§ˆë¬¸ í‘œì¤€í™” ì‚¬ì „
KEYWORD_DICT = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ì„ëŒ€ì¸", "ì„ëŒ€ì—…ì": "ì„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ì„ëŒ€ì¸",
    "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëŒ": "ì„ì°¨ì¸", "ì„ì°¨ì": "ì„ì°¨ì¸", "ì…ì£¼ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ì„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ì„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ì„ì°¨ì£¼íƒ", "ìš°ë¦¬ì§‘": "ì„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ì„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „ (ë³´ì¦ê¸ˆ_ëŒ€í•­ë ¥, ì„ëŒ€ë£Œ_ì¦ê°)
    "ì „ì„¸ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë§¡ê¸´ëˆ": "ì„ì°¨ë³´ì¦ê¸ˆ", "ë–¼ì¸ëˆ": "ì„ì°¨ë³´ì¦ê¸ˆ",
    "ì›”ì„¸": "ì°¨ì„", "ë°©ì„¸": "ì°¨ì„", "ë‹¤ë‹¬ì´ë‚´ëŠ”ì§€ì¶œ": "ì°¨ì„", "ë ŒíŠ¸ë¹„": "ì°¨ì„", "ì„ëŒ€ë£Œ": "ì°¨ì„",
    "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ": "ì¤‘ê°œë³´ìˆ˜", "ì¤‘ê°œë¹„": "ì¤‘ê°œë³´ìˆ˜",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ë”ë‹¬ë¼ê³ í•¨": "ì¦ì•¡", "5í”„ë¡œ": "5í¼ì„¼íŠ¸ìƒí•œ",
    "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡", "ë‚´ë¦¬ê¸°": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥", "ëŒë ¤ë°›ê¸°": "ë³´ì¦ê¸ˆë°˜í™˜",
    "ë³´í—˜": "ë°˜í™˜ë³´ì¦", "í—ˆê·¸": "HUG", "ë‚˜ë¼ë³´ì¦": "ë³´ì¦ë³´í—˜",

    # 3. ê³„ì•½ ìƒíƒœ ë° ë³€í™” (ê³„ì•½ê°±ì‹ , ê³„ì•½í•´ì§€_ëª…ë„)
    "ì—°ì¥í•˜ê¸°": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "í•œë²ˆë”ì‚´ê¸°": "ê³„ì•½ê°±ì‹ ", "2í”ŒëŸ¬ìŠ¤2": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ", "ì—°ë½ì—†ìŒ": "ë¬µì‹œì ê°±ì‹ ", "ê·¸ëƒ¥ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„", "ë°©ë¹¼": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ë§¤ë§¤": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„", "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½ (ìˆ˜ì„ _ì›ìƒíšŒë³µ, ìƒí™œí™˜ê²½_íŠ¹ì•½)
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´", "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ì", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ì¥": "í•˜ì", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ë˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ", "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½", "ë‹´ë°°": "í¡ì—°ê¸ˆì§€íŠ¹ì•½",

    # 5. ë¦¬ìŠ¤í¬ ë° ë¶„ìŸ (ê¶Œë¦¬_ì •ë³´ë¦¬ìŠ¤í¬, ë¶„ìŸí•´ê²°)
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ê²½ë§¤ë„˜ì–´ê°": "ê¶Œë¦¬ë¦¬ìŠ¤í¬", "ë¹š": "ê·¼ì €ë‹¹",
    "ì„¸ê¸ˆì•ˆëƒ„": "ì²´ë‚©", "ë‚˜ë¼ë¹š": "ì¡°ì„¸ì±„ê¶Œ", "ë¹Œë¦°ëˆ": "ê°€ì••ë¥˜", "ì‹ íƒ": "ì‹ íƒë¶€ë™ì‚°",
    "íŠ¹ì•½": "íŠ¹ì•½ì‚¬í•­", "ë¶ˆê³µì •": "ê°•í–‰ê·œì •ìœ„ë°˜", "ë…ì†Œì¡°í•­": "ë¶ˆë¦¬í•œì•½ì •", "íš¨ë ¥ìˆë‚˜": "ë¬´íš¨ì—¬ë¶€",
    "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •", "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ì„ì°¨ê¶ŒìŠ¹ê³„", "ìì‹ìƒì†": "ì„ì°¨ê¶ŒìŠ¹ê³„"
}


def normalize_query(user_query: str, llm_model: str = "exaone3.5:2.4b") -> str:
    """
    KEYWORD_DICTë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë²•ë¥  ìš©ì–´ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    
    Args:
        user_query: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
        llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: exaone3.5:2.4b, í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
    
    Returns:
        í‘œì¤€í™”ëœ ì§ˆë¬¸ ë¬¸ìì—´
    """
    # LLM ì„¤ì • (ì „ì²˜ë¦¬ëŠ” ì°½ì˜ì„±ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ temperature=0ìœ¼ë¡œ ì„¤ì •)
    response_llm = ChatOllama(model=llm_model, temperature=0)
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê³ ë„í™”
    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤. 
ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë‹¨ì–´ë¥¼ ë³€ê²½í•  ë•Œ ë¬¸ë§¥ì— ë§ê²Œ ì¡°ì‚¬(ì´/ê°€, ì„/ë¥¼ ë“±)ë‚˜ ì„œìˆ ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
4. ì˜¤ì§ 'ë³€ê²½ëœ ì§ˆë¬¸' í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ê¸ˆì§€)

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:""")
    
    # ì²´ì¸ ìƒì„±
    keyword_chain = prompt | response_llm | StrOutputParser()
    
    try:
        normalized = keyword_chain.invoke({
            "dictionary": KEYWORD_DICT, 
            "question": user_query
        })
        return normalized.strip()
    except Exception as e:
        print(f"âš ï¸ ì „ì²˜ë¦¬ ì—ëŸ¬: {e}")
        return user_query


# ==========================================
# Section 2-2: RAG ì¡°íšŒ í•¨ìˆ˜ ì •ì˜
# ==========================================

def initialize_vector_stores(
    pc_api_key: Optional[str] = None,
    up_api_key: Optional[str] = None
) -> tuple:
    """
    Pinecone VectorStore 3ê°œ ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    
    Args:
        pc_api_key: Pinecone API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        up_api_key: Upstage API í‚¤ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    
    Returns:
        (law_store, rule_store, case_store) íŠœí”Œ
    """
    # í™˜ê²½ ì„¤ì •
    load_dotenv(override=True)
    pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
    up_api_key = up_api_key or os.getenv("UPSTAGE_API_KEY")
    
    if not pc_api_key or not up_api_key:
        raise ValueError("PINECONE_API_KEYì™€ UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # Pinecone & Embedding ì´ˆê¸°í™”
    pc = Pinecone(api_key=pc_api_key)
    embedding = UpstageEmbeddings(model="solar-embedding-1-large-passage")
    
    print("ğŸ”— 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì‹œë„...")
    
    # (1) Law Index: ì£¼ì„ë²•, ë¯¼ë²• ë“± í•µì‹¬ ë²•ë¥  (Priority 1,2,4,5)
    law_store = PineconeVectorStore(
        index_name="law-index-final",
        embedding=embedding,
        pinecone_api_key=pc_api_key
    )
    
    # (2) Rule Index: ì‹œí–‰ê·œì¹™, ì¡°ë¡€, ì ˆì°¨ ë“± (Priority 3,6,7,8,11)
    rule_store = PineconeVectorStore(
        index_name="rule-index-final",
        embedding=embedding,
        pinecone_api_key=pc_api_key
    )
    
    # (3) Case Index: íŒë¡€, ìƒë‹´ì‚¬ë¡€ (Priority 9)
    case_store = PineconeVectorStore(
        index_name="case-index-final",
        embedding=embedding,
        pinecone_api_key=pc_api_key
    )
    
    print("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
    
    return law_store, rule_store, case_store


def get_full_case_context(case_no: str, case_index: PineconeVectorStore, top_k: int = 50) -> str:
    """
    íŠ¹ì • ì‚¬ê±´ë²ˆí˜¸(case_no)ë¥¼ ê°€ì§„ ëª¨ë“  ì²­í¬ë¥¼ ê°€ì ¸ì™€ì„œ íŒë¡€ ì „ë¬¸ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
    
    Args:
        case_no: ì‚¬ê±´ë²ˆí˜¸ (ì˜ˆ: "2020ë‚˜56247")
        case_index: Case VectorStore ê°ì²´
        top_k: ìµœëŒ€ ê²€ìƒ‰ ê°œìˆ˜
    
    Returns:
        íŒë¡€ ì „ë¬¸ í…ìŠ¤íŠ¸
    """
    try:
        # ë”ë¯¸ ì¿¼ë¦¬ ì‚¬ìš©ìœ¼ë¡œ API ì—ëŸ¬ ë°©ì§€
        results = case_index.similarity_search(
            query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰", 
            k=top_k, 
            filter={"case_no": {"$eq": case_no}}
        )
        
        # chunk_id ìˆœ ì •ë ¬
        sorted_docs = sorted(results, key=lambda x: x.metadata.get('chunk_id', ''))
        
        # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        seen_chunks = set()
        unique_docs = []
        for doc in sorted_docs:
            cid = doc.metadata.get('chunk_id')
            if cid and cid not in seen_chunks:
                unique_docs.append(doc)
                seen_chunks.add(cid)
        
        full_text = "\n".join([doc.page_content for doc in unique_docs])
        return full_text
        
    except Exception as e:
        print(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
        return ""


def triple_hybrid_retrieval(
    query: str, 
    law_store: PineconeVectorStore, 
    rule_store: PineconeVectorStore, 
    case_store: PineconeVectorStore, 
    k_law: int = 5, 
    k_rule: int = 5, 
    k_case: int = 3, 
    score_threshold: float = 0.2,
    cohere_api_key: Optional[str] = None
) -> List[Document]:
    """
    1ë‹¨ê³„: Law, Rule, Case ì¸ë±ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
    2ë‹¨ê³„: Rerankë¡œ ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œ ì„ ë³„
    3ë‹¨ê³„: Priority ë©”íƒ€ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ 'ë²•ì  ìœ„ê³„' ì •ë ¬í•˜ì—¬ ë°˜í™˜
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        law_store: Law VectorStore
        rule_store: Rule VectorStore
        case_store: Case VectorStore
        k_law: Lawì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        k_rule: Ruleì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        k_case: Caseì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        score_threshold: Rerank ì ìˆ˜ ì„ê³„ê°’
        cohere_api_key: Cohere API í‚¤ (ì„ íƒ)
    
    Returns:
        ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬ëœ Document ë¦¬ìŠ¤íŠ¸
    """
    print(f"ğŸ” [í†µí•© ê²€ìƒ‰] ì¿¼ë¦¬: '{query}'")
    
    # 1. ë³‘ë ¬ ê²€ìƒ‰ (Parallel Retrieval)
    # (A) Law: ë²•ì  ê·¼ê±° (ì˜ˆ: ì£¼ì„ë²• ì œ3ì¡°)
    docs_law = law_store.similarity_search(query, k=k_law * 2)
    
    # (B) Rule: í–‰ì • ì ˆì°¨ ë° ì„œì‹ (ì˜ˆ: í™•ì •ì¼ì ë¶€ì—¬ ê·œì¹™)
    docs_rule = rule_store.similarity_search(query, k=k_rule * 2)
    
    # (C) Case: ìœ ì‚¬ íŒë¡€ (ì˜ˆ: ëŒ€ë²•ì› 2020ë‹¤...)
    docs_case_initial = case_store.similarity_search(query, k=k_case * 2)
    
    # 2. íŒë¡€ ë¬¸ë§¥ í™•ì¥ (Context Expansion)
    docs_case_expanded = []
    seen_cases = set()
    
    for doc in docs_case_initial:
        case_no = doc.metadata.get('case_no')
        if case_no and case_no not in seen_cases:
            full_text = get_full_case_context(case_no, case_store)
            if full_text:
                # íŒë¡€ ì „ë¬¸ìœ¼ë¡œ êµì²´í•˜ë˜, ì¶œì²˜ í‘œê¸°ë¥¼ ìœ„í•´ ë©”íƒ€ë°ì´í„° ìœ ì§€
                new_doc = doc 
                new_doc.page_content = f"[íŒë¡€ ì „ë¬¸: {doc.metadata.get('title')}]\n{full_text}"
                docs_case_expanded.append(new_doc)
                seen_cases.add(case_no)
            
            if len(docs_case_expanded) >= k_case:
                break
    
    # 3. ë¬¸ì„œ í†µí•© (Law + Rule + Case)
    combined_docs = docs_law + docs_rule + docs_case_expanded
    
    # 4. Reranking (ì¤‘ìš”: ì„œë¡œ ë‹¤ë¥¸ ì¸ë±ìŠ¤ì˜ ì ìˆ˜ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•¨)
    selected_docs = combined_docs  # ê¸°ë³¸ê°’

    if not cohere_api_key:
        cohere_api_key = os.getenv("COHERE_API_KEY")

    if COHERE_AVAILABLE and cohere_api_key:
        try:
            co = cohere.Client(api_key=cohere_api_key)
            docs_content = [d.page_content for d in combined_docs]
            
            # í•œêµ­ì–´ì— íŠ¹í™”ëœ ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©
            rerank_results = co.rerank(
                model="rerank-multilingual-v3.0",
                query=query,
                documents=docs_content,
                top_n=len(combined_docs) 
            )
            
            filtered_docs = []
            print(f"ğŸ“Š Rerank ê²°ê³¼ (ì´ {len(combined_docs)}ê°œ ì¤‘ ì„ ë³„):")
            print(f"ğŸ“Š Rerank ê´€ë ¨ë„ ì ìˆ˜ (Threshold {score_threshold}):")
            for r in rerank_results.results:
                # ê´€ë ¨ë„ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ì€ ê²ƒì€ ì œì™¸ (Noise Filtering)
                if r.relevance_score > score_threshold: 
                    doc = combined_docs[r.index]
                    # ë””ë²„ê¹…ìš© ì¶œë ¥
                    p = doc.metadata.get('priority', 99)
                    t = doc.metadata.get('title', 'Untitled')
                    print(f" - [Score: {r.relevance_score:.4f}] [P-{p}] {t}")
                    filtered_docs.append(doc)
            selected_docs = filtered_docs
            
        except Exception as e:
            print(f"âš ï¸ Rerank ì‹¤íŒ¨ (ê¸°ë³¸ ë³‘í•© ë°˜í™˜): {e}")
            return combined_docs

    # 5. Priority Sorting (ë²•ì  ê¶Œìœ„ ì •ë ¬)
    # priority ìˆ«ì ì˜¤ë¦„ì°¨ìˆœ(1â†’9)ìœ¼ë¡œ ì •ë ¬
    # priorityê°€ ì—†ëŠ” ê²½ìš° 99ë¡œ ì·¨ê¸‰í•˜ì—¬ ë§¨ ë’¤ë¡œ
    sorted_docs = sorted(selected_docs, key=lambda x: int(x.metadata.get('priority', 99)))
    return sorted_docs


# ==========================================
# Section 2-3: LLM Pipeline
# ==========================================

def format_context_with_hierarchy(docs: List[Document]) -> str:
    """
    ë¬¸ì„œë“¤ì„ Priorityì— ë”°ë¼ ê·¸ë£¹í™”í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜.
    
    Args:
        docs: Document ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ìœ„ê³„ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    section_1_law = []   # Priority 1, 2, 4, 5 (ë²•ë¥ , ì‹œí–‰ë ¹)
    section_2_rule = []  # Priority 3, 6, 7, 8, 11 (ê·œì¹™, ì¡°ë¡€)
    section_3_case = []  # Priority 9 (íŒë¡€, í•´ì„)
    
    for doc in docs:
        p = int(doc.metadata.get('priority', 99))
        src = doc.metadata.get('src_title', 'ìë£Œ')
        title = doc.metadata.get('title', '')
        content = doc.page_content
        
        entry = f"[{src}] {title}\n{content}"
        
        if p in [1, 2, 4, 5]:
            section_1_law.append(entry)
        elif p in [3, 6, 7, 8, 11]:
            section_2_rule.append(entry)
        else:
            section_3_case.append(entry)
            
    # LLMì´ ì½ì„ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
    formatted_text = ""
    
    if section_1_law:
        formatted_text += "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" + "\n\n".join(section_1_law) + "\n\n"
    if section_2_rule:
        formatted_text += "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" + "\n\n".join(section_2_rule) + "\n\n"
    if section_3_case:
        formatted_text += "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" + "\n\n".join(section_3_case) + "\n\n"
        
    return formatted_text


def generate_final_answer(
    user_input: str,
    law_store: PineconeVectorStore,
    rule_store: PineconeVectorStore,
    case_store: PineconeVectorStore,
    llm_model: str = "exaone3.5:2.4b",
    temperature: float = 0.1,
    k_law: int = 3,
    k_rule: int = 3,
    k_case: int = 2,
    score_threshold: float = 0.2
) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        user_input: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
        law_store: Law VectorStore
        rule_store: Rule VectorStore
        case_store: Case VectorStore
        llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: exaone3.5:2.4b, í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
        temperature: LLM temperature ì„¤ì •
        k_law: Lawì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        k_rule: Ruleì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        k_case: Caseì—ì„œ ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        score_threshold: Rerank ì ìˆ˜ ì„ê³„ê°’
    
    Returns:
        ìµœì¢… ë‹µë³€ ë¬¸ìì—´
    """
    # 1. ì§ˆë¬¸ í‘œì¤€í™”
    try:
        normalized_query = normalize_query(user_input, llm_model=llm_model)
        print(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")
    except:
        normalized_query = user_input
    
    # 2. í†µí•© ê²€ìƒ‰ ë° ìœ„ê³„ ì •ë ¬
    retrieved_docs = triple_hybrid_retrieval(
        normalized_query, 
        law_store, rule_store, case_store,
        k_law=k_law, k_rule=k_rule, k_case=k_case,
        score_threshold=score_threshold
    )
    
    if not retrieved_docs:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3. ìœ„ê³„ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    hierarchical_context = format_context_with_hierarchy(retrieved_docs)

    # 4. LLM í”„ë¡¬í”„íŠ¸ (ìœ„ê³„ êµ¬ì¡° ë°˜ì˜)
    system_prompt = """
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 'ì£¼íƒ ì „ì›”ì„¸ ì‚¬ê¸° ì˜ˆë°© ë° ì„ëŒ€ì°¨ ë²•ë¥  ì „ë¬¸ê°€ AI'ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

    [ë‹µë³€ ìƒì„± ì›ì¹™]
    1. **ë²•ì  ìœ„ê³„ ì¤€ìˆ˜**: 
       - ë°˜ë“œì‹œ [SECTION 1: í•µì‹¬ ë²•ë ¹]ì˜ ë‚´ìš©ì„ ìµœìš°ì„  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìœ¼ì„¸ìš”.
       - [SECTION 1]ì˜ ë‚´ìš©ì´ ëª¨í˜¸í•  ë•Œë§Œ [SECTION 2]ì™€ [SECTION 3]ë¥¼ ë³´ì¶© ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
       - ë§Œì•½ [SECTION 3: íŒë¡€]ê°€ [SECTION 1: ë²•ë ¹]ê³¼ ë‹¤ë¥´ê²Œ í•´ì„ë˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¼ë©´, "ì›ì¹™ì€ ë²•ë ¹ì— ë”°ë¥´ë‚˜, íŒë¡€ëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ..."ë¼ê³  ì„¤ëª…í•˜ì„¸ìš”.
    
    2. **ë‹µë³€ êµ¬ì¡°**:
       - **í•µì‹¬ ê²°ë¡ **: ì§ˆë¬¸ì— ëŒ€í•œ ê²°ë¡ ì„ ë‘ê´„ì‹ìœ¼ë¡œ ìš”ì•½.
       - **ë²•ì  ê·¼ê±°**: "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œOì¡°ì— ë”°ë¥´ë©´..." (SECTION 1 ì¸ìš©)
       - **ì‹¤ë¬´ ì ˆì°¨**: í•„ìš”ì‹œ ì‹ ê³  ë°©ë²•, ì„œë¥˜ ë“± ì•ˆë‚´ (SECTION 2 ì¸ìš©)
       - **ì°¸ê³  ì‚¬ë¡€**: ìœ ì‚¬í•œ ìƒí™©ì—ì„œì˜ íŒê²°ì´ë‚˜ í•´ì„ (SECTION 3 ì¸ìš©)
       
    3. **ì£¼ì˜ì‚¬í•­**:
       - ì‚¬ìš©ìì˜ ê³„ì•½ì„œ ë‚´ìš©ì´ ë²•ë ¹(ê°•í–‰ê·œì •)ì— ìœ„ë°˜ë˜ë©´ "íš¨ë ¥ì´ ì—†ë‹¤(ë¬´íš¨)"ê³  ëª…í™•íˆ ê²½ê³ í•˜ì„¸ìš”.
       - ë²•ë¥ ì  ì¡°ì–¸ì¼ ë¿ì´ë¯€ë¡œ, ìµœì¢…ì ìœ¼ë¡œëŠ” ë³€í˜¸ì‚¬ ë“±ì˜ ì „ë¬¸ê°€ í™•ì¸ì´ í•„ìš”í•¨ì„ ê³ ì§€í•˜ì„¸ìš”.

    [ë²•ì  ìœ„ê³„ê°€ ì •ë¦¬ëœ ì°¸ê³  ë¬¸ì„œ]
    {context}
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{question}"),
    ])
    
    llm = ChatOllama(model=llm_model, temperature=temperature)
    chain = prompt | llm | StrOutputParser()
    
    print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
    return chain.invoke({"context": hierarchical_context, "question": normalized_query})


# ==========================================
# ì‚¬ìš© ì˜ˆì‹œ (Django Viewì—ì„œ ì‚¬ìš©)
# ==========================================

if __name__ == "__main__":
    """
    Djangoì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ:
    
    # views.py
    from .rag_llm_pipeline import initialize_vector_stores, generate_final_answer
    
    # ì•± ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
    law_store, rule_store, case_store = initialize_vector_stores()
    
    # View í•¨ìˆ˜ì—ì„œ ì‚¬ìš©
    def chat_view(request):
        user_question = request.POST.get('question')
        answer = generate_final_answer(
            user_question,
            law_store,
            rule_store,
            case_store
        )
        return JsonResponse({'answer': answer})
    """
    
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("=" * 60)
    print("RAG LLM Pipeline í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # VectorStore ì´ˆê¸°í™”
    law_store, rule_store, case_store = initialize_vector_stores()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "ì§‘ì£¼ì¸ì´ ì›”ì„¸ë¥¼ ì˜¬ë ¤ ë‹¬ë˜ìš”. ì œê°€ ì–´ë µë‹¤ê³  í•˜ë‹ˆê¹Œ ì €ë³´ê³  ë‹¤ìŒ ë‹¬ê¹Œì§€ ë‚˜ê°€ë¼ê³  í•˜ë„¤ìš”. ê³„ì•½ì„œì—ëŠ” ê·¸ëŸ° ì–˜ê¸°ê°€ ì—†ëŠ”ë° ì´ë ‡ê²Œ ì«“ê²¨ë‚˜ë„ ë˜ë‚˜ìš”?"
    
    # ë‹µë³€ ìƒì„±
    answer = generate_final_answer(
        test_query,
        law_store,
        rule_store,
        case_store
    )
    
    print("\n" + "=" * 60)
    print("ìµœì¢… ë‹µë³€:")
    print("=" * 60)
    print(answer)
