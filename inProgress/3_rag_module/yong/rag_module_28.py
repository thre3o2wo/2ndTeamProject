""" 
Unified Hybrid RAG module (Dense + Sparse) for Korean legal Q&A (e.g., housing lease).

Best-of fusion based on your three candidates:
- final_module_gpt.py: robust pipeline + optional deps + candidate-level BM25 fusion
- final_module_cld.py: modular fusion/rerank utilities (but removed Ollama/EXAONE fallback in this final)
- final_module_gmn.py: optional global-BM25 (true sparse search) pattern

Model choices (as requested)
- normalize_query(): Upstage SOLAR Pro2 (chat)  -> model="solar-pro2"
- generate_answer(): OpenAI GPT-4o-mini         -> model="gpt-4o-mini"
- embeddings (dense retrieval): Upstage SOLAR embedding (configurable)

Hybrid retrieval (Dense + Sparse)
- Dense: PineconeVectorStore similarity_search_with_score (fallback: similarity_search)
- Sparse:
  * default: BM25 on *dense candidates* (no extra corpus preload)
  * optional: *global BM25* (true sparse retrieval) if you call build_global_bm25(...)
- Fusion: rank-based RRF (default) or rank_sum

Environment variables
- PINECONE_API_KEY (required)
- UPSTAGE_API_KEY  (required for Upstage embeddings & normalize_query)
- OPENAI_API_KEY   (required for generate_answer)
- COHERE_API_KEY   (optional, only if enable_rerank=True)

No FastAPI integration. Pure Python module.
"""
from __future__ import annotations

import logging
import math
import os
import re
import heapq
from abc import ABC, abstractmethod
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

from langchain_pinecone import PineconeVectorStore

# ----------------------------
# Optional: Upstage (embeddings + chat)
# ----------------------------
try:
    from langchain_upstage import UpstageEmbeddings, ChatUpstage  # type: ignore
    UPSTAGE_AVAILABLE = True
except Exception:
    UpstageEmbeddings = None  # type: ignore
    ChatUpstage = None  # type: ignore
    UPSTAGE_AVAILABLE = False

# ----------------------------
# Optional: OpenAI chat
# ----------------------------
try:
    from langchain_openai import ChatOpenAI  # type: ignore
    OPENAI_AVAILABLE = True
except Exception:
    ChatOpenAI = None  # type: ignore
    OPENAI_AVAILABLE = False

# ----------------------------
# Optional: BM25 (rank_bm25)
# ----------------------------
try:
    from rank_bm25 import BM25Okapi, BM25Plus  # type: ignore
    BM25_AVAILABLE = True
except Exception:
    BM25Okapi = None  # type: ignore
    BM25Plus = None  # type: ignore
    BM25_AVAILABLE = False

# ----------------------------
# Optional: Kiwi tokenizer
# ----------------------------
try:
    from kiwipiepy import Kiwi  # type: ignore
    KIWI_AVAILABLE = True
except Exception:
    Kiwi = None  # type: ignore
    KIWI_AVAILABLE = False

# ----------------------------
# Optional: Cohere rerank
# ----------------------------
try:
    import cohere  # type: ignore
    COHERE_AVAILABLE = True
except Exception:
    cohere = None  # type: ignore
    COHERE_AVAILABLE = False


# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# ----------------------------
# Pinecone helper (best-effort)
# ----------------------------
def _list_pinecone_indexes(api_key: str) -> Optional[List[str]]:
    """Best-effort Pinecone index listing for better error messages."""
    try:
        from pinecone import Pinecone  # type: ignore
        pc = Pinecone(api_key=api_key)
        # Newer clients return an object with .names() or list-like; handle both.
        idx = pc.list_indexes()
        if hasattr(idx, "names"):
            return list(idx.names())  # type: ignore[arg-type]
        # some versions return list[dict] or list[str]
        if isinstance(idx, list):
            names = []
            for it in idx:
                if isinstance(it, str):
                    names.append(it)
                elif isinstance(it, dict) and "name" in it:
                    names.append(it["name"])
            return names or None
        return None
    except Exception:
        return None


# --------------------------------------------------------------------------------------
# Index names
# --------------------------------------------------------------------------------------
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index",
    "rule": "rule-index",
    "case": "case-index",
}


# --------------------------------------------------------------------------------------
# Keyword dictionary (query normalization)
# --------------------------------------------------------------------------------------
KEYWORD_DICT: Dict[str, str] = {
    # 1. Í≥ÑÏïΩ Ï£ºÏ≤¥ Î∞è ÎåÄÏÉÅ
    "ÏßëÏ£ºÏù∏": "ÏûÑÎåÄÏù∏", "Í±¥Î¨ºÏ£º": "ÏûÑÎåÄÏù∏", "Ï£ºÏù∏Ïßë": "ÏûÑÎåÄÏù∏",
    "ÏûÑÎåÄÏóÖÏûê": "ÏûÑÎåÄÏù∏", "ÏÉàÏ£ºÏù∏": "ÏûÑÎåÄÏù∏",
    "ÏÑ∏ÏûÖÏûê": "ÏûÑÏ∞®Ïù∏", "ÏõîÏÑ∏ÏûÖÏûê": "ÏûÑÏ∞®Ïù∏", "ÏÑ∏Îì§Ïñ¥ÏÇ¨ÎäîÏÇ¨Îûå": "ÏûÑÏ∞®Ïù∏",
    "ÏûÑÏ∞®Ïûê": "ÏûÑÏ∞®Ïù∏", "ÏûÖÏ£ºÏûê": "ÏûÑÏ∞®Ïù∏",
    "Î∂ÄÎèôÏÇ∞": "Í≥µÏù∏Ï§ëÍ∞úÏÇ¨", "Ï§ëÍ∞úÏù∏": "Í≥µÏù∏Ï§ëÍ∞úÏÇ¨", "Ï§ëÍ∞úÏÜå": "Í≥µÏù∏Ï§ëÍ∞úÏÇ¨",
    "ÎπåÎùº": "ÏûÑÏ∞®Ï£ºÌÉù", "ÏïÑÌååÌä∏": "ÏûÑÏ∞®Ï£ºÌÉù", "Ïò§ÌîºÏä§ÌÖî": "ÏûÑÏ∞®Ï£ºÌÉù",
    "Ïö∞Î¶¨Ïßë": "ÏûÑÏ∞®Ï£ºÌÉù", "Í±∞Ï£ºÏßÄ": "ÏûÑÏ∞®Ï£ºÌÉù",
    "Í≥ÑÏïΩÏÑú": "ÏûÑÎåÄÏ∞®Í≥ÑÏïΩÏ¶ùÏÑú", "ÏßëÎ¨∏ÏÑú": "ÏûÑÎåÄÏ∞®Í≥ÑÏïΩÏ¶ùÏÑú", "Ï¢ÖÏù¥": "ÏûÑÎåÄÏ∞®Í≥ÑÏïΩÏ¶ùÏÑú",

    # 2. Î≥¥Ï¶ùÍ∏à Î∞è Í∏àÏ†Ñ
    "Î≥¥Ï¶ùÍ∏à": "ÏûÑÎåÄÏ∞®Î≥¥Ï¶ùÍ∏à", "Ï†ÑÏÑ∏Í∏à": "ÏûÑÎåÄÏ∞®Î≥¥Ï¶ùÍ∏à", "Î≥¥Ï¶ùÎ≥¥Ìóò": "Î≥¥Ï¶ùÍ∏àÎ∞òÌôòÎ≥¥Ï¶ù",
    "ÎèàÎ™ªÎ∞õÏùå": "Î≥¥Ï¶ùÍ∏àÎØ∏Î∞òÌôò", "ÏïàÎèåÎ†§Ï§å": "Î≥¥Ï¶ùÍ∏àÎØ∏Î∞òÌôò", "Î™ªÎèåÎ†§Î∞õÏùå": "Î≥¥Ï¶ùÍ∏àÎØ∏Î∞òÌôò",
    "ÏõîÏÑ∏": "Ï∞®ÏûÑ", "Í¥ÄÎ¶¨ÎπÑ": "Í¥ÄÎ¶¨ÎπÑ", "Ïó∞Ï≤¥": "Ï∞®ÏûÑÏó∞Ï≤¥", "Î∞ÄÎ¶º": "Ï∞®ÏûÑÏó∞Ï≤¥",
    "Î≥µÎπÑ": "Ï§ëÍ∞úÎ≥¥Ïàò", "ÏàòÏàòÎ£å": "Ï§ëÍ∞úÎ≥¥Ïàò", "Ï§ëÍ∞úÎπÑ": "Ï§ëÍ∞úÎ≥¥Ïàò",
    "ÏõîÏÑ∏Ïò¨Î¶¨Í∏∞": "Ï∞®ÏûÑÏ¶ùÏï°", "Ïù∏ÏÉÅ": "Ï¶ùÏï°", "ÎçîÎã¨ÎùºÍ≥†Ìï®": "Ï¶ùÏï°",
    "ÏõîÏÑ∏ÍπéÍ∏∞": "Ï∞®ÏûÑÍ∞êÏï°", "Ìï†Ïù∏": "Í∞êÏï°", "ÎÇ¥Î¶¨Í∏∞": "Í∞êÏï°",
    "ÎèàÎ®ºÏ†ÄÎ∞õÍ∏∞": "Ïö∞ÏÑ†Î≥ÄÏ†úÍ∂å", "ÏàúÏúÑ": "Ïö∞ÏÑ†Î≥ÄÏ†úÍ∂å", "ÏïàÏ†ÑÏû•Ïπò": "ÎåÄÌï≠Î†•",
    "ÎèåÎ†§Î∞õÍ∏∞": "Î≥¥Ï¶ùÍ∏àÎ∞òÌôò",

    # 3. Í∏∞Í∞Ñ Î∞è Ï¢ÖÎ£å/Í∞±Ïã†
    "Ïû¨Í≥ÑÏïΩ": "Í≥ÑÏïΩÍ∞±Ïã†", "Ïó∞Ïû•": "Í≥ÑÏïΩÍ∞±Ïã†", "Í∞±Ïã†": "Í≥ÑÏïΩÍ∞±Ïã†",
    "Í∞±Ïã†Ï≤≠Íµ¨": "Í≥ÑÏïΩÍ∞±Ïã†ÏöîÍµ¨Í∂å", "2ÎÖÑÎçî": "Í≥ÑÏïΩÍ∞±Ïã†ÏöîÍµ¨Í∂å", "2ÌîåÎü¨Ïä§2": "Í≥ÑÏïΩÍ∞±Ïã†ÏöîÍµ¨Í∂å",
    "ÏûêÎèôÏó∞Ïû•": "Î¨µÏãúÏ†ÅÍ∞±Ïã†", "Î¨µÏãú": "Î¨µÏãúÏ†ÅÍ∞±Ïã†", "Ïó∞ÎùΩÏóÜÏùå": "Î¨µÏãúÏ†ÅÍ∞±Ïã†",
    "Ïù¥ÏÇ¨": "Ï£ºÌÉùÏùòÏù∏ÎèÑ", "ÏßêÎπºÍ∏∞": "Ï£ºÌÉùÏùòÏù∏ÎèÑ", "Ìá¥Í±∞": "Ï£ºÌÉùÏùòÏù∏ÎèÑ",
    "Î∞©Îπº": "Í≥ÑÏïΩÌï¥ÏßÄ", "Ï§ëÎèÑÌï¥ÏßÄ": "Í≥ÑÏïΩÌï¥ÏßÄ",
    "Ï£ºÏÜåÏòÆÍ∏∞Í∏∞": "Ï£ºÎØºÎì±Î°ù", "Ï†ÑÏûÖÏã†Í≥†": "Ï£ºÎØºÎì±Î°ù", "Ï£ºÏÜåÏßÄÏù¥Ï†Ñ": "Ï£ºÎØºÎì±Î°ù",
    "ÏßëÏ£ºÏù∏Î∞îÎÄú": "ÏûÑÎåÄÏù∏ÏßÄÏúÑÏäπÍ≥Ñ", "Ï£ºÏù∏Î∞îÎÄú": "ÏûÑÎåÄÏù∏ÏßÄÏúÑÏäπÍ≥Ñ",
    "Îß§Îß§": "ÏûÑÎåÄÏù∏ÏßÄÏúÑÏäπÍ≥Ñ",
    "ÎÇòÍ∞ÄÎùºÍ≥†Ìï®": "Í≥ÑÏïΩÍ∞±Ïã†Í±∞Ï†à", "Ï´ìÍ≤®ÎÇ®": "Î™ÖÎèÑ", "ÎπÑÏõåÎã¨Îùº": "Î™ÖÎèÑ",

    # 4. ÏàòÎ¶¨ Î∞è ÏÉùÌôúÌôòÍ≤Ω
    "ÏßëÍ≥†ÏπòÍ∏∞": "ÏàòÏÑ†ÏùòÎ¨¥", "ÏàòÎ¶¨": "ÏàòÏÑ†ÏùòÎ¨¥", "Í≥†Ï≥êÏ§ò": "ÏàòÏÑ†ÏùòÎ¨¥",
    "ÏïàÍ≥†Ï≥êÏ§å": "ÏàòÏÑ†ÏùòÎ¨¥ÏúÑÎ∞ò",
    "Í≥∞Ìå°Ïù¥": "ÌïòÏûê", "Î¨ºÏÉò": "ÎàÑÏàò", "Î≥¥ÏùºÎü¨Í≥†Ïû•": "ÌïòÏûê", "ÌååÏÜê": "ÌõºÏÜê",
    "Íπ®ÎÅóÏù¥ÏπòÏö∞Í∏∞": "ÏõêÏÉÅÌöåÎ≥µÏùòÎ¨¥", "ÏõêÎûòÎåÄÎ°úÌï¥ÎÜìÍ∏∞": "ÏõêÏÉÅÌöåÎ≥µ",
    "Ï≤≠ÏÜåÎπÑ": "ÏõêÏÉÅÌöåÎ≥µÎπÑÏö©", "Ï≤≠ÏÜå": "ÏõêÏÉÅÌöåÎ≥µ",
    "Ï∏µÍ∞ÑÏÜåÏùå": "Í≥µÎèôÏÉùÌôúÌèâÏò®", "ÏòÜÏßëÏÜåÏùå": "Î∞©Ïùå", "Í∞úÌÇ§Ïö∞Í∏∞": "Î∞òÎ†§ÎèôÎ¨ºÌäπÏïΩ",
    "Îã¥Î∞∞": "Ìù°Ïó∞Í∏àÏßÄÌäπÏïΩ",

    # 5. Í∂åÎ¶¨/ÎåÄÌï≠Î†•/ÌôïÏ†ïÏùºÏûê
    "ÌôïÏ†ïÏùºÏûê": "ÌôïÏ†ïÏùºÏûê", "Ï†ÑÏûÖ": "Ï£ºÎØºÎì±Î°ù", "ÎåÄÌï≠Î†•": "ÎåÄÌï≠Î†•",
    "Ïö∞ÏÑ†Î≥ÄÏ†ú": "Ïö∞ÏÑ†Î≥ÄÏ†úÍ∂å", "ÏµúÏö∞ÏÑ†": "ÏµúÏö∞ÏÑ†Î≥ÄÏ†úÍ∂å",
    "Í≤ΩÎß§": "Í≤ΩÎß§Ï†àÏ∞®", "Í≥µÎß§": "Í≥µÎß§Ï†àÏ∞®",
    "Îì±Í∏∞": "Îì±Í∏∞Î∂ÄÎì±Î≥∏", "Îì±Î≥∏": "Îì±Í∏∞Î∂ÄÎì±Î≥∏",
    "Í∑ºÏ†ÄÎãπ": "Í∑ºÏ†ÄÎãπÍ∂å", "Í∞ÄÏïïÎ•ò": "Í∞ÄÏïïÎ•ò", "Í∞ÄÏ≤òÎ∂Ñ": "Í∞ÄÏ≤òÎ∂Ñ",
    "Íπ°ÌÜµÏ†ÑÏÑ∏": "Ï†ÑÏÑ∏ÌîºÌï¥", "ÏÇ¨Í∏∞": "Ï†ÑÏÑ∏ÏÇ¨Í∏∞", "Í≤ΩÎß§ÎÑòÏñ¥Í∞ê": "Í∂åÎ¶¨Î¶¨Ïä§ÌÅ¨",

    # 6. Î∂ÑÏüÅ Ìï¥Í≤∞
    "ÎÇ¥Ïö©Ï¶ùÎ™Ö": "ÎÇ¥Ïö©Ï¶ùÎ™Ö", "ÏÜåÏÜ°": "ÏÜåÏÜ°", "ÎØºÏÇ¨": "ÎØºÏÇ¨ÏÜåÏÜ°",
    "Ï°∞Ï†ïÏúÑ": "Ï£ºÌÉùÏûÑÎåÄÏ∞®Î∂ÑÏüÅÏ°∞Ï†ïÏúÑÏõêÌöå", "ÏÜåÏÜ°ÎßêÍ≥†": "Î∂ÑÏüÅÏ°∞Ï†ï",
    "Î≤ïÏõêÍ∞ÄÍ∏∞Ïã´Ïùå": "Î∂ÑÏüÅÏ°∞Ï†ï",
    "ÏßëÏ£ºÏù∏ÏÇ¨Îßù": "ÏûÑÏ∞®Í∂åÏäπÍ≥Ñ", "ÏûêÏãùÏÉÅÏÜç": "ÏûÑÏ∞®Í∂åÏäπÍ≥Ñ",
    "ÌäπÏïΩ": "ÌäπÏïΩÏÇ¨Ìï≠", "Î∂àÍ≥µÏ†ï": "Í∞ïÌñâÍ∑úÏ†ïÏúÑÎ∞ò", "ÎèÖÏÜåÏ°∞Ìï≠": "Î∂àÎ¶¨ÌïúÏïΩÏ†ï",
    "Ìö®Î†•ÏûàÎÇò": "Î¨¥Ìö®Ïó¨Î∂Ä",
}


# --------------------------------------------------------------------------------------
# Prompts
# --------------------------------------------------------------------------------------
NORMALIZATION_PROMPT: str = """
ÎãπÏã†ÏùÄ Î≤ïÎ•† AI Ï±óÎ¥áÏùò Ï†ÑÏ≤òÎ¶¨ Îã¥ÎãπÏûêÏûÖÎãàÎã§.
ÏïÑÎûò [Ïö©Ïñ¥ ÏÇ¨Ï†Ñ]ÏùÑ ÏóÑÍ≤©Ìûà Ï§ÄÏàòÌïòÏó¨ ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏùÑ 'Î≤ïÎ•† ÌëúÏ§ÄÏñ¥'Î°ú Î≥ÄÌôòÌï¥ Ï£ºÏÑ∏Ïöî.

[ÏàòÌñâ ÏßÄÏπ®]
1. ÏÇ¨Ï†ÑÏóê ÏûàÎäî Îã®Ïñ¥Îäî Î∞òÎìúÏãú Îß§ÌïëÎêú Î≤ïÎ•† Ïö©Ïñ¥Î°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî.
2. Î≥ÄÍ≤Ω Ï†ÑÏùò Í∏∞Ï°¥ Îã®Ïñ¥ Îí§Ïóê Î≥ÄÍ≤ΩÎêú Îã®Ïñ¥Î•º Í¥ÑÌò∏Î°ú ÎçßÎ∂ôÏó¨, ÏµúÏ¢Ö ÌÖçÏä§Ìä∏Îßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî. ex. "ÏßëÏ£ºÏù∏(ÏûÑÎåÄÏù∏)Ïù¥..."
3. ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ ÏùòÎèÑÎ•º ÏôúÍ≥°ÌïòÍ±∞ÎÇò Ï∂îÍ∞ÄÏ†ÅÏù∏ ÎãµÎ≥Ä, Î≥ÑÎèÑÏùò ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÏßÄ ÎßàÏÑ∏Ïöî. 

[Ïö©Ïñ¥ ÏÇ¨Ï†Ñ]
{dictionary}

ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏: {question}
Î≥ÄÍ≤ΩÎêú ÏßàÎ¨∏:
"""

SYSTEM_PROMPT: str = """
ÎãπÏã†ÏùÄ ÎåÄÌïúÎØºÍµ≠ 'Ï£ºÌÉù ÏûÑÎåÄÏ∞®(Ï†ÑÏõîÏÑ∏)¬∑Ï†ÑÏÑ∏ÏÇ¨Í∏∞ ÏòàÎ∞©' Î∂ÑÏïºÏùò Î≤ïÎ•†Ï†ïÎ≥¥ ÏÉÅÎã¥ AIÏûÖÎãàÎã§.
Î™©ÌëúÎäî ÏûÑÏ∞®Ïù∏(ÏÑ∏ÏûÖÏûê)Ïùò Í∂åÎ¶¨ Î≥¥Ìò∏ÏôÄ ÌîºÌï¥ ÏòàÎ∞©ÏùÑ ÏµúÏö∞ÏÑ†ÏúºÎ°ú, ÏÇ¨Ïö©ÏûêÍ∞Ä Î∞îÎ°ú Ïã§ÌñâÌï† Ïàò ÏûàÎäî ÏïàÏ†ÑÌïú Îã§Ïùå ÌñâÎèôÏùÑ Ï†úÏãúÌïòÎäî Í≤ÉÏûÖÎãàÎã§.

[ÎåÄÌôî ÌÜ§/ÌÉúÎèÑ]
- Îî±Îî±Ìïú Î¨∏Ïû•Î≥¥Îã§, ÏßßÍ≥† Î™ÖÌôïÌïú Î¨∏Ïû•ÏúºÎ°ú ÏπúÏ†àÌïòÍ≤å ÏÑ§Î™ÖÌï©ÎãàÎã§.
- ÏÇ¨Ïö©ÏûêÏùò Î∂àÏïà/Í∏¥Í∏âÏÑ±ÏùÑ Í≥†Î†§Ìï¥ 'ÏßÄÍ∏à ÎãπÏû• Ìï† Ïùº'ÏùÑ Î®ºÏ†Ä Ï†úÏãúÌï©ÎãàÎã§.
- Îã®Ï†ïÏù¥ Ïñ¥Î†§Ïö∞Î©¥ "Í∞ÄÎä•ÏÑ±Ïù¥ ÌÅ¨Îã§/Ï∂îÍ∞Ä ÌôïÏù∏Ïù¥ ÌïÑÏöî"Ï≤òÎüº Ï°∞Í±¥Î∂ÄÎ°ú ÎßêÌïòÍ≥†, ÌôïÏù∏ ÏßàÎ¨∏ 2~4Í∞úÎ•º Ï†úÏïàÌï©ÎãàÎã§.

[Ï∂úÏ≤ò¬∑Ï°∞Ìï≠ ÌëúÍ∏∞ Í∑úÏπô(Îß§Ïö∞ Ï§ëÏöî)]
- ÏïÑÎûò [Ï∞∏Í≥† Î¨∏ÏÑú]Îäî Î≤ïÏ†Å ÏúÑÍ≥Ñ(Î≤ïÎ†π > ÌïòÏúÑÍ∑úÏ†ï > ÌåêÎ°Ä) Î∞è Ï§ëÏöîÎèÑ(priority) Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ†¨ÎêòÏñ¥ Ï†úÍ≥µÎê©ÎãàÎã§.
- Í∞Å Ìï≠Î™©ÏùÄ "{src_title} {article} - {text}" ÌòïÏãùÏûÖÎãàÎã§.
- src_title(Ï∂úÏ≤ò Î≤ïÎ†π/Í∑úÏ†ï/ÌåêÎ°ÄÎ™Ö)Í≥º article(Ï°∞Î¨∏/ÏÇ¨Í±¥Î≤àÌò∏ Îì±)ÏùÑ ÌòºÎèôÌïòÏßÄ ÎßêÍ≥†, Ïù∏Ïö© Ïãú "src_title article"ÏùÑ Í∑∏ÎåÄÎ°ú Ìï®Íªò ÎßêÌïòÏÑ∏Ïöî.
- Ï∞∏Í≥† Î¨∏ÏÑúÏóê ÏóÜÎäî Î≤ïÎ†πÎ™Ö¬∑Ï°∞Î¨∏ Î≤àÌò∏Î•º ÎßåÎì§Ïñ¥ÎÇ¥Í±∞ÎÇò, Îã§Î•∏ Î≤ïÎ†πÎ™ÖÏúºÎ°ú Î∞îÍøî ÏûòÎ™ªÎêú Î≤ïÎ†πÎ™Ö+Ï°∞Î¨∏ÏùÑ Í≤∞Ìï©ÌïòÏßÄ ÎßàÏÑ∏Ïöî.
- Í∑ºÍ±∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎßå Ï°∞Î¨∏ Î≤àÌò∏Î•º Ïñ∏Í∏âÌïòÍ≥†, Í∑ºÍ±∞ Î¨∏ÏÑúÍ∞Ä Î∂àÏ∂©Î∂ÑÌïòÎ©¥ "Ï†úÍ≥µÎêú ÏûêÎ£å Î≤îÏúÑÏóêÏÑú ÌôïÏù∏ÎêòÎäî ÎÇ¥Ïö©"Ïù¥ÎùºÍ≥† Ï†úÌïúÌï©ÎãàÎã§.

[ÎãµÎ≥Ä ÏÉùÏÑ± ÏõêÏπô]
1) Î≤ïÏ†Å ÏúÑÍ≥Ñ Ï§ÄÏàò
- Î∞òÎìúÏãú [SECTION 1: ÌïµÏã¨ Î≤ïÎ†π]ÏùÑ ÏµúÏö∞ÏÑ† ÌåêÎã® Í∏∞Ï§ÄÏúºÎ°ú ÏÇºÏäµÎãàÎã§(Í∞ïÌñâÍ∑úÏ†ï ÏúÑÎ∞ò Í∞ÄÎä•ÏÑ± Ìè¨Ìï®).
- [SECTION 1]Ïù¥ Î™®Ìò∏Ìï† ÎïåÎßå [SECTION 2: Í¥ÄÎ†® Í∑úÏ†ï]Í≥º [SECTION 3: ÌåêÎ°Ä/ÏÇ¨Î°Ä]Î•º Î≥¥Ï∂© Í∑ºÍ±∞Î°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§.
- [SECTION 3]Í∞Ä [SECTION 1]Í≥º Îã¨Î¶¨ ÌåêÎã®Ìïú ÏòàÏô∏Ï†Å ÏÇ¨ÏïàÏù¥Î©¥, "ÏõêÏπô(Î≤ïÎ†π) / ÏòàÏô∏(ÌåêÎ°Ä)"Î°ú Íµ¨Î∂ÑÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§.
- Î≤ïÎ†π¬∑ÌåêÎ°ÄÍ∞Ä Ï∂©ÎèåÌïòÍ±∞ÎÇò Í∞úÏ†ï Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏúºÎ©¥, "ÏµúÏã† Î≤ïÎ†π/ÌåêÎ°Ä ÌôïÏù∏ ÌïÑÏöî"Î•º Î™ÖÏãúÌï©ÎãàÎã§.

2) ÎãµÎ≥Ä Íµ¨Ï°∞(Í∂åÎ¶¨Î≥¥Ìò∏/ÌîºÌï¥ÏòàÎ∞© Ï§ëÏã¨)
A. Ìïú Ï§Ñ Í≤∞Î°†(ÎëêÍ¥ÑÏãù)
B. ÏßÄÍ∏à ÎãπÏû• Ìï† Ïùº(Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏ 3~7Í∞ú)
   - Ïòà: Îì±Í∏∞Î∂Ä¬∑Í∂åÎ¶¨Í¥ÄÍ≥Ñ ÌôïÏù∏, Ï†ÑÏûÖÏã†Í≥†/ÌôïÏ†ïÏùºÏûê/Ï†êÏú†, ÎÇ¥Ïö©Ï¶ùÎ™Ö, ÏûÑÏ∞®Í∂åÎì±Í∏∞Î™ÖÎ†π, Î≥¥Ï¶ùÍ∏∞Í¥Ä/Î∂ÑÏüÅÏ°∞Ï†ï Î¨∏Ïùò Îì±
C. Î≤ïÏ†Å Í∑ºÍ±∞(ÌïµÏã¨ Ï°∞Î¨∏¬∑Í∑úÏ†ï 1~3Í∞úÎ•º [SRC] Í∏∞Ï§ÄÏúºÎ°ú)
D. Ï†àÏ∞®/Ï¶ùÎπô(Í∏∞Í¥Ä¬∑ÏÑúÎ•ò¬∑Í∏∞Ìïú¬∑Ï£ºÏùòÏ†ê)
E. Ïú†ÏÇ¨ ÌåêÎ°Ä/ÏÇ¨Î°Ä(ÏûàÎã§Î©¥ 1~2Í∞ú, Ï†ÅÏö© ÌïúÍ≥Ñ Ìè¨Ìï®)
F. Ï∂îÍ∞Ä ÌôïÏù∏ ÏßàÎ¨∏(ÏÇ¨Ïã§Í¥ÄÍ≥Ñ 2~4Í∞ú)

3) ÏïàÏ†ÑÏû•Ïπò/Î©¥Ï±Ö
- ÎãµÎ≥ÄÏùÄ ÏùºÎ∞òÏ†ÅÏù∏ Î≤ïÎ•†Ï†ïÎ≥¥Ïù¥Î©∞, Íµ¨Ï≤¥ ÏÇ¨Í±¥Ïùò Î≤ïÎ•† ÏûêÎ¨∏Ïù¥ ÏïÑÎãòÏùÑ ÎßàÏßÄÎßâÏóê ÏßßÍ≤å Í≥†ÏßÄÌï©ÎãàÎã§.
- ÏÇ¨Ïö©ÏûêÏóêÍ≤å Î∂àÎ¶¨Ìïú Í≤∞Í≥ºÍ∞Ä Í∞ÄÎä•Ìïú Í≤ΩÏö∞ÏóêÎèÑ, **ÏÇ¨Ïã§ÌôïÏù∏¬∑Ï¶ùÍ±∞Î≥¥Ï†Ñ¬∑Í∏∞Ìïú Ï§ÄÏàò**Î•º Ïö∞ÏÑ† ÏïàÎÇ¥Ìï©ÎãàÎã§.

[Ï∞∏Í≥† Î¨∏ÏÑú]
{context}

"""

# --------------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------------
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)  # type: ignore[arg-type]
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "‚Ä¶"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Ï§ëÎ≥µ Ï†úÍ±∞ (chunk_id/id Ïö∞ÏÑ†)."""
    seen: set[str] = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key: Optional[str] = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# --------------------------------------------------------------------------------------
# Tokenizers (for BM25)
# --------------------------------------------------------------------------------------
class Tokenizer(ABC):
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        raise NotImplementedError


class SimpleTokenizer(Tokenizer):
    """Regex Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä (kiwi ÎØ∏ÏÑ§Ïπò Ïãú fallback)."""

    def __init__(self, min_length: int = 1):
        self.min_length = min_length
        self._pattern = re.compile(r"[Í∞Ä-Ìû£a-zA-Z0-9]+")

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = self._pattern.findall(text.lower())
        return [t for t in tokens if len(t) >= self.min_length]


class KiwiTokenizer(Tokenizer):
    """Kiwi ÌòïÌÉúÏÜå Î∂ÑÏÑù Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä."""

    def __init__(self, pos_tags: Optional[Tuple[str, ...]] = None, min_length: int = 1):
        if not KIWI_AVAILABLE:
            raise ImportError("kiwipiepyÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: pip install kiwipiepy")
        self._kiwi = Kiwi()  # type: ignore[call-arg]
        self.pos_tags = pos_tags or ("NNG", "NNP", "VV", "VA", "SL", "SH")
        self.min_length = min_length

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens: List[str] = []
        for t in self._kiwi.tokenize(text):  # type: ignore[union-attr]
            if t.tag in self.pos_tags and len(t.form) >= self.min_length:
                tokens.append(t.form.lower())
        return tokens


# --------------------------------------------------------------------------------------
# BM25 (candidate-level) scoring
# --------------------------------------------------------------------------------------
def _bm25_lite_scores(
    query_tokens: List[str],
    docs_tokens: List[List[str]],
    *,
    k1: float = 1.5,
    b: float = 0.75,
) -> List[float]:
    """BM25Okapi-lite. Candidate-level only (N is small, e.g., 10~80)."""
    N = len(docs_tokens)
    if N == 0:
        return []
    if not query_tokens:
        return [0.0] * N

    doc_lens = [len(toks) for toks in docs_tokens]
    avgdl = (sum(doc_lens) / N) if N else 1.0
    avgdl = max(avgdl, 1e-9)

    df: Dict[str, int] = defaultdict(int)
    for toks in docs_tokens:
        for term in set(toks):
            df[term] += 1

    idf: Dict[str, float] = {}
    for term, dfi in df.items():
        idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

    qtf = Counter(query_tokens)
    scores: List[float] = []
    for toks, dl in zip(docs_tokens, doc_lens):
        tf = Counter(toks)
        score = 0.0
        norm = (1.0 - b) + b * (dl / avgdl)
        for term, qf in qtf.items():
            f = tf.get(term, 0)
            if f <= 0:
                continue
            denom = f + k1 * norm
            if denom <= 0:
                continue
            score += (idf.get(term, 0.0) * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))
        scores.append(float(score))
    return scores


def _compute_bm25_scores(
    query: str,
    docs: List[Document],
    *,
    tokenizer: Tokenizer,
    algorithm: str,
    k1: float,
    b: float,
    max_doc_chars: int,
) -> List[float]:
    """Returns BM25 scores aligned with docs (higher is better)."""
    if not docs:
        return []

    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(d.page_content or "", max_doc_chars)) for d in docs]

    if BM25_AVAILABLE:
        BM25Class = BM25Plus if algorithm == "plus" else BM25Okapi
        if BM25Class is None:
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)
        try:
            bm25 = BM25Class(docs_tokens, k1=k1, b=b)  # type: ignore[misc]
            scores = bm25.get_scores(query_tokens)
            return [float(x) for x in list(scores)]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è rank_bm25 Ïã§Ìå® ‚Üí lite BM25Î°ú Ìè¥Î∞±: {e}")
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)


def _compute_bm25_scores_from_texts(
    query: str,
    texts: List[str],
    *,
    tokenizer: Tokenizer,
    algorithm: str,
    k1: float,
    b: float,
    max_doc_chars: int,
) -> List[float]:
    """Returns BM25 scores aligned with texts (higher is better)."""
    if not texts:
        return []
    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(t or "", max_doc_chars)) for t in texts]

    if BM25_AVAILABLE:
        BM25Class = BM25Plus if algorithm == "plus" else BM25Okapi
        if BM25Class is None:
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)
        try:
            bm25 = BM25Class(docs_tokens, k1=k1, b=b)  # type: ignore[misc]
            scores = bm25.get_scores(query_tokens)
            return [float(x) for x in list(scores)]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è rank_bm25 Ïã§Ìå® ‚Üí lite BM25Î°ú Ìè¥Î∞±: {e}")
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)

    return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)

    return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)



# --------------------------------------------------------------------------------------
# Global BM25 index (optional, true sparse retrieval)
# --------------------------------------------------------------------------------------
class BM25InvertedIndex:
    """Lightweight BM25 inverted index for *global* sparse retrieval.

    - Build once with a corpus (per source: law/rule/case) using build().
    - Search returns top-k Documents with BM25 scores.
    - Uses the same doc identity logic as _dedupe_docs (metadata key_fields first, fallback to content hash).

    Notes:
      * This is NOT required for the default candidate-level BM25 in _dense_sparse_fuse().
      * For large corpora, memory usage grows with the number of unique terms.
    """

    def __init__(
        self,
        *,
        tokenizer: Tokenizer,
        key_fields: Sequence[str] = ("chunk_id", "id"),
        k1: float = 1.5,
        b: float = 0.75,
        max_doc_chars: int = 4000,
    ) -> None:
        self.tokenizer = tokenizer
        self.key_fields = tuple(key_fields)
        self.k1 = float(k1)
        self.b = float(b)
        self.max_doc_chars = int(max_doc_chars)

        self._docs: List[Document] = []
        self._doc_lens: List[int] = []
        self._avgdl: float = 0.0

        # postings[term] = list of (doc_idx, tf)
        self._postings: Dict[str, List[Tuple[int, int]]] = defaultdict(list)
        self._idf: Dict[str, float] = {}
        self._built: bool = False

    def build(self, docs: Sequence[Document]) -> None:
        deduped = _dedupe_docs(docs, self.key_fields)
        self._docs = list(deduped)
        self._postings.clear()
        self._idf.clear()
        self._doc_lens = []

        df: Dict[str, int] = defaultdict(int)

        for idx, d in enumerate(self._docs):
            text = _truncate(d.page_content or "", self.max_doc_chars)
            toks = self.tokenizer.tokenize(text)
            dl = len(toks)
            self._doc_lens.append(dl)

            tf = Counter(toks)
            for term, f in tf.items():
                if not term:
                    continue
                self._postings[term].append((idx, int(f)))
            for term in tf.keys():
                df[term] += 1

        N = len(self._docs)
        self._avgdl = (sum(self._doc_lens) / N) if N else 0.0

        for term, dfi in df.items():
            self._idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

        self._built = True

    def is_built(self) -> bool:
        return self._built and bool(self._docs)

    def search(self, query: str, *, top_k: int = 20) -> List[Tuple[Document, float]]:
        if not self.is_built():
            return []
        q_tokens = self.tokenizer.tokenize(query)
        if not q_tokens:
            return []

        qtf = Counter(q_tokens)
        scores: Dict[int, float] = defaultdict(float)

        avgdl = self._avgdl or 1.0
        k1 = self.k1
        b = self.b

        for term, qf in qtf.items():
            postings = self._postings.get(term)
            if not postings:
                continue
            idf = self._idf.get(term, 0.0)
            if idf == 0.0:
                continue
            for doc_idx, f in postings:
                dl = self._doc_lens[doc_idx] or 0
                norm = (1.0 - b) + b * (dl / avgdl)
                denom = f + k1 * norm
                if denom <= 0:
                    continue
                scores[doc_idx] += (idf * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))

        if not scores:
            return []

        top = heapq.nlargest(int(top_k), scores.items(), key=lambda x: x[1])
        return [(self._docs[i], float(s)) for (i, s) in top]

# --------------------------------------------------------------------------------------
# Hybrid fusion (rank-based default)
# --------------------------------------------------------------------------------------
def _rank_fusion(
    dense_ranks: List[int],
    sparse_ranks: List[int],
    *,
    mode: str = "rrf",          # "rrf" | "rank_sum" | "weighted"
    w_dense: float = 0.6,
    w_sparse: float = 0.4,
    rrf_k: int = 60,
) -> List[float]:
    """Return fused scores aligned with docs (higher is better)."""
    n = len(dense_ranks)
    if n == 0:
        return []

    mode = (mode or "rrf").lower()
    if mode == "rrf":
        k = max(1, int(rrf_k))
        return [(w_dense / (k + dense_ranks[i])) + (w_sparse / (k + sparse_ranks[i])) for i in range(n)]

    if mode == "rank_sum":
        if n == 1:
            return [w_dense + w_sparse]

        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)

        return [(w_dense * to_unit(dense_ranks[i])) + (w_sparse * to_unit(sparse_ranks[i])) for i in range(n)]

    # mode == "weighted": min-max normalize (dense=1/rank, sparse=1/rank) then weighted sum
    dense_scores = [1.0 / max(1, r) for r in dense_ranks]
    sparse_scores = [1.0 / max(1, r) for r in sparse_ranks]

    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    d = minmax(dense_scores)
    s = minmax(sparse_scores)
    return [(w_dense * d[i]) + (w_sparse * s[i]) for i in range(n)]



def _rank_fusion_multi(
    ranks_list: List[List[int]],
    *,
    mode: str = "rrf",          # "rrf" | "rank_sum" | "weighted"
    weights: Optional[List[float]] = None,
    rrf_k: int = 60,
) -> List[float]:
    """Return fused scores aligned with items (higher is better) for 2+ channels."""
    if not ranks_list:
        return []
    n = len(ranks_list[0])
    if n == 0:
        return []
    for rs in ranks_list:
        if len(rs) != n:
            raise ValueError("All rank lists must have the same length.")

    if weights is None:
        weights = [1.0] * len(ranks_list)
    if len(weights) != len(ranks_list):
        raise ValueError("weights length must match ranks_list length")

    mode = (mode or "rrf").lower()

    if mode == "rrf":
        k = max(1, int(rrf_k))
        out: List[float] = []
        for i in range(n):
            s = 0.0
            for w, rs in zip(weights, ranks_list):
                if w == 0:
                    continue
                s += float(w) / (k + int(rs[i]))
            out.append(s)
        return out

    if mode == "rank_sum":
        if n == 1:
            return [float(sum(weights))]
        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)
        out: List[float] = []
        for i in range(n):
            s = 0.0
            for w, rs in zip(weights, ranks_list):
                if w == 0:
                    continue
                s += float(w) * to_unit(int(rs[i]))
            out.append(s)
        return out

    # mode == "weighted": min-max normalize (1/rank) per channel then weighted sum
    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    channel_scores: List[List[float]] = []
    for rs in ranks_list:
        channel_scores.append(minmax([1.0 / max(1, int(r)) for r in rs]))

    out: List[float] = []
    for i in range(n):
        s = 0.0
        for w, cs in zip(weights, channel_scores):
            if w == 0:
                continue
            s += float(w) * float(cs[i])
        out.append(s)
    return out

# --------------------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------------------
@dataclass
class RAGConfig:
    # -------- LLMs --------
    normalize_model: str = "solar-pro2"  # Upstage chat model
    generation_model: str = "gpt-4o-mini"  # OpenAI model
    temperature: float = 0.1
    normalize_temperature: float = 0.0

    # -------- Embeddings --------
    embedding_backend: str = "upstage"  # "upstage" | "auto" (auto keeps option for other backends if you inject)
    embedding_model: str = "solar-embedding-1-large-passage"

    # -------- Pinecone index names (override) --------
    # Set these if your Pinecone index names differ. You can also set env vars:
    # PINECONE_INDEX_LAW, PINECONE_INDEX_RULE, PINECONE_INDEX_CASE
    index_name_law: str = ""
    index_name_rule: str = ""
    index_name_case: str = ""

    # -------- Retrieval sizes --------
    k_law: int = 5
    k_rule: int = 5
    k_case: int = 3
    search_multiplier: int = 2

    # -------- Candidate-level BM25 --------
    enable_bm25: bool = True
    # ============ Sparse retrieval mode (true sparse BM25) ============
    # - candidate: BM25 reorders only dense candidates (fast, no corpus preload)
    # - global: BM25 searches a prebuilt BM25 index over the full corpus you provide via build_global_bm25()
    # - auto: use global if available, else candidate
    sparse_mode: str = "auto"  # "auto" | "candidate" | "global"
    sparse_k_law: Optional[int] = None
    sparse_k_rule: Optional[int] = None
    sparse_k_case: Optional[int] = None

    bm25_algorithm: str = "okapi"  # "okapi" | "plus"
    bm25_k1: float = 1.5
    bm25_b: float = 0.75
    bm25_use_kiwi: bool = True
    bm25_max_doc_chars: int = 4000


    # -------- BM25 title scoring (sparse on title + text) --------
    enable_bm25_title: bool = True
    bm25_title_field: str = "title"   # metadata field name
    bm25_title_max_chars: int = 512   # title is short; keep tokenization light

    # -------- 3-channel fusion (dense + bm25_text + bm25_title) --------
    # hybrid_sparse_weight is split into text/title weights by this ratio.
    # w_title = hybrid_sparse_weight * hybrid_sparse_title_ratio
    # w_text  = hybrid_sparse_weight * (1 - hybrid_sparse_title_ratio)
    hybrid_sparse_title_ratio: float = 0.35

    # -------- Context formatting --------
    context_doc_max_chars: int = 2500
    # -------- Fusion --------
    hybrid_fusion: str = "rrf"  # "rrf" | "rank_sum" | "weighted"
    hybrid_dense_weight: float = 0.6
    hybrid_sparse_weight: float = 0.4
    rrf_k: int = 60

    # -------- Rerank (optional) --------
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80
    rerank_doc_max_chars: int = 2000

    # -------- 2-stage case expansion --------
    case_candidate_k: int = 40
    case_expand_top_n: Optional[int] = None  # None => k_case
    case_context_top_k: int = 50

    # -------- Deduping --------
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")

    def __post_init__(self) -> None:
        if not (0 <= self.temperature <= 2):
            raise ValueError("temperatureÎäî 0~2 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
        if not (0 <= self.normalize_temperature <= 2):
            raise ValueError("normalize_temperatureÎäî 0~2 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
        if self.search_multiplier < 1:
            raise ValueError("search_multiplierÎäî 1 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
        if self.case_candidate_k < 1 or self.case_context_top_k < 1:
            raise ValueError("case_* Í∞íÏùÄ 1 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")

        if self.enable_bm25:
            if self.bm25_k1 <= 0:
                raise ValueError("bm25_k1ÏùÄ 0Î≥¥Îã§ Ïª§Ïïº Ìï©ÎãàÎã§.")
            if not (0 <= self.bm25_b <= 1):
                raise ValueError("bm25_bÎäî 0~1 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
            if self.bm25_algorithm not in ("okapi", "plus"):
                raise ValueError('bm25_algorithmÏùÄ "okapi" ÎòêÎäî "plus" Ïù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.')

        if not (0.0 <= float(self.hybrid_sparse_title_ratio) <= 1.0):
            raise ValueError("hybrid_sparse_title_ratioÎäî 0~1 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
        if int(self.context_doc_max_chars) < 200:
            raise ValueError("context_doc_max_charsÎäî ÎÑàÎ¨¥ ÏûëÏùÑ Ïàò ÏóÜÏäµÎãàÎã§ (>=200 Í∂åÏû•).")

        if self.hybrid_fusion not in ("rrf", "rank_sum", "weighted"):
            raise ValueError('hybrid_fusionÏùÄ "rrf" | "rank_sum" | "weighted" Ï§ë ÌïòÎÇòÏó¨Ïïº Ìï©ÎãàÎã§.')
        if self.rrf_k < 1:
            raise ValueError("rrf_kÎäî 1 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
        if self.hybrid_dense_weight < 0 or self.hybrid_sparse_weight < 0:
            raise ValueError("hybrid_*_weightÎäî 0 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
        if self.hybrid_dense_weight == 0 and self.hybrid_sparse_weight == 0:
            raise ValueError("hybrid_dense_weightÏôÄ hybrid_sparse_weightÍ∞Ä Î™®Îëê 0Ïùº ÏàòÎäî ÏóÜÏäµÎãàÎã§.")


# --------------------------------------------------------------------------------------
# Pipeline
# --------------------------------------------------------------------------------------
class RAGPipeline:
    """Unified Hybrid RAG pipeline (no web framework integration)."""

    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        upstage_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
        embedding: Optional[object] = None,
        normalize_llm: Optional[object] = None,
        generation_llm: Optional[object] = None,
        cohere_client: Optional[object] = None,
        tokenizer: Optional[Tokenizer] = None,
    ) -> None:
        self.config = config or RAGConfig()

        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._upstage_api_key = upstage_api_key or os.getenv("UPSTAGE_API_KEY")
        self._openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")

        if not self._pc_api_key:
            raise ValueError("PINECONE_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. pc_api_key Ïù∏Ïûê ÎòêÎäî ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.")

        # ---- Pinecone index names ----
        self._index_names = {
            "law": self.config.index_name_law or os.getenv("PINECONE_INDEX_LAW") or INDEX_NAMES["law"],
            "rule": self.config.index_name_rule or os.getenv("PINECONE_INDEX_RULE") or INDEX_NAMES["rule"],
            "case": self.config.index_name_case or os.getenv("PINECONE_INDEX_CASE") or INDEX_NAMES["case"],
        }

        # ---- Embedding (dense) ----
        if embedding is not None:
            self._embedding = embedding
        else:
            backend = (self.config.embedding_backend or "auto").lower()
            if backend in ("auto", "upstage"):
                if not UPSTAGE_AVAILABLE:
                    raise ImportError("langchain_upstageÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§: pip install langchain-upstage")
                if not self._upstage_api_key:
                    raise ValueError("UPSTAGE_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§ (UpstageEmbeddings).")
                os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
                self._embedding = UpstageEmbeddings(model=self.config.embedding_model)  # type: ignore[call-arg]
            else:
                raise ValueError(
                    "ÌòÑÏû¨ unified Î™®ÎìàÏùÄ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Upstage(SOLAR) embeddingÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. "
                    "Îã§Î•∏ embeddingÏùÑ Ïì∞Î†§Î©¥ embedding Í∞ùÏ≤¥Î•º ÏßÅÏ†ë Ï£ºÏûÖÌïòÏÑ∏Ïöî."
                )

        # ---- Pinecone vector stores ----
        logger.info("üîó Pinecone 3Ï§ë Ïù∏Îç±Ïä§ Ïó∞Í≤∞ Ï§ë...")
        self._law_store = PineconeVectorStore(
            index_name=self._index_names["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=self._index_names["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=self._index_names["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        logger.info("‚úÖ [Law / Rule / Case] 3Í∞ú Ïù∏Îç±Ïä§ Î°úÎìú ÏôÑÎ£å!")

        # ---- LLMs ----
        # normalize: Upstage solar-pro2
        if normalize_llm is not None:
            self._normalize_llm = normalize_llm
        else:
            if not UPSTAGE_AVAILABLE or ChatUpstage is None:
                raise ImportError("normalize_queryÏóê Upstage chatÏùÑ Ïì∞Î†§Î©¥ langchain_upstageÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            if not self._upstage_api_key:
                raise ValueError("normalize_queryÏóê UPSTAGE_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
            self._normalize_llm = ChatUpstage(
                model=self.config.normalize_model,
                temperature=self.config.normalize_temperature,
            )

        # generation: OpenAI gpt-4o-mini
        if generation_llm is not None:
            self._generation_llm = generation_llm
        else:
            if not OPENAI_AVAILABLE or ChatOpenAI is None:
                raise ImportError("generate_answerÏóê OpenAI chatÏùÑ Ïì∞Î†§Î©¥ langchain_openaiÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            if not self._openai_api_key:
                raise ValueError("generate_answerÏóê OPENAI_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            os.environ.setdefault("OPENAI_API_KEY", self._openai_api_key)
            self._generation_llm = ChatOpenAI(
                model=self.config.generation_model,
                temperature=self.config.temperature,
            )

        # ---- Tokenizer (for BM25) ----
        if tokenizer is not None:
            self._tokenizer = tokenizer
        else:
            if self.config.bm25_use_kiwi and KIWI_AVAILABLE:
                logger.info("‚úÖ Kiwi ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö© (BM25)")
                self._tokenizer = KiwiTokenizer()
            else:
                logger.info("‚ÑπÔ∏è SimpleTokenizer ÏÇ¨Ïö© (BM25)")
                self._tokenizer = SimpleTokenizer()

        # ---- Cohere rerank client (optional) ----
        self._cohere_client = None
        if self.config.enable_rerank:
            if not COHERE_AVAILABLE:
                logger.warning("‚ö†Ô∏è cohere Ìå®ÌÇ§ÏßÄÍ∞Ä ÏóÜÏñ¥ rerankÎ•º ÎπÑÌôúÏÑ±ÌôîÌï©ÎãàÎã§.")
            elif not self._cohere_api_key:
                logger.warning("‚ö†Ô∏è COHERE_API_KEYÍ∞Ä ÏóÜÏñ¥ rerankÎ•º ÎπÑÌôúÏÑ±ÌôîÌï©ÎãàÎã§.")
            else:
                self._cohere_client = cohere_client or cohere.Client(self._cohere_api_key)  # type: ignore[attr-defined]

        # ---- Global BM25 indices (optional, for true sparse retrieval) ----
        self._global_bm25: Dict[str, BM25InvertedIndex] = {}

    # ----------------------------
    # Stores
    # ----------------------------
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store

    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store

    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store

    # ----------------------------
    # Query normalization
    # ----------------------------
    def normalize_query(self, user_query: str) -> str:
        """Upstage SOLAR Pro2Î°ú ÏßàÎ¨∏ÏùÑ Î≤ïÎ•† Ïö©Ïñ¥Î°ú ÌëúÏ§ÄÌôî."""
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()

        try:
            normalized = chain.invoke({"dictionary": KEYWORD_DICT, "question": user_query})
            out = str(normalized).strip()
            return out or user_query
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå® (ÏõêÎ≥∏ ÏÇ¨Ïö©): {e}")
            return user_query

    # ----------------------------
    # Case expansion
    # ----------------------------
    def get_full_case_context(self, case_no: str) -> str:
        """ÌäπÏ†ï ÏÇ¨Í±¥Î≤àÌò∏(case_no)Ïùò ÌåêÎ°Ä Ï†ÑÎ¨∏(Ï≤≠ÌÅ¨Îì§ÏùÑ Ïó∞Í≤∞)ÏùÑ Í∞ÄÏ†∏Ïò¥."""
        try:
            results = self.case_store.similarity_search(
                query="ÌåêÎ°Ä Ï†ÑÎ¨∏ Í≤ÄÏÉâ",
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            sorted_docs = sorted(results, key=lambda x: str((x.metadata or {}).get("chunk_id", "")))
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌåêÎ°Ä Ï†ÑÎ¨∏ Î°úÎî© Ïã§Ìå® ({case_no}): {e}")
            return ""

    # ----------------------------
    # Internal helpers
    # ----------------------------
    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs

    def build_global_bm25(
        self,
        *,
        law_docs: Optional[Sequence[Document]] = None,
        rule_docs: Optional[Sequence[Document]] = None,
        case_docs: Optional[Sequence[Document]] = None,
    ) -> None:
        """(Optional) Build *global* BM25 indices for true sparse retrieval.

        Provide the same corpus you indexed into Pinecone (or a superset). Metadata should contain
        stable identifiers (e.g., chunk_id) to enable deduplication/merge with dense results.
        """
        cfg = self.config

        def _build(name: str, docs: Optional[Sequence[Document]]) -> None:
            if not docs:
                return
            idx = BM25InvertedIndex(
                tokenizer=self._tokenizer,
                key_fields=cfg.dedupe_key_fields,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_max_doc_chars,
            )
            idx.build(docs)
            if idx.is_built():
                self._global_bm25[name] = idx
                logger.info(f"‚úÖ Global BM25 index built for '{name}' (docs={len(docs)})")

        _build("law", law_docs)
        _build("rule", rule_docs)
        _build("case", case_docs)

    def _hybrid_fuse_per_source(self, source: str, query: str, dense_docs: List[Document]) -> List[Document]:
        """Choose hybrid strategy per source (candidate-level vs global BM25)."""
        cfg = self.config
        mode = (cfg.sparse_mode or "auto").lower()

        use_global = False
        if mode == "global":
            use_global = True
        elif mode == "auto":
            use_global = source in self._global_bm25 and self._global_bm25[source].is_built()

        if not use_global:
            return self._dense_sparse_fuse(query, dense_docs)

        if source == "law":
            sk = cfg.sparse_k_law or (cfg.k_law * max(1, cfg.search_multiplier))
        elif source == "rule":
            sk = cfg.sparse_k_rule or (cfg.k_rule * max(1, cfg.search_multiplier))
        else:
            sk = cfg.sparse_k_case or max(cfg.case_candidate_k, cfg.k_case * max(1, cfg.search_multiplier))

        sparse_pairs = self._global_bm25[source].search(query, top_k=int(sk))
        sparse_docs: List[Document] = []
        for rank, (d, score) in enumerate(sparse_pairs, start=1):
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__bm25_score"] = float(score)
            d.metadata["__bm25_rank"] = int(rank)
            sparse_docs.append(d)

        dense_docs = self._attach_source(dense_docs, source)
        sparse_docs = self._attach_source(sparse_docs, source)

        merged = _dedupe_docs(list(dense_docs) + list(sparse_docs), cfg.dedupe_key_fields)
        if len(merged) <= 1:
            return merged

        def _key(d: Document) -> str:
            md = d.metadata or {}
            for f in cfg.dedupe_key_fields:
                v = md.get(f)
                if v:
                    return f"{f}:{v}"
            return f"content:{hash(d.page_content)}"

        dense_rank_map: Dict[str, int] = {}
        sparse_rank_map: Dict[str, int] = {}

        for i, d in enumerate(dense_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__dense_rank", i))
            dense_rank_map[k] = min(dense_rank_map.get(k, r), r)

        for i, d in enumerate(sparse_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__bm25_rank", i))
            sparse_rank_map[k] = min(sparse_rank_map.get(k, r), r)

        max_dense = max(dense_rank_map.values()) if dense_rank_map else 1000
        max_sparse = max(sparse_rank_map.values()) if sparse_rank_map else 1000
        fill_dense = max_dense + 1000
        fill_sparse = max_sparse + 1000

        dense_ranks: List[int] = []
        sparse_ranks: List[int] = []
        for d in merged:
            k = _key(d)
            dense_ranks.append(int(dense_rank_map.get(k, fill_dense)))
            sparse_ranks.append(int(sparse_rank_map.get(k, fill_sparse)))

                # --- add title BM25 ranks over merged (candidate-level on metadata title) ---
        bm25_title_scores: List[float] = [0.0] * len(merged)
        bm25_title_ranks: List[int] = [len(merged) + 1000] * len(merged)
        if cfg.enable_bm25_title:
            titles = [str((d.metadata or {}).get(cfg.bm25_title_field, "") or "") for d in merged]
            bm25_title_scores = _compute_bm25_scores_from_texts(
                query,
                titles,
                tokenizer=self._tokenizer,
                algorithm=cfg.bm25_algorithm,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_title_max_chars,
            )
            # tie-break with dense ranks (lower is better)
            order_title = sorted(range(len(merged)), key=lambda i: (-bm25_title_scores[i], dense_ranks[i]))
            bm25_title_ranks = [0] * len(merged)
            for r, idx in enumerate(order_title, start=1):
                bm25_title_ranks[idx] = r

        # store title score/rank
        for i, d in enumerate(merged):
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__bm25_title_score"] = float(bm25_title_scores[i])
            d.metadata["__bm25_title_rank"] = int(bm25_title_ranks[i])

        w_dense = float(cfg.hybrid_dense_weight)
        w_title = float(cfg.hybrid_sparse_weight) * float(cfg.hybrid_sparse_title_ratio) if cfg.enable_bm25_title else 0.0
        w_text = float(cfg.hybrid_sparse_weight) - w_title

        fused = _rank_fusion_multi(
            [dense_ranks, sparse_ranks, bm25_title_ranks],
            mode=cfg.hybrid_fusion,
            weights=[w_dense, w_text, w_title],
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(len(merged)), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, idx in enumerate(order, start=1):
            d = merged[idx]
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__hybrid_score"] = float(fused[idx])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _search_dense_candidates(self, store: PineconeVectorStore, query: str, k: int) -> List[Document]:
        """Dense retrieval via PineconeVectorStore."""
        try:
            pairs = store.similarity_search_with_score(query, k=k)  # type: ignore[attr-defined]
            docs: List[Document] = []
            for rank, (doc, score) in enumerate(pairs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_score"] = float(score)
                doc.metadata["__dense_rank"] = int(rank)
                docs.append(doc)
            return docs
        except Exception:
            docs = store.similarity_search(query, k=k)
            for rank, doc in enumerate(docs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_rank"] = int(rank)
            return docs

        def _dense_sparse_fuse(self, query: str, docs: List[Document]) -> List[Document]:
            """Dense candidatesÎ•º (BM25-text + BM25-title)Î°ú Ï†êÏàòÌôîÌïòÍ≥† 3Ï±ÑÎÑê RRFÎ°ú Í≤∞Ìï©."""
            cfg = self.config
            if not cfg.enable_bm25:
                return docs

            docs = _dedupe_docs(docs, cfg.dedupe_key_fields)
            n = len(docs)
            if n <= 1:
                return docs

            # --- dense ranks ---
            dense_ranks: List[int] = []
            for i, d in enumerate(docs, start=1):
                if d.metadata is None:
                    d.metadata = {}
                dense_ranks.append(int(d.metadata.get("__dense_rank", i)))

            # --- sparse: BM25 on text ---
            bm25_text_scores = _compute_bm25_scores(
                query,
                docs,
                tokenizer=self._tokenizer,
                algorithm=cfg.bm25_algorithm,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_max_doc_chars,
            )
            order_text = sorted(range(n), key=lambda i: (-bm25_text_scores[i], dense_ranks[i]))
            bm25_text_ranks = [0] * n
            for r, idx in enumerate(order_text, start=1):
                bm25_text_ranks[idx] = r

            # --- sparse: BM25 on title (metadata field) ---
            bm25_title_scores: List[float] = [0.0] * n
            bm25_title_ranks: List[int] = [n + 1000] * n
            if cfg.enable_bm25_title:
                titles = [str((d.metadata or {}).get(cfg.bm25_title_field, "") or "") for d in docs]
                bm25_title_scores = _compute_bm25_scores_from_texts(
                    query,
                    titles,
                    tokenizer=self._tokenizer,
                    algorithm=cfg.bm25_algorithm,
                    k1=cfg.bm25_k1,
                    b=cfg.bm25_b,
                    max_doc_chars=cfg.bm25_title_max_chars,
                )
                order_title = sorted(range(n), key=lambda i: (-bm25_title_scores[i], dense_ranks[i]))
                bm25_title_ranks = [0] * n
                for r, idx in enumerate(order_title, start=1):
                    bm25_title_ranks[idx] = r

            # --- attach metadata (keep legacy keys for compatibility) ---
            for i, d in enumerate(docs):
                d.metadata["__bm25_text_score"] = float(bm25_text_scores[i])
                d.metadata["__bm25_text_rank"] = int(bm25_text_ranks[i])
                # legacy
                d.metadata["__bm25_score"] = float(bm25_text_scores[i])
                d.metadata["__bm25_rank"] = int(bm25_text_ranks[i])

                d.metadata["__bm25_title_score"] = float(bm25_title_scores[i])
                d.metadata["__bm25_title_rank"] = int(bm25_title_ranks[i])

            # --- 3-channel fusion (dense + text + title) ---
            w_dense = float(cfg.hybrid_dense_weight)
            w_title = float(cfg.hybrid_sparse_weight) * float(cfg.hybrid_sparse_title_ratio) if cfg.enable_bm25_title else 0.0
            w_text = float(cfg.hybrid_sparse_weight) - w_title

            fused = _rank_fusion_multi(
                [dense_ranks, bm25_text_ranks, bm25_title_ranks],
                mode=cfg.hybrid_fusion,
                weights=[w_dense, w_text, w_title],
                rrf_k=cfg.rrf_k,
            )
            order = sorted(range(n), key=lambda i: fused[i], reverse=True)

            out: List[Document] = []
            for rank, i in enumerate(order, start=1):
                d = docs[i]
                d.metadata["__hybrid_score"] = float(fused[i])
                d.metadata["__hybrid_rank"] = int(rank)
                out.append(d)
            return out

    def _rerank(self, query: str, docs: List[Document]) -> Optional[List[Tuple[int, float]]]:
        """Cohere rerank Ïã§Ìñâ. Ïã§Ìå®/ÎπÑÌôúÏÑ± Ïãú None."""
        if not self._cohere_client:
            return None

        cfg = self.config
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]
        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            return [(r.index, float(r.relevance_score)) for r in rerank_results.results]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Rerank Ïã§Ìå® (skip): {e}")
            return None

    def _cap_for_rerank(self, law: List[Document], rule: List[Document], case: List[Document]) -> List[Document]:
        """rerank ÏûÖÎ†• Î¨∏ÏÑú Ïàò Ï†úÌïú: law/rule Ïö∞ÏÑ†, caseÎäî ÎÇ®Îäî Ïä¨Î°ØÎßå."""
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)

        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[: cfg.rerank_max_documents]
        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]

    # ----------------------------
    # Retrieval: triple index + hybrid + optional rerank + 2-stage case expansion
    # ----------------------------
    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        cfg = self.config
        mult = cfg.search_multiplier

        logger.info(f"üîç [Hybrid Retrieval] query='{query}'")

        docs_law = self._attach_source(
            self._search_dense_candidates(self.law_store, query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self._search_dense_candidates(self.rule_store, query, k=cfg.k_rule * mult),
            "rule",
        )
        docs_case_chunks = self._attach_source(
            self._search_dense_candidates(self.case_store, query, k=cfg.case_candidate_k),
            "case",
        )

        # candidate-level BM25 fusion per index
        docs_law = self._hybrid_fuse_per_source("law", query, docs_law)
        docs_rule = self._hybrid_fuse_per_source("rule", query, docs_rule)
        docs_case_chunks = self._hybrid_fuse_per_source("case", query, docs_case_chunks)

        combined_for_rerank = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)

        ranked = self._rerank(query, combined_for_rerank) if cfg.enable_rerank else None
        if ranked:
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]
            selected_docs = [combined_for_rerank[i] for (i, _s) in filtered]
            logger.info(f"üìå Rerank selected={len(selected_docs)} (threshold={cfg.rerank_threshold})")
        else:
            selected_docs = combined_for_rerank

        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)

        law_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "rule"]
        case_ranked_chunks = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "case"]

        final_law = law_ranked[: cfg.k_law]
        final_rule = rule_ranked[: cfg.k_rule]

        top_n = cfg.case_expand_top_n if cfg.case_expand_top_n is not None else cfg.k_case
        seen_case_no: set[str] = set()
        chosen_case_docs: List[Document] = []
        for d in case_ranked_chunks:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no or str(case_no) in seen_case_no:
                continue
            seen_case_no.add(str(case_no))
            chosen_case_docs.append(d)
            if len(chosen_case_docs) >= top_n:
                break

        expanded_cases: List[Document] = []
        for d in chosen_case_docs:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                expanded_cases.append(d)
                continue

            title = (d.metadata or {}).get("title") or (d.metadata or {}).get("case_name") or str(case_no)
            md = dict(d.metadata or {})
            md["__expanded"] = True
            expanded_cases.append(
                Document(
                    page_content=f"[ÌåêÎ°Ä Ï†ÑÎ¨∏: {title}]\n{full_text}",
                    metadata=md,
                )
            )

        final_case = expanded_cases[: cfg.k_case]

        final_docs = final_law + final_rule + final_case
        final_docs = sorted(final_docs, key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99))
        return final_docs

# ----------------------------
# Context / references formatting
# ----------------------------
@staticmethod
def format_reference_line(doc: Document, *, text_max_chars: int = 2500) -> str:
    """'{src_title} {article} - {text}' Ìïú Ï§Ñ ÏÉùÏÑ±."""
    md = doc.metadata or {}
    src_title = md.get("src_title") or md.get("__source_index") or "ÏûêÎ£å"

    article = (
        md.get("article")
        or md.get("case_no")
        or md.get("section")
        or md.get("Ï°∞Î¨∏")
        or ""
    )
    prefix = f"{src_title} {article}".strip()

    text = doc.page_content or md.get("text") or ""
    if text_max_chars and int(text_max_chars) > 0:
        text = _truncate(str(text), int(text_max_chars))
    return f"{prefix} - {text}".strip()

def format_context(self, docs: List[Document]) -> str:
    """LLMÏóê ÎÑ£ÏùÑ {context} ÌÖçÏä§Ìä∏. Ìï≠Î™©ÏùÄ '- {src_title} {article} - {text}'Î°ú ÎÇòÏó¥."""
    cfg = self.config
    ordered = sorted(
        docs,
        key=lambda d: (
            _safe_int((d.metadata or {}).get("priority", 99), 99),
            _safe_int((d.metadata or {}).get("__hybrid_rank", 999), 999),
        ),
    )
    lines = [self.format_reference_line(d, text_max_chars=cfg.context_doc_max_chars) for d in ordered]
    return "\n".join([f"- {ln}" for ln in lines]).strip()

def format_references(self, docs: List[Document]) -> List[str]:
    """UIÏö© Ï∞∏Í≥† Î¨∏ÏÑú ÎùºÏù∏ Î¶¨Ïä§Ìä∏ (contextÏôÄ ÎèôÏùº ÌòïÏãù, '- ' ÏóÜÏù¥)."""
    cfg = self.config
    ordered = sorted(
        docs,
        key=lambda d: (
            _safe_int((d.metadata or {}).get("priority", 99), 99),
            _safe_int((d.metadata or {}).get("__hybrid_rank", 999), 999),
        ),
    )
    return [self.format_reference_line(d, text_max_chars=cfg.context_doc_max_chars) for d in ordered]


# ----------------------------
# Answer generation
# ----------------------------
def _generate_from_context(self, *, context: str, question: str) -> str:
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            ("human", "{question}"),
        ]
    )
    chain = prompt | self._generation_llm | StrOutputParser()
    logger.info("ü§ñ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë...")
    try:
        return str(chain.invoke({"context": context, "question": question})).strip()
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return "Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."

def answer_with_trace(self, user_input: str, *, skip_normalization: bool = False) -> Dict[str, Any]:
    """Streamlit/UIÏö©: normalized_query, references, answerÎ•º Ìï®Íªò Î∞òÌôò."""
    normalized_query = user_input if skip_normalization else self.normalize_query(user_input)
    if not skip_normalization:
        logger.info(f"üîÑ ÌëúÏ§ÄÌôîÎêú ÏßàÎ¨∏: {normalized_query}")

    docs = self.triple_hybrid_retrieval(normalized_query)
    if not docs:
        return {
            "normalized_query": normalized_query,
            "references": [],
            "answer": "Ï£ÑÏÜ°Ìï©ÎãàÎã§. Í¥ÄÎ†® Î≤ïÎ†πÏù¥ÎÇò ÌåêÎ°ÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.",
            "docs": [],
        }

    context = self.format_context(docs)
    answer = self._generate_from_context(context=context, question=normalized_query)
    return {
        "normalized_query": normalized_query,
        "references": self.format_references(docs),
        "answer": answer,
        "docs": docs,
    }

def generate_answer(self, user_input: str, *, skip_normalization: bool = False) -> str:
    """Ìò∏ÌôòÏö©: Í∏∞Ï°¥Ï≤òÎüº ÎãµÎ≥Ä Î¨∏ÏûêÏó¥Îßå Î∞òÌôò."""
    return str(self.answer_with_trace(user_input, skip_normalization=skip_normalization).get("answer", "")).strip()



def create_pipeline(**kwargs: Any) -> RAGPipeline:
    """Convenience helper."""
    return RAGPipeline(**kwargs)


__all__ = [
    "RAGConfig",
    "RAGPipeline",
    "create_pipeline",
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
]
