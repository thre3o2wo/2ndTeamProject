{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf1f414-b66f-44a4-b68a-efa5d62ce8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI & Pinecone ì—°ê²° ì™„ë£Œ\n",
      "   - Index: new-realestate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# âœ… ë°˜ë“œì‹œ í•„ìš” (ì´ ì…€ì—ì„œ .env ë¡œë“œ)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# === í™˜ê²½ë³€ìˆ˜ ì½ê¸° ===\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = \"new-realestate\"\n",
    "\n",
    "# === í™˜ê²½ë³€ìˆ˜ ê²€ì¦ (ì¤‘ìš”!) ===\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"âŒ PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "if not PINECONE_INDEX_NAME:\n",
    "    raise ValueError(\"âŒ PINECONE_INDEX_NAMEê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (.env í™•ì¸).\")\n",
    "\n",
    "# === í´ë¼ì´ì–¸íŠ¸ ìƒì„± ===\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "# === ëª¨ë¸ ì„¤ì • ===\n",
    "EMBED_MODEL = \"text-embedding-3-large\"   # 3072-dim\n",
    "CHAT_MODEL = \"gpt-4.1-mini\"\n",
    "\n",
    "print(\"âœ… OpenAI & Pinecone ì—°ê²° ì™„ë£Œ\")\n",
    "print(\"   - Index:\", PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a78810-c3e1-4d0c-a70a-c6eef67e7b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7ac5d-7cd2-453c-bd99-f0afea050611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… connected to index: new-realestate\n",
      "ðŸ—‘ï¸ namespace ì „ì²´ ì‚­ì œ: __default__\n",
      "âš ï¸ namespace '__default__'ì˜ ëª¨ë“  vectorë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# =========================\n",
    "# CONFIG (ì—¬ê¸°ë§Œ ìˆ˜ì •)\n",
    "# =========================\n",
    "INDEX_NAME = \"new-realestate\"\n",
    "NAMESPACE  = \"__default__\"\n",
    "DRY_RUN    = False   # Trueë©´ ì‚­ì œ ì•ˆ í•¨\n",
    "\n",
    "# =========================\n",
    "def confirm(msg: str):\n",
    "    print(msg)\n",
    "    ans = input(\"ì •ë§ ì§„í–‰í• ê¹Œìš”? (yes/no): \").strip().lower()\n",
    "    if ans != \"yes\":\n",
    "        raise SystemExit(\"âŒ ì‚¬ìš©ìž ì·¨ì†Œ\")\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"âŒ PINECONE_API_KEY ì—†ìŒ\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "print(\"âœ… connected to index:\", INDEX_NAME)\n",
    "\n",
    "# =========================\n",
    "# ðŸ”¥ namespace ì „ì²´ ì‚­ì œ\n",
    "# =========================\n",
    "print(f\"ðŸ—‘ï¸ namespace ì „ì²´ ì‚­ì œ: {NAMESPACE}\")\n",
    "\n",
    "if DRY_RUN:\n",
    "    print(\"âš ï¸ DRY_RUN=True â†’ ì‹¤ì œ ì‚­ì œ ì•ˆ í•¨\")\n",
    "else:\n",
    "    confirm(f\"âš ï¸ namespace '{NAMESPACE}'ì˜ ëª¨ë“  vectorë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\")\n",
    "    index.delete(delete_all=True, namespace=NAMESPACE)\n",
    "    print(\"âœ… namespace ì „ì²´ ì‚­ì œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580daaf1-63c0-49f3-abe3-1ded4a520a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0243336-da17-45a8-9e67-64994cf119b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e58dd-9997-409a-b09b-e5d67ca84e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca1c6c9-2da4-433b-9d37-a523b7c95fac",
   "metadata": {},
   "source": [
    "== íŒŒì¼ ë³€í™˜ =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8a4439-0155-45bd-921b-ddce96feec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JSONL ìƒì„± ì™„ë£Œ: out/chunks.jsonl\n",
      "   units=705, chunks=776\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PDF / DOCX -> JSONL\n",
    "# (Jupyter single-cell version)\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Text utils\n",
    "# -------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\u00a0\", \" \").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:\n",
    "    text = clean_text(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    n = len(text)\n",
    "    start = 0\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) PDF loader (page-based)\n",
    "# -------------------------\n",
    "def load_pdf_units(path: str) -> List[Dict[str, Any]]:\n",
    "    reader = PdfReader(path)\n",
    "    source = os.path.basename(path)\n",
    "    units = []\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = clean_text(page.extract_text() or \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        page_no = i + 1\n",
    "        units.append({\n",
    "            \"source\": source,\n",
    "            \"unit_type\": \"page\",\n",
    "            \"unit_key\": f\"p{page_no}\",\n",
    "            \"text\": text,\n",
    "            \"meta\": {\"page\": page_no},\n",
    "        })\n",
    "\n",
    "    return units\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) DOCX loader (article-based)\n",
    "# -------------------------\n",
    "ARTICLE_RE = re.compile(r\"(ì œ\\s*\\d+\\s*ì¡°(?:ì˜\\s*\\d+)?)\")\n",
    "\n",
    "def split_korean_law_articles(full_text: str) -> List[Dict[str, Any]]:\n",
    "    text = clean_text(full_text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    matches = list(ARTICLE_RE.finditer(text))\n",
    "    if not matches:\n",
    "        return [{\"article\": None, \"article_no\": None, \"text\": text}]\n",
    "\n",
    "    blocks = []\n",
    "    for i, m in enumerate(matches):\n",
    "        start = m.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        article = re.sub(r\"\\s+\", \"\", m.group(1))\n",
    "        nums = re.findall(r\"\\d+\", article)\n",
    "        article_no = int(nums[0]) if nums else None\n",
    "\n",
    "        blocks.append({\n",
    "            \"article\": article,\n",
    "            \"article_no\": article_no,\n",
    "            \"text\": text[start:end].strip()\n",
    "        })\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def load_docx_units(path: str) -> List[Dict[str, Any]]:\n",
    "    doc = Document(path)\n",
    "    paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]\n",
    "    full_text = clean_text(\"\\n\".join(paras))\n",
    "    if not full_text:\n",
    "        return []\n",
    "\n",
    "    source = os.path.basename(path)\n",
    "    units = []\n",
    "\n",
    "    for i, blk in enumerate(split_korean_law_articles(full_text)):\n",
    "        unit_key = f\"a{blk['article_no']}\" if blk[\"article_no\"] else f\"blk{i}\"\n",
    "        units.append({\n",
    "            \"source\": source,\n",
    "            \"unit_type\": \"article\",\n",
    "            \"unit_key\": unit_key,\n",
    "            \"text\": blk[\"text\"],\n",
    "            \"meta\": {\n",
    "                \"article\": blk[\"article\"],\n",
    "                \"article_no\": blk[\"article_no\"],\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return units\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) ID / version helpers\n",
    "# -------------------------\n",
    "def guess_doc_version(filename: str) -> str:\n",
    "    m = re.search(r\"\\((\\d{8})\\)\", filename)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    m = re.search(r\"[_-](\\d{6})\", filename)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    m = re.search(r\"(20\\d{2})\", filename)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def make_doc_id(source: str) -> str:\n",
    "    return hashlib.sha1(source.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "\n",
    "def make_chunk_id(doc_id: str, doc_version: str, unit_type: str, unit_key: str, chunk_id: int) -> str:\n",
    "    raw = f\"{doc_id}|{doc_version}|{unit_type}|{unit_key}|c{chunk_id}\"\n",
    "    return hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5) Build JSONL\n",
    "# -------------------------\n",
    "def convert_to_jsonl(\n",
    "    pdf_paths: List[str],\n",
    "    docx_paths: List[str],\n",
    "    out_path: str = \"out/chunks.jsonl\",\n",
    "    chunk_size: int = 900,\n",
    "    overlap: int = 150,\n",
    "):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    total_units = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        # ---- PDF (ì‚¬ë¡€ì§‘) ----\n",
    "        for path in pdf_paths:\n",
    "            source = os.path.basename(path)\n",
    "            doc_version = guess_doc_version(source)\n",
    "            doc_id = make_doc_id(source)\n",
    "\n",
    "            for u in load_pdf_units(path):\n",
    "                total_units += 1\n",
    "                chunks = chunk_text(u[\"text\"], chunk_size, overlap)\n",
    "\n",
    "                for i, ch in enumerate(chunks):\n",
    "                    rec = {\n",
    "                        \"id\": make_chunk_id(doc_id, doc_version, u[\"unit_type\"], u[\"unit_key\"], i),\n",
    "                        \"text\": ch,\n",
    "                        \"metadata\": {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"doc_version\": doc_version,\n",
    "                            \"category\": \"case\",\n",
    "                            \"doc_type\": \"casebook\",\n",
    "                            \"source\": source,\n",
    "                            \"unit_type\": u[\"unit_type\"],\n",
    "                            \"unit_key\": u[\"unit_key\"],\n",
    "                            \"chunk_id\": i,\n",
    "                            \"priority\": 60,\n",
    "                            **u[\"meta\"],\n",
    "                        }\n",
    "                    }\n",
    "                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                    total_chunks += 1\n",
    "\n",
    "        # ---- DOCX (ë²•ë ¹) ----\n",
    "        for path in docx_paths:\n",
    "            source = os.path.basename(path)\n",
    "            doc_version = guess_doc_version(source)\n",
    "            doc_id = make_doc_id(source)\n",
    "            law_name = source.split(\"(\")[0].strip()\n",
    "\n",
    "            for u in load_docx_units(path):\n",
    "                total_units += 1\n",
    "                chunks = chunk_text(u[\"text\"], chunk_size, overlap)\n",
    "\n",
    "                for i, ch in enumerate(chunks):\n",
    "                    rec = {\n",
    "                        \"id\": make_chunk_id(doc_id, doc_version, u[\"unit_type\"], u[\"unit_key\"], i),\n",
    "                        \"text\": ch,\n",
    "                        \"metadata\": {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"doc_version\": doc_version,\n",
    "                            \"category\": \"law\",\n",
    "                            \"doc_type\": \"law_text\",\n",
    "                            \"law_name\": law_name,\n",
    "                            \"source\": source,\n",
    "                            \"unit_type\": u[\"unit_type\"],\n",
    "                            \"unit_key\": u[\"unit_key\"],\n",
    "                            \"chunk_id\": i,\n",
    "                            \"priority\": 95,\n",
    "                            **u[\"meta\"],\n",
    "                        }\n",
    "                    }\n",
    "                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                    total_chunks += 1\n",
    "\n",
    "    print(f\"âœ… JSONL ìƒì„± ì™„ë£Œ: {out_path}\")\n",
    "    print(f\"   units={total_units}, chunks={total_chunks}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6) ì‹¤í–‰ ë¶€ë¶„\n",
    "# -------------------------\n",
    "pdfs = [\n",
    "        \"data/raw/case/2025ì „ì„¸í”¼í•´ì§€ì›ì‚¬ë¡€ì§‘.pdf\",\n",
    "        \"data/raw/case/ì „ì„¸í”¼í•´ë²•ë¥ ìƒë‹´ì‚¬ë¡€ì§‘_250102.pdf\",\n",
    "    ]\n",
    "\n",
    "docxs = [\n",
    "        \"data/raw/law/ë¯¼ë²•(ë²•ë¥ )(ì œ20432í˜¸)(20260101)-ê°„ëžµ.docx\",\n",
    "        \"data/raw/law/ìƒê°€ê±´ë¬¼ ìž„ëŒ€ì°¨ë³´í˜¸ë²•(ë²•ë¥ )(ì œ21065í˜¸)(20260102).docx\",\n",
    "        \"data/raw/law/ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²•(ë²•ë¥ )(ì œ21065í˜¸)(20260102).docx\",\n",
    "        \"data/raw/rule/ì£¼íƒìž„ëŒ€ì°¨ê³„ì•½ì¦ì„œì˜ í™•ì •ì¼ìž ë¶€ì—¬ ë° ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™(ëŒ€ë²•ì›ê·œì¹™)(ì œ02986í˜¸)(20210610).docx\",\n",
    "        \"data/raw/rule/ì£¼íƒìž„ëŒ€ì°¨ê³„ì•½ì¦ì„œìƒì˜ í™•ì •ì¼ìž ë¶€ì—¬ ë° ìž„ëŒ€ì°¨ ì •ë³´ì œê³µì— ê´€í•œ ê·œì¹™(ë²•ë¬´ë¶€ë ¹)(ì œ01022í˜¸)(20220207).docx\",\n",
    "        \"data/raw/rule/ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹)(ì œ35947í˜¸)(20260102).docx\",\n",
    "    ]\n",
    "\n",
    "convert_to_jsonl(pdfs, docxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671a2b0-a6d8-4203-8171-cdb5050281c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070536d-0f91-4a39-ad2d-5c06b60ac743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f021075-01d5-4d46-9e96-7ae07214e183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a5e54-3907-4638-8587-752a58b73026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51be1c2-837a-43f3-99c0-1f87cc36becb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aacccd-3f64-4b67-b33f-7bb4ccc9dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34e83f1-a3f9-4ba9-852d-d0e598e74840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… clients ready: new-realestate __default__\n",
      "âœ… upserted: 100 (skipped: 0)\n",
      "âœ… upserted: 200 (skipped: 0)\n",
      "âœ… upserted: 300 (skipped: 0)\n",
      "âœ… upserted: 400 (skipped: 0)\n",
      "âœ… upserted: 500 (skipped: 0)\n",
      "âœ… upserted: 600 (skipped: 0)\n",
      "âœ… upserted: 700 (skipped: 0)\n",
      "ðŸŽ‰ DONE. total upserted: 776, skipped: 0\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# JUPYTER: JSONL -> OpenAI Embeddings -> Pinecone Upsert (SAFE)\n",
    "# - metadataì—ì„œ None(null) ì œê±°\n",
    "# - dict/list íƒ€ìž…ë„ ì•ˆì „ ë³€í™˜\n",
    "# =========================\n",
    "\n",
    "import os, json, time\n",
    "from typing import Any, Dict, Iterable, List\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# ====== CONFIG (ì—¬ê¸°ë§Œ ìˆ˜ì •) ======\n",
    "JSONL_PATH = \"out/chunks.jsonl\"   # ë„¤ jsonl ê²½ë¡œ\n",
    "INDEX_NAME = \"new-realestate\"\n",
    "NAMESPACE  = \"__default__\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-large\"  # 3072\n",
    "# =================================\n",
    "\n",
    "# ====== Clients ======\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì—†ìŒ\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"âŒ PINECONE_API_KEY í™˜ê²½ë³€ìˆ˜ ì—†ìŒ\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "print(\"âœ… clients ready:\", INDEX_NAME, NAMESPACE)\n",
    "\n",
    "# ====== JSONL Loader ======\n",
    "def load_jsonl(path: str) -> Iterable[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"âŒ JSONL íŒŒì‹± ì‹¤íŒ¨ (line {line_no}): {e}\") from e\n",
    "\n",
    "# ====== Embedding ======\n",
    "def embed_texts(texts: List[str], max_retries: int = 5) -> List[List[float]]:\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "            return [d.embedding for d in resp.data]\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            wait = min(8.0, 1.5 * attempt)\n",
    "            print(f\"âš ï¸ embedding retry {attempt}/{max_retries} in {wait:.1f}s: {e}\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"âŒ embedding failed: {last_err}\")\n",
    "\n",
    "# ====== Pinecone Metadata Cleaner (í•µì‹¬) ======\n",
    "def clean_value(v: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Pinecone metadata í—ˆìš© íƒ€ìž…:\n",
    "    - string, number, boolean\n",
    "    - list[string]\n",
    "    None/null ë¶ˆê°€\n",
    "    \"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(v, (str, int, float, bool)):\n",
    "        return v\n",
    "\n",
    "    if isinstance(v, list):\n",
    "        # listëŠ” string listë§Œ ì•ˆì „. ì•„ë‹ˆë©´ ë¬¸ìžì—´ë¡œ ìºìŠ¤íŒ…\n",
    "        return [x if isinstance(x, str) else str(x) for x in v]\n",
    "\n",
    "    if isinstance(v, dict):\n",
    "        # dictëŠ” JSON ë¬¸ìžì—´ë¡œ ë³€í™˜\n",
    "        return json.dumps(v, ensure_ascii=False)\n",
    "\n",
    "    # ê·¸ ì™¸ íƒ€ìž…ì€ ë¬¸ìžì—´ë¡œ\n",
    "    return str(v)\n",
    "\n",
    "def clean_metadata(md: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    for k, v in md.items():\n",
    "        vv = clean_value(v)\n",
    "        if vv is None:\n",
    "            continue  # âœ… Noneì´ë©´ key ìžì²´ ì œê±°\n",
    "        out[k] = vv\n",
    "    return out\n",
    "\n",
    "# ====== Upsert one batch ======\n",
    "def upsert_batch(rows: List[Dict[str, Any]]):\n",
    "    # í•„ìˆ˜ í•„ë“œ ì²´í¬\n",
    "    for r in rows:\n",
    "        if \"id\" not in r or not r[\"id\"]:\n",
    "            raise ValueError(\"âŒ ì–´ë–¤ rowì— idê°€ ë¹„ì–´ìžˆìŒ\")\n",
    "        if \"text\" not in r or not r[\"text\"]:\n",
    "            raise ValueError(f\"âŒ id={r.get('id')} rowì— textê°€ ë¹„ì–´ìžˆìŒ\")\n",
    "\n",
    "    texts = [r[\"text\"] for r in rows]\n",
    "    embs = embed_texts(texts)\n",
    "\n",
    "    vectors = []\n",
    "    for r, emb in zip(rows, embs):\n",
    "        md = {\n",
    "            \"article\":  r.get(\"article\"),     # âœ… Noneì´ë©´ ì œê±°ë¨\n",
    "            \"chunk_id\": r.get(\"chunk_id\"),\n",
    "            \"category\": r.get(\"category\"),\n",
    "            \"law_type\": r.get(\"law_type\"),\n",
    "            \"priority\": r.get(\"priority\"),\n",
    "            \"source\":   r.get(\"source\"),\n",
    "            \"text\":     r.get(\"text\"),        # RAG ê·¼ê±° ì¶œë ¥ìš©\n",
    "        }\n",
    "\n",
    "        vectors.append({\n",
    "            \"id\": str(r[\"id\"]),\n",
    "            \"values\": emb,\n",
    "            \"metadata\": clean_metadata(md),\n",
    "        })\n",
    "\n",
    "    index.upsert(vectors=vectors, namespace=NAMESPACE)\n",
    "\n",
    "# ====== Main loop ======\n",
    "buffer = []\n",
    "total = 0\n",
    "skipped = 0\n",
    "\n",
    "for row in load_jsonl(JSONL_PATH):\n",
    "    # row ìžì²´ê°€ ì´ìƒí•œ ê²½ìš° ìŠ¤í‚µ\n",
    "    if not isinstance(row, dict):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    if not row.get(\"id\") or not row.get(\"text\"):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    buffer.append(row)\n",
    "\n",
    "    if len(buffer) >= BATCH_SIZE:\n",
    "        upsert_batch(buffer)\n",
    "        total += len(buffer)\n",
    "        print(f\"âœ… upserted: {total} (skipped: {skipped})\")\n",
    "        buffer.clear()\n",
    "\n",
    "if buffer:\n",
    "    upsert_batch(buffer)\n",
    "    total += len(buffer)\n",
    "\n",
    "print(f\"ðŸŽ‰ DONE. total upserted: {total}, skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e3a67-10b4-45be-b589-9ad640ad66a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a7da0-8aa3-41c0-9fb6-90e3ea259a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c0601-245a-4f9e-85e2-6e0d500b78f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a7806-4d8c-49c8-857c-87a39ac305f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44e416-d930-41b4-b98b-ceeee4c2ec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62432fb7-8abe-4f4f-93d4-44dc77344886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794538e-9e3d-46dd-aeae-ccaa6fbcb996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcde39f-087c-404d-bb3b-960eeaa5e7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf495c-59a1-4f46-a2e1-27390eca1a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad975b-d9ac-4ff9-a1b8-dc56d9831c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d4f08-5064-46ba-ac5f-4629ef6395cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4363cf-d23d-48d5-9f42-00e51d038d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13967665-1aae-40df-b71c-697c42b2b29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1303990-97aa-4cb9-98af-4f2d242c6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG Chatbot ready (index=new-realestate, ns=__default__, embed=text-embedding-3-large, chat=gpt-4.1-mini)\n",
      "ì¢…ë£Œ: Ctrl+C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rag_chatbot.py\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ====== ENV ======\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"new-realestate\")\n",
    "NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\", \"__default__\")\n",
    "\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"text-embedding-3-large\")  # 3072\n",
    "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4.1-mini\")\n",
    "\n",
    "TOP_K = int(os.getenv(\"TOP_K\", \"6\"))\n",
    "SCORE_THRESHOLD = float(os.getenv(\"SCORE_THRESHOLD\", \"0.20\"))  # í”„ë¡œì íŠ¸ì— ë§žê²Œ ì¡°ì ˆ\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"âŒ PINECONE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ====== CLIENTS ======\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    id: str\n",
    "    score: float\n",
    "    text: str\n",
    "    metadata: dict[str, Any]\n",
    "\n",
    "\n",
    "def embed(text: str) -> list[float]:\n",
    "    r = client.embeddings.create(model=EMBED_MODEL, input=text)\n",
    "    return r.data[0].embedding\n",
    "\n",
    "\n",
    "def retrieve(query: str, top_k: int = TOP_K) -> list[RetrievedChunk]:\n",
    "    qvec = embed(query)\n",
    "\n",
    "    res = index.query(\n",
    "        namespace=NAMESPACE,\n",
    "        vector=qvec,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "\n",
    "    matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
    "\n",
    "    out: list[RetrievedChunk] = []\n",
    "    for m in matches:\n",
    "        mid = m.get(\"id\") if isinstance(m, dict) else getattr(m, \"id\", \"\")\n",
    "        score = m.get(\"score\") if isinstance(m, dict) else getattr(m, \"score\", 0.0)\n",
    "        md = m.get(\"metadata\", {}) if isinstance(m, dict) else getattr(m, \"metadata\", {})\n",
    "\n",
    "        text = (md.get(\"text\") or md.get(\"chunk\") or md.get(\"content\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        out.append(RetrievedChunk(id=mid, score=float(score), text=text, metadata=md))\n",
    "\n",
    "    # score threshold ì ìš©\n",
    "    out = [c for c in out if c.score >= SCORE_THRESHOLD]\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_context(chunks: list[RetrievedChunk]) -> str:\n",
    "    \"\"\"\n",
    "    LLMì—ê²Œ ë³´ì—¬ì¤„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¹”ë”í•˜ê²Œ êµ¬ì„±.\n",
    "    ë©”íƒ€ë°ì´í„° í‚¤ëŠ” ë„ˆ ì ìž¬ í¬ë§·ì— ë§žì¶° ìœ ì—°í•˜ê²Œ ì²˜ë¦¬.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        md = c.metadata or {}\n",
    "        source = md.get(\"source\", \"\")\n",
    "        article = md.get(\"article\", \"\")\n",
    "        law_type = md.get(\"law_type\", \"\")\n",
    "        chunk_id = md.get(\"chunk_id\", \"\")\n",
    "\n",
    "        header_bits = []\n",
    "        if law_type: header_bits.append(f\"law_type={law_type}\")\n",
    "        if source:   header_bits.append(f\"source={source}\")\n",
    "        if article:  header_bits.append(f\"article={article}\")\n",
    "        if chunk_id != \"\": header_bits.append(f\"chunk_id={chunk_id}\")\n",
    "        header = \" | \".join(header_bits) if header_bits else \"metadata=unknown\"\n",
    "\n",
    "        parts.append(\n",
    "            f\"[ê·¼ê±° {i}] score={c.score:.4f} id={c.id}\\n\"\n",
    "            f\"{header}\\n\"\n",
    "            f\"{c.text}\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "ë„ˆëŠ” í•œêµ­ì˜ ì „ì›”ì„¸(ì£¼íƒìž„ëŒ€ì°¨) ê´€ë ¨ ë²•ë¥ /ì‚¬ë¡€ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.\n",
    "ë°˜ë“œì‹œ ì œê³µëœ [ê·¼ê±°] ì•ˆì—ì„œë§Œ ì‚¬ì‹¤ì„ ì£¼ìž¥í•˜ê³ , ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ì¸¡í•˜ì§€ ë§ê³ \n",
    "ì œê³µëœ ê·¼ê±°ë¡œëŠ” íŒë‹¨í•˜ê¸° ì–´ë µë‹¤\"ë¼ê³  ë§í•œë‹¤.\n",
    "\n",
    "ìž„ëŒ€ì°¨ì— ê´€í•œ ì§ˆë¬¸ì—ì„œëŠ” ì‚¬ìš©ëŒ€ì°¨(ì°¨ì£¼)ì— ê´€í•œ ê·œì •ì„ ì§ì ‘ì ì¸ ë²•ì  ê·¼ê±°ë¡œ ë‹¨ì •ì ìœ¼ë¡œ ì¸ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "\n",
    "ë‹µë³€ í˜•ì‹:\n",
    "1) ê²°ë¡ (í•œë‘ ë¬¸ìž¥)\n",
    "2) ê·¼ê±° ìš”ì•½(ê·¼ê±° ë²ˆí˜¸ ì¸ìš©í•´ì„œ bullet)\n",
    "3) ì¶”ê°€ë¡œ í™•ì¸í•  ì‚¬ì‹¤(í•„ìš”í•˜ë©´)\n",
    "\"\"\"\n",
    "\n",
    "def answer(query: str) -> dict[str, Any]:\n",
    "    chunks = retrieve(query)\n",
    "    if not chunks:\n",
    "        return {\n",
    "            \"answer\": \"ì œê³µëœ ê·¼ê±°(ê²€ìƒ‰ ê²°ê³¼)ê°€ ë¶€ì¡±í•´ì„œ ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•´ ì£¼ì„¸ìš”.\",\n",
    "            \"sources\": [],\n",
    "        }\n",
    "\n",
    "    context = build_context(chunks)\n",
    "\n",
    "    msg = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\\\n",
    "[ì§ˆë¬¸]\n",
    "{query}\n",
    "\n",
    "[ê·¼ê±°]\n",
    "{context}\n",
    "\"\"\"},\n",
    "    ]\n",
    "\n",
    "    r = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=msg,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    text = r.choices[0].message.content\n",
    "\n",
    "    # ì‚¬ìš©ìžì—ê²Œ ê·¼ê±° ëª©ë¡ë„ í•¨ê»˜ ì œê³µ\n",
    "    sources = []\n",
    "    for c in chunks:\n",
    "        md = c.metadata or {}\n",
    "        sources.append({\n",
    "            \"id\": c.id,\n",
    "            \"score\": c.score,\n",
    "            \"source\": md.get(\"source\"),\n",
    "            \"article\": md.get(\"article\"),\n",
    "            \"law_type\": md.get(\"law_type\"),\n",
    "            \"chunk_id\": md.get(\"chunk_id\"),\n",
    "        })\n",
    "\n",
    "    return {\"answer\": text, \"sources\": sources}\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"âœ… RAG Chatbot ready (index={INDEX_NAME}, ns={NAMESPACE}, embed={EMBED_MODEL}, chat={CHAT_MODEL})\")\n",
    "    print(\"ì¢…ë£Œ: Ctrl+C\\n\")\n",
    "\n",
    "    while True:\n",
    "        q = input(\"Q> \").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        out = answer(q)\n",
    "        print(\"\\n=== A ===\")\n",
    "        print(out[\"answer\"])\n",
    "\n",
    "        print(\"\\n=== SOURCES ===\")\n",
    "        for i, s in enumerate(out[\"sources\"], 1):\n",
    "            print(f\"[{i}] score={s['score']:.4f} law_type={s.get('law_type')} source={s.get('source')} article={s.get('article')} chunk_id={s.get('chunk_id')} id={s.get('id')}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8945d2-246f-4d87-8201-fb2d24335199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc796c8f-f6ef-4919-97e9-7fd00daab418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2e0c2-0200-447e-8542-06da0bc16148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa985f-a98b-49ed-9901-f4c733712b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e546f7c-1300-4568-8024-747ca1d81fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eefba73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… loaded: LAW_CATEGORY_WEIGHTS= 11 synonym_rules= 75\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# (ADD) ì¹´í…Œê³ ë¦¬ í•¨ìˆ˜ / í‚¤ì›Œë“œ ì‚¬ì „ ë¡œë”© + ì •ê·œí™” ìœ í‹¸\n",
    "# - ì—…ë¡œë“œí•œ `ì¹´í…Œê³ ë¦¬ í•¨ìˆ˜.txt`, `í‚¤ì›Œë“œì‚¬ì „.txt`ë¥¼ ê·¸ëŒ€ë¡œ í™œìš©\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def parse_synonym_rules(raw: str):\n",
    "    \"\"\"'A -> B' í˜•íƒœì˜ ì¹˜í™˜ ê·œì¹™ íŒŒì‹±\"\"\"\n",
    "    rules = []\n",
    "    for line in raw.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        line = line.strip('\",')\n",
    "        if \"->\" in line:\n",
    "            src, dst = [x.strip() for x in line.split(\"->\", 1)]\n",
    "            if src and dst and src != dst:\n",
    "                rules.append((src, dst))\n",
    "    # ê¸´ í‘œí˜„ë¶€í„° ìš°ì„  ì¹˜í™˜\n",
    "    rules.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    return rules\n",
    "\n",
    "def normalize_text(text: str, rules):\n",
    "    out = text\n",
    "    for src, dst in rules:\n",
    "        out = out.replace(src, dst)\n",
    "    return out\n",
    "\n",
    "# --- 1) ì¹´í…Œê³ ë¦¬ í•¨ìˆ˜ ë¡œë”© ---\n",
    "_cat_ns = {}\n",
    "cat_text = Path(\"data/ì¹´í…Œê³ ë¦¬ í•¨ìˆ˜.txt\").read_text(encoding=\"utf-8\")\n",
    "exec(cat_text, _cat_ns)  # get_law_category() ì£¼ìž…\n",
    "get_law_category = _cat_ns[\"get_law_category\"]\n",
    "LAW_CATEGORY_WEIGHTS = get_law_category()\n",
    "\n",
    "# --- 2) í‚¤ì›Œë“œ ì‚¬ì „ ë¡œë”© ---\n",
    "_kw_ns = {}\n",
    "kw_text = Path(\"data/í‚¤ì›Œë“œì‚¬ì „.txt\").read_text(encoding=\"utf-8\")\n",
    "exec(kw_text, _kw_ns)  # keyword_dict ì£¼ìž…\n",
    "keyword_dict = _kw_ns[\"keyword_dict\"]\n",
    "\n",
    "SYNONYM_RULES = parse_synonym_rules(\"\\n\".join(keyword_dict))\n",
    "\n",
    "print(\"âœ… loaded:\", \"LAW_CATEGORY_WEIGHTS=\", len(LAW_CATEGORY_WEIGHTS), \"synonym_rules=\", len(SYNONYM_RULES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e86713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# (ADD) ë²•ë ¹/ìžë£Œ íƒ€ìž… ë¶„ë¥˜ + ì¤‘ìš”ë„(priority) ì ìš© ë²„ì „ JSONL ìƒì„±\n",
    "# - ê¸°ì¡´ convert_to_jsonl() ì½”ë“œëŠ” ìœ ì§€\n",
    "# - priority ê·œì¹™:\n",
    "#   ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²•=1, ì‹œí–‰ë ¹=2, ì‹œí–‰ê·œì¹™/ëŒ€ë²•ì›ê·œì¹™=3, ë¯¼ë²•=4, ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´=50\n",
    "# =========================\n",
    "import os\n",
    "\n",
    "PRIORITY_MAP = {\n",
    "    \"ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²•\": 1,\n",
    "    \"ì‹œí–‰ë ¹\": 2,\n",
    "    \"ì‹œí–‰ê·œì¹™/ëŒ€ë²•ì›ê·œì¹™\": 3,\n",
    "    \"ë¯¼ë²•\": 4,\n",
    "    \"ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´\": 50,\n",
    "}\n",
    "\n",
    "def detect_law_type_from_source(source_filename: str, full_path: str = \"\") -> str:\n",
    "    s = source_filename\n",
    "\n",
    "    # ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´: pdf case í´ë” ë˜ëŠ” íŒŒì¼ëª… ížŒíŠ¸\n",
    "    p = (full_path or \"\").replace(\"\\\\\", \"/\")\n",
    "    if p.endswith(\".pdf\") or \"/case/\" in p or \"ì‚¬ë¡€\" in s or \"íŒë¡€\" in s or \"ìƒë‹´\" in s or \"í”¼í•´\" in s:\n",
    "        return \"ì‚¬ë¡€/íŒë¡€/í”¼í•´ìƒë‹´\"\n",
    "\n",
    "    # ì‹œí–‰ë ¹\n",
    "    if \"ì‹œí–‰ë ¹\" in s:\n",
    "        return \"ì‹œí–‰ë ¹\"\n",
    "\n",
    "    # ì‹œí–‰ê·œì¹™/ëŒ€ë²•ì›ê·œì¹™\n",
    "    if \"ê·œì¹™\" in s or \"ëŒ€ë²•ì›ê·œì¹™\" in s or \"ë²•ë¬´ë¶€ë ¹\" in s:\n",
    "        return \"ì‹œí–‰ê·œì¹™/ëŒ€ë²•ì›ê·œì¹™\"\n",
    "\n",
    "    # ë¯¼ë²•\n",
    "    if s.startswith(\"ë¯¼ë²•\") or \"ë¯¼ë²•\" in s:\n",
    "        return \"ë¯¼ë²•\"\n",
    "\n",
    "    # ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²•(ì›ë¬¸/ë²•ë¥ )\n",
    "    if \"ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²•\" in s:\n",
    "        return \"ì£¼íƒìž„ëŒ€ì°¨ë³´í˜¸ë²•\"\n",
    "\n",
    "    # fallback\n",
    "    return \"ê¸°íƒ€\"\n",
    "\n",
    "def priority_from_law_type(law_type: str) -> int:\n",
    "    return PRIORITY_MAP.get(law_type, 99)\n",
    "\n",
    "def convert_to_jsonl_v2(\n",
    "    pdf_paths,\n",
    "    docx_paths,\n",
    "    out_path: str = \"out/chunks.jsonl\",\n",
    "    chunk_size: int = 900,\n",
    "    overlap: int = 150,\n",
    "):\n",
    "    \"\"\"ê¸°ì¡´ convert_to_jsonlê³¼ ë™ì¼í•˜ê²Œ JSONLì„ ë§Œë“¤ë˜, law_type/priorityë¥¼ ê·œì¹™ëŒ€ë¡œ ì¶”ê°€\"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    total_units = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # ---- PDF (ì‚¬ë¡€ì§‘ ë“±) ----\n",
    "        for path in pdf_paths:\n",
    "            source = os.path.basename(path)\n",
    "            doc_version = guess_doc_version(source)\n",
    "            doc_id = make_doc_id(source)\n",
    "\n",
    "            law_type = detect_law_type_from_source(source, path)\n",
    "            pr = priority_from_law_type(law_type)\n",
    "\n",
    "            units = extract_units_from_pdf(path)  # ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©\n",
    "            for u in units:\n",
    "                total_units += 1\n",
    "                chunks = chunk_text(u[\"text\"], chunk_size, overlap)\n",
    "                for i, ch in enumerate(chunks):\n",
    "                    rec = {\n",
    "                        \"id\": make_chunk_id(doc_id, doc_version, u[\"unit_type\"], u[\"unit_key\"], i),\n",
    "                        \"text\": ch,\n",
    "                        \"metadata\": {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"doc_version\": doc_version,\n",
    "                            \"category\": \"law\",\n",
    "                            \"doc_type\": \"case_pdf\",\n",
    "                            \"law_type\": law_type,\n",
    "                            \"priority\": pr,\n",
    "                            \"base_priority\": 60,  # ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜(ì°¸ê³ ìš©)\n",
    "                            \"source\": source,\n",
    "                            \"unit_type\": u[\"unit_type\"],\n",
    "                            \"unit_key\": u[\"unit_key\"],\n",
    "                            \"chunk_id\": i,\n",
    "                            **u[\"meta\"],\n",
    "                        },\n",
    "                    }\n",
    "                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                    total_chunks += 1\n",
    "\n",
    "        # ---- DOCX (ë²•ë ¹) ----\n",
    "        for path in docx_paths:\n",
    "            source = os.path.basename(path)\n",
    "            doc_version = guess_doc_version(source)\n",
    "            doc_id = make_doc_id(source)\n",
    "\n",
    "            law_name = guess_law_name(source)\n",
    "            law_type = detect_law_type_from_source(source, path)\n",
    "            pr = priority_from_law_type(law_type)\n",
    "\n",
    "            units = extract_units_from_docx(path)  # ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©\n",
    "            for u in units:\n",
    "                total_units += 1\n",
    "                chunks = chunk_text(u[\"text\"], chunk_size, overlap)\n",
    "                for i, ch in enumerate(chunks):\n",
    "                    rec = {\n",
    "                        \"id\": make_chunk_id(doc_id, doc_version, u[\"unit_type\"], u[\"unit_key\"], i),\n",
    "                        \"text\": ch,\n",
    "                        \"metadata\": {\n",
    "                            \"doc_id\": doc_id,\n",
    "                            \"doc_version\": doc_version,\n",
    "                            \"category\": \"law\",\n",
    "                            \"doc_type\": \"law_text\",\n",
    "                            \"law_name\": law_name,\n",
    "                            \"law_type\": law_type,\n",
    "                            \"priority\": pr,\n",
    "                            \"base_priority\": 95,  # ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜(ì°¸ê³ ìš©)\n",
    "                            \"source\": source,\n",
    "                            \"unit_type\": u[\"unit_type\"],\n",
    "                            \"unit_key\": u[\"unit_key\"],\n",
    "                            \"chunk_id\": i,\n",
    "                            **u[\"meta\"],\n",
    "                        },\n",
    "                    }\n",
    "                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                    total_chunks += 1\n",
    "\n",
    "    print(f\"âœ… JSONL(v2) ìƒì„± ì™„ë£Œ: {out_path}\")\n",
    "    print(f\"   units={total_units}, chunks={total_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1734ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# (ADD) RAG Chatbot v2\n",
    "# - ì‚¬ìš©ìž ì§ˆë¬¸: synonym ì •ê·œí™” ì ìš©\n",
    "# - ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ë¡œ \"ìž¬ëž­í‚¹\" (ê°„ë‹¨ ë²„ì „)\n",
    "# - priority(ì¤‘ìš”ë„)ë¡œ ì¶”ê°€ ë³´ì •\n",
    "# =========================\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class Retrieved:\n",
    "    id: str\n",
    "    score: float\n",
    "    text: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "def embed_query(text: str) -> List[float]:\n",
    "    # ì§ˆë¬¸ì€ ì •ê·œí™” í›„ ìž„ë² ë”©\n",
    "    norm = normalize_text(text, SYNONYM_RULES)\n",
    "    r = client.embeddings.create(model=EMBED_MODEL, input=norm)\n",
    "    return r.data[0].embedding\n",
    "\n",
    "def keyword_bonus(query_norm: str, chunk_text: str) -> float:\n",
    "    \"\"\"LAW_CATEGORY_WEIGHTS ê¸°ë°˜: queryì— ë‚˜íƒ€ë‚œ í•µì‹¬í‚¤ì›Œë“œë¥¼ chunkê°€ í¬í•¨í•˜ë©´ ë³´ë„ˆìŠ¤\"\"\"\n",
    "    bonus = 0.0\n",
    "    for cat, kwmap in LAW_CATEGORY_WEIGHTS.items():\n",
    "        # queryì— í¬í•¨ëœ í‚¤ì›Œë“œë§Œ ëŒ€ìƒìœ¼ë¡œ ì¶•ì†Œ\n",
    "        active = [ (k,w) for k,w in kwmap.items() if k and (k in query_norm) ]\n",
    "        if not active:\n",
    "            continue\n",
    "        # chunkì— í¬í•¨ë˜ë©´ ê°€ì¤‘ì¹˜ í•©ì‚°\n",
    "        hit = 0\n",
    "        for k,w in active:\n",
    "            if k in chunk_text:\n",
    "                hit += w\n",
    "        if hit:\n",
    "            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì™„ë§Œí•˜ê²Œ ë°˜ì˜\n",
    "            bonus += math.log1p(hit) * 0.02\n",
    "    return bonus\n",
    "\n",
    "def priority_bonus(md: Dict[str, Any]) -> float:\n",
    "    p = md.get(\"priority\", 99)\n",
    "    # ìˆ«ìžê°€ ìž‘ì„ìˆ˜ë¡ ì¤‘ìš”(1~4). case(50)ëŠ” ë³´ë„ˆìŠ¤ ê±°ì˜ ì—†ìŒ\n",
    "    if p in (1,2,3,4):\n",
    "        return (5 - p) * 0.02  # 1->0.08, 4->0.02\n",
    "    return 0.0\n",
    "\n",
    "def retrieve_v2(question: str, top_k: int = 12, fetch_k: int = 40) -> List[Retrieved]:\n",
    "    q_norm = normalize_text(question, SYNONYM_RULES)\n",
    "    q_vec = embed_query(question)\n",
    "\n",
    "    res = index.query(\n",
    "        namespace=NAMESPACE,\n",
    "        vector=q_vec,\n",
    "        top_k=fetch_k,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "\n",
    "    matches = res.get(\"matches\", []) if isinstance(res, dict) else getattr(res, \"matches\", [])\n",
    "    out: List[Retrieved] = []\n",
    "\n",
    "    for m in matches:\n",
    "        md = m.get(\"metadata\", {}) if isinstance(m, dict) else getattr(m, \"metadata\", {})\n",
    "        text = (md.get(\"text\") or md.get(\"chunk\") or md.get(\"content\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        out.append(Retrieved(\n",
    "            id=m.get(\"id\") if isinstance(m, dict) else getattr(m, \"id\", \"\"),\n",
    "            score=float(m.get(\"score\") if isinstance(m, dict) else getattr(m, \"score\", 0.0)),\n",
    "            text=text,\n",
    "            metadata=md,\n",
    "        ))\n",
    "\n",
    "    # --- re-rank ---\n",
    "    reranked = []\n",
    "    for r in out:\n",
    "        b = keyword_bonus(q_norm, r.text) + priority_bonus(r.metadata)\n",
    "        reranked.append((r.score + b, r))\n",
    "    reranked.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [r for _, r in reranked[:top_k]]\n",
    "\n",
    "def answer_v2(question: str) -> Dict[str, Any]:\n",
    "    chunks = retrieve_v2(question, top_k=TOP_K, fetch_k=max(30, TOP_K*6))\n",
    "\n",
    "    # promptì— \"ì •ê·œí™”ëœ ì§ˆë¬¸\"ë„ í•¨ê»˜ ë„£ì–´ì£¼ë©´ ë” ì•ˆì •ì \n",
    "    q_norm = normalize_text(question, SYNONYM_RULES)\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{i+1}] (law_type={c.metadata.get('law_type')}, priority={c.metadata.get('priority')}, source={c.metadata.get('source')}, unit={c.metadata.get('unit_key')})\\n{c.text}\"\n",
    "        for i, c in enumerate(chunks)\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"ë„ˆëŠ” ì£¼íƒìž„ëŒ€ì°¨ ë¶„ì•¼ RAG ì±—ë´‡ì´ë‹¤. ê·¼ê±°ì— ê¸°ë°˜í•´ ë‹µí•˜ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•œë‹¤.\"},\n",
    "        {\"role\":\"user\",\"content\":f\"\"\"ì§ˆë¬¸: {question}\n",
    "(ì •ê·œí™” ì§ˆë¬¸: {q_norm})\n",
    "\n",
    "ì•„ëž˜ ê·¼ê±°ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì¤˜. ê·¼ê±°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆ.\n",
    "\n",
    "=== ê·¼ê±° ===\n",
    "{context}\n",
    "\"\"\"}\n",
    "    ]\n",
    "\n",
    "    r = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    ans = r.choices[0].message.content\n",
    "\n",
    "    sources = []\n",
    "    for c in chunks:\n",
    "        md = dict(c.metadata)\n",
    "        md.update({\"id\": c.id, \"score\": c.score})\n",
    "        sources.append(md)\n",
    "\n",
    "    return {\"answer\": ans, \"sources\": sources}\n",
    "\n",
    "# --- ê°„ë‹¨ ì‹¤í–‰ ---\n",
    "def chat_loop_v2():\n",
    "    while True:\n",
    "        q = input(\"\\nQ> \").strip()\n",
    "        if q.lower() in (\"q\",\"quit\",\"exit\"):\n",
    "            break\n",
    "        out = answer_v2(q)\n",
    "        print(\"\\n=== A ===\")\n",
    "        print(out[\"answer\"])\n",
    "        print(\"\\n=== SOURCES ===\")\n",
    "        for i, s in enumerate(out[\"sources\"], 1):\n",
    "            print(f\"[{i}] score={s.get('score'):.4f} law_type={s.get('law_type')} priority={s.get('priority')} article={s.get('article')} chunk_id={s.get('chunk_id')} source={s.get('source')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9d8dd-b36d-4b73-a50b-433d7fba6c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
