""" 
Unified Hybrid RAG module (Dense + Sparse) for Korean legal Q&A (e.g., housing lease).

Model choices (as requested)
- normalize_query(): Upstage SOLAR Pro2 (chat)  -> model="solar-pro2"
- generate_answer(): OpenAI GPT-4o-mini         -> model="gpt-4o-mini"
- embeddings (dense retrieval): Upstage SOLAR embedding (configurable)

Hybrid retrieval (Dense + Sparse)
- Dense: PineconeVectorStore similarity_search_with_score (fallback: similarity_search)
- Sparse:
  * default: BM25 on *dense candidates* (no extra corpus preload)
  * optional: *global BM25* (true sparse retrieval) if you call build_global_bm25(...)
- Fusion: rank-based RRF (default) or rank_sum

Environment variables
- PINECONE_API_KEY (required)
- UPSTAGE_API_KEY  (required for Upstage embeddings & normalize_query)
- OPENAI_API_KEY   (required for generate_answer)
- COHERE_API_KEY   (optional, only if enable_rerank=True)

No FastAPI integration. Pure Python module.
"""
from __future__ import annotations

import logging
import math
import os
import re
import heapq
from abc import ABC, abstractmethod
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

from langchain_pinecone import PineconeVectorStore

# ----------------------------
# Optional: Upstage (embeddings + chat)
# ----------------------------
try:
    from langchain_upstage import UpstageEmbeddings, ChatUpstage  # type: ignore
    UPSTAGE_AVAILABLE = True
except Exception:
    UpstageEmbeddings = None  # type: ignore
    ChatUpstage = None  # type: ignore
    UPSTAGE_AVAILABLE = False

# ----------------------------
# Optional: OpenAI chat
# ----------------------------
try:
    from langchain_openai import ChatOpenAI  # type: ignore
    OPENAI_AVAILABLE = True
except Exception:
    ChatOpenAI = None  # type: ignore
    OPENAI_AVAILABLE = False

# ----------------------------
# Optional: BM25 (rank_bm25)
# ----------------------------
try:
    from rank_bm25 import BM25Okapi, BM25Plus  # type: ignore
    BM25_AVAILABLE = True
except Exception:
    BM25Okapi = None  # type: ignore
    BM25Plus = None  # type: ignore
    BM25_AVAILABLE = False

# ----------------------------
# Optional: Kiwi tokenizer
# ----------------------------
try:
    from kiwipiepy import Kiwi  # type: ignore
    KIWI_AVAILABLE = True
except Exception:
    Kiwi = None  # type: ignore
    KIWI_AVAILABLE = False

# ----------------------------
# Optional: Cohere rerank
# ----------------------------
try:
    import cohere  # type: ignore
    COHERE_AVAILABLE = True
except Exception:
    cohere = None  # type: ignore
    COHERE_AVAILABLE = False


# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# --------------------------------------------------------------------------------------
# Index names
# --------------------------------------------------------------------------------------
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index",
    "rule": "rule-index",
    "case": "case-index",
}


# --------------------------------------------------------------------------------------
# Keyword dictionary (query normalization)
# --------------------------------------------------------------------------------------
KEYWORD_DICT: Dict[str, str] = {
    # 1. Í≥ÑÏïΩ Ï£ºÏ≤¥ Î∞è ÎåÄÏÉÅ
    "ÏßëÏ£ºÏù∏": "ÏûÑÎåÄÏù∏", "Í±¥Î¨ºÏ£º": "ÏûÑÎåÄÏù∏", "Ï£ºÏù∏Ïßë": "ÏûÑÎåÄÏù∏",
    "ÏûÑÎåÄÏóÖÏûê": "ÏûÑÎåÄÏù∏", "ÏÉàÏ£ºÏù∏": "ÏûÑÎåÄÏù∏",
    "ÏÑ∏ÏûÖÏûê": "ÏûÑÏ∞®Ïù∏", "ÏõîÏÑ∏ÏûÖÏûê": "ÏûÑÏ∞®Ïù∏", "ÏÑ∏Îì§Ïñ¥ÏÇ¨ÎäîÏÇ¨Îûå": "ÏûÑÏ∞®Ïù∏",
    "ÏûÑÏ∞®Ïûê": "ÏûÑÏ∞®Ïù∏", "ÏûÖÏ£ºÏûê": "ÏûÑÏ∞®Ïù∏",
    "Î∂ÄÎèôÏÇ∞": "Í≥µÏù∏Ï§ëÍ∞úÏÇ¨", "Ï§ëÍ∞úÏù∏": "Í≥µÏù∏Ï§ëÍ∞úÏÇ¨", "Ï§ëÍ∞úÏÜå": "Í≥µÏù∏Ï§ëÍ∞úÏÇ¨",
    "ÎπåÎùº": "ÏûÑÏ∞®Ï£ºÌÉù", "ÏïÑÌååÌä∏": "ÏûÑÏ∞®Ï£ºÌÉù", "Ïò§ÌîºÏä§ÌÖî": "ÏûÑÏ∞®Ï£ºÌÉù",
    "Ïö∞Î¶¨Ïßë": "ÏûÑÏ∞®Ï£ºÌÉù", "Í±∞Ï£ºÏßÄ": "ÏûÑÏ∞®Ï£ºÌÉù",
    "Í≥ÑÏïΩÏÑú": "ÏûÑÎåÄÏ∞®Í≥ÑÏïΩÏ¶ùÏÑú", "ÏßëÎ¨∏ÏÑú": "ÏûÑÎåÄÏ∞®Í≥ÑÏïΩÏ¶ùÏÑú", "Ï¢ÖÏù¥": "ÏûÑÎåÄÏ∞®Í≥ÑÏïΩÏ¶ùÏÑú",

    # 2. Î≥¥Ï¶ùÍ∏à Î∞è Í∏àÏ†Ñ
    "Î≥¥Ï¶ùÍ∏à": "ÏûÑÎåÄÏ∞®Î≥¥Ï¶ùÍ∏à", "Ï†ÑÏÑ∏Í∏à": "ÏûÑÎåÄÏ∞®Î≥¥Ï¶ùÍ∏à", "Î≥¥Ï¶ùÎ≥¥Ìóò": "Î≥¥Ï¶ùÍ∏àÎ∞òÌôòÎ≥¥Ï¶ù",
    "ÎèàÎ™ªÎ∞õÏùå": "Î≥¥Ï¶ùÍ∏àÎØ∏Î∞òÌôò", "ÏïàÎèåÎ†§Ï§å": "Î≥¥Ï¶ùÍ∏àÎØ∏Î∞òÌôò", "Î™ªÎèåÎ†§Î∞õÏùå": "Î≥¥Ï¶ùÍ∏àÎØ∏Î∞òÌôò",
    "ÏõîÏÑ∏": "Ï∞®ÏûÑ", "Í¥ÄÎ¶¨ÎπÑ": "Í¥ÄÎ¶¨ÎπÑ", "Ïó∞Ï≤¥": "Ï∞®ÏûÑÏó∞Ï≤¥", "Î∞ÄÎ¶º": "Ï∞®ÏûÑÏó∞Ï≤¥",
    "Î≥µÎπÑ": "Ï§ëÍ∞úÎ≥¥Ïàò", "ÏàòÏàòÎ£å": "Ï§ëÍ∞úÎ≥¥Ïàò", "Ï§ëÍ∞úÎπÑ": "Ï§ëÍ∞úÎ≥¥Ïàò",
    "ÏõîÏÑ∏Ïò¨Î¶¨Í∏∞": "Ï∞®ÏûÑÏ¶ùÏï°", "Ïù∏ÏÉÅ": "Ï¶ùÏï°", "ÎçîÎã¨ÎùºÍ≥†Ìï®": "Ï¶ùÏï°",
    "ÏõîÏÑ∏ÍπéÍ∏∞": "Ï∞®ÏûÑÍ∞êÏï°", "Ìï†Ïù∏": "Í∞êÏï°", "ÎÇ¥Î¶¨Í∏∞": "Í∞êÏï°",
    "ÎèàÎ®ºÏ†ÄÎ∞õÍ∏∞": "Ïö∞ÏÑ†Î≥ÄÏ†úÍ∂å", "ÏàúÏúÑ": "Ïö∞ÏÑ†Î≥ÄÏ†úÍ∂å", "ÏïàÏ†ÑÏû•Ïπò": "ÎåÄÌï≠Î†•",
    "ÎèåÎ†§Î∞õÍ∏∞": "Î≥¥Ï¶ùÍ∏àÎ∞òÌôò",

    # 3. Í∏∞Í∞Ñ Î∞è Ï¢ÖÎ£å/Í∞±Ïã†
    "Ïû¨Í≥ÑÏïΩ": "Í≥ÑÏïΩÍ∞±Ïã†", "Ïó∞Ïû•": "Í≥ÑÏïΩÍ∞±Ïã†", "Í∞±Ïã†": "Í≥ÑÏïΩÍ∞±Ïã†",
    "Í∞±Ïã†Ï≤≠Íµ¨": "Í≥ÑÏïΩÍ∞±Ïã†ÏöîÍµ¨Í∂å", "2ÎÖÑÎçî": "Í≥ÑÏïΩÍ∞±Ïã†ÏöîÍµ¨Í∂å", "2ÌîåÎü¨Ïä§2": "Í≥ÑÏïΩÍ∞±Ïã†ÏöîÍµ¨Í∂å",
    "ÏûêÎèôÏó∞Ïû•": "Î¨µÏãúÏ†ÅÍ∞±Ïã†", "Î¨µÏãú": "Î¨µÏãúÏ†ÅÍ∞±Ïã†", "Ïó∞ÎùΩÏóÜÏùå": "Î¨µÏãúÏ†ÅÍ∞±Ïã†",
    "Ïù¥ÏÇ¨": "Ï£ºÌÉùÏùòÏù∏ÎèÑ", "ÏßêÎπºÍ∏∞": "Ï£ºÌÉùÏùòÏù∏ÎèÑ", "Ìá¥Í±∞": "Ï£ºÌÉùÏùòÏù∏ÎèÑ",
    "Î∞©Îπº": "Í≥ÑÏïΩÌï¥ÏßÄ", "Ï§ëÎèÑÌï¥ÏßÄ": "Í≥ÑÏïΩÌï¥ÏßÄ",
    "Ï£ºÏÜåÏòÆÍ∏∞Í∏∞": "Ï£ºÎØºÎì±Î°ù", "Ï†ÑÏûÖÏã†Í≥†": "Ï£ºÎØºÎì±Î°ù", "Ï£ºÏÜåÏßÄÏù¥Ï†Ñ": "Ï£ºÎØºÎì±Î°ù",
    "ÏßëÏ£ºÏù∏Î∞îÎÄú": "ÏûÑÎåÄÏù∏ÏßÄÏúÑÏäπÍ≥Ñ", "Ï£ºÏù∏Î∞îÎÄú": "ÏûÑÎåÄÏù∏ÏßÄÏúÑÏäπÍ≥Ñ",
    "Îß§Îß§": "ÏûÑÎåÄÏù∏ÏßÄÏúÑÏäπÍ≥Ñ",
    "ÎÇòÍ∞ÄÎùºÍ≥†Ìï®": "Í≥ÑÏïΩÍ∞±Ïã†Í±∞Ï†à", "Ï´ìÍ≤®ÎÇ®": "Î™ÖÎèÑ", "ÎπÑÏõåÎã¨Îùº": "Î™ÖÎèÑ",

    # 4. ÏàòÎ¶¨ Î∞è ÏÉùÌôúÌôòÍ≤Ω
    "ÏßëÍ≥†ÏπòÍ∏∞": "ÏàòÏÑ†ÏùòÎ¨¥", "ÏàòÎ¶¨": "ÏàòÏÑ†ÏùòÎ¨¥", "Í≥†Ï≥êÏ§ò": "ÏàòÏÑ†ÏùòÎ¨¥",
    "ÏïàÍ≥†Ï≥êÏ§å": "ÏàòÏÑ†ÏùòÎ¨¥ÏúÑÎ∞ò",
    "Í≥∞Ìå°Ïù¥": "ÌïòÏûê", "Î¨ºÏÉò": "ÎàÑÏàò", "Î≥¥ÏùºÎü¨Í≥†Ïû•": "ÌïòÏûê", "ÌååÏÜê": "ÌõºÏÜê",
    "Íπ®ÎÅóÏù¥ÏπòÏö∞Í∏∞": "ÏõêÏÉÅÌöåÎ≥µÏùòÎ¨¥", "ÏõêÎûòÎåÄÎ°úÌï¥ÎÜìÍ∏∞": "ÏõêÏÉÅÌöåÎ≥µ",
    "Ï≤≠ÏÜåÎπÑ": "ÏõêÏÉÅÌöåÎ≥µÎπÑÏö©", "Ï≤≠ÏÜå": "ÏõêÏÉÅÌöåÎ≥µ",
    "Ï∏µÍ∞ÑÏÜåÏùå": "Í≥µÎèôÏÉùÌôúÌèâÏò®", "ÏòÜÏßëÏÜåÏùå": "Î∞©Ïùå", "Í∞úÌÇ§Ïö∞Í∏∞": "Î∞òÎ†§ÎèôÎ¨ºÌäπÏïΩ",
    "Îã¥Î∞∞": "Ìù°Ïó∞Í∏àÏßÄÌäπÏïΩ",

    # 5. Í∂åÎ¶¨/ÎåÄÌï≠Î†•/ÌôïÏ†ïÏùºÏûê
    "ÌôïÏ†ïÏùºÏûê": "ÌôïÏ†ïÏùºÏûê", "Ï†ÑÏûÖ": "Ï£ºÎØºÎì±Î°ù", "ÎåÄÌï≠Î†•": "ÎåÄÌï≠Î†•",
    "Ïö∞ÏÑ†Î≥ÄÏ†ú": "Ïö∞ÏÑ†Î≥ÄÏ†úÍ∂å", "ÏµúÏö∞ÏÑ†": "ÏµúÏö∞ÏÑ†Î≥ÄÏ†úÍ∂å",
    "Í≤ΩÎß§": "Í≤ΩÎß§Ï†àÏ∞®", "Í≥µÎß§": "Í≥µÎß§Ï†àÏ∞®",
    "Îì±Í∏∞": "Îì±Í∏∞Î∂ÄÎì±Î≥∏", "Îì±Î≥∏": "Îì±Í∏∞Î∂ÄÎì±Î≥∏",
    "Í∑ºÏ†ÄÎãπ": "Í∑ºÏ†ÄÎãπÍ∂å", "Í∞ÄÏïïÎ•ò": "Í∞ÄÏïïÎ•ò", "Í∞ÄÏ≤òÎ∂Ñ": "Í∞ÄÏ≤òÎ∂Ñ",
    "Íπ°ÌÜµÏ†ÑÏÑ∏": "Ï†ÑÏÑ∏ÌîºÌï¥", "ÏÇ¨Í∏∞": "Ï†ÑÏÑ∏ÏÇ¨Í∏∞", "Í≤ΩÎß§ÎÑòÏñ¥Í∞ê": "Í∂åÎ¶¨Î¶¨Ïä§ÌÅ¨",

    # 6. Î∂ÑÏüÅ Ìï¥Í≤∞
    "ÎÇ¥Ïö©Ï¶ùÎ™Ö": "ÎÇ¥Ïö©Ï¶ùÎ™Ö", "ÏÜåÏÜ°": "ÏÜåÏÜ°", "ÎØºÏÇ¨": "ÎØºÏÇ¨ÏÜåÏÜ°",
    "Ï°∞Ï†ïÏúÑ": "Ï£ºÌÉùÏûÑÎåÄÏ∞®Î∂ÑÏüÅÏ°∞Ï†ïÏúÑÏõêÌöå", "ÏÜåÏÜ°ÎßêÍ≥†": "Î∂ÑÏüÅÏ°∞Ï†ï",
    "Î≤ïÏõêÍ∞ÄÍ∏∞Ïã´Ïùå": "Î∂ÑÏüÅÏ°∞Ï†ï",
    "ÏßëÏ£ºÏù∏ÏÇ¨Îßù": "ÏûÑÏ∞®Í∂åÏäπÍ≥Ñ", "ÏûêÏãùÏÉÅÏÜç": "ÏûÑÏ∞®Í∂åÏäπÍ≥Ñ",
    "ÌäπÏïΩ": "ÌäπÏïΩÏÇ¨Ìï≠", "Î∂àÍ≥µÏ†ï": "Í∞ïÌñâÍ∑úÏ†ïÏúÑÎ∞ò", "ÎèÖÏÜåÏ°∞Ìï≠": "Î∂àÎ¶¨ÌïúÏïΩÏ†ï",
    "Ìö®Î†•ÏûàÎÇò": "Î¨¥Ìö®Ïó¨Î∂Ä",
}


# --------------------------------------------------------------------------------------
# Prompts
# --------------------------------------------------------------------------------------
NORMALIZATION_PROMPT: str = """
ÎãπÏã†ÏùÄ Î≤ïÎ•† AI Ï±óÎ¥áÏùò Ï†ÑÏ≤òÎ¶¨ Îã¥ÎãπÏûêÏûÖÎãàÎã§.
ÏïÑÎûò [Ïö©Ïñ¥ ÏÇ¨Ï†Ñ]ÏùÑ ÏóÑÍ≤©Ìûà Ï§ÄÏàòÌïòÏó¨ ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏùÑ 'Î≤ïÎ•† ÌëúÏ§ÄÏñ¥'Î°ú Î≥ÄÌôòÌï¥ Ï£ºÏÑ∏Ïöî.

[ÏàòÌñâ ÏßÄÏπ®]
1. ÏÇ¨Ï†ÑÏóê ÏûàÎäî Îã®Ïñ¥Îäî Î∞òÎìúÏãú Îß§ÌïëÎêú Î≤ïÎ•† Ïö©Ïñ¥Î°ú Î≥ÄÍ≤ΩÌïòÏÑ∏Ïöî.
2. Î≥ÄÍ≤Ω Ï†ÑÏùò Í∏∞Ï°¥ Îã®Ïñ¥ Îí§Ïóê Î≥ÄÍ≤ΩÎêú Îã®Ïñ¥Î•º Í¥ÑÌò∏Î°ú ÎçßÎ∂ôÏó¨, ÏµúÏ¢Ö ÌÖçÏä§Ìä∏Îßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî. ex. "ÏßëÏ£ºÏù∏(ÏûÑÎåÄÏù∏)Ïù¥..."
3. ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ ÏùòÎèÑÎ•º ÏôúÍ≥°ÌïòÍ±∞ÎÇò Ï∂îÍ∞ÄÏ†ÅÏù∏ ÎãµÎ≥Ä, Î≥ÑÎèÑÏùò ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÏßÄ ÎßàÏÑ∏Ïöî. 

[Ïö©Ïñ¥ ÏÇ¨Ï†Ñ]
{dictionary}

ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏: {question}
Î≥ÄÍ≤ΩÎêú ÏßàÎ¨∏:
"""

SYSTEM_PROMPT_WITH_CONTRACT : str = """
ÎãπÏã†ÏùÄ ÏûÑÏ∞®Ïù∏ Í∂åÎ¶¨ Î≥¥Ìò∏ Ï†ÑÎ¨∏ AIÏûÖÎãàÎã§.

[Î™®Îìú: Í≥ÑÏïΩÏÑú(OCR) Î∂ÑÏÑù]
- SECTION 0Ïóê ÏûàÎäî Í≥ÑÏïΩÏÑú/ÌäπÏïΩ Î¨∏Íµ¨Î•º Ïö∞ÏÑ†Ìï©ÎãàÎã§. Ï∂îÏ†ï Í∏àÏßÄ.
- 'Î∂àÎ¶¨Ìïú Ï°∞Ìï≠'ÏùÄ Îã§Ïùå Ï§ë ÌïòÎÇòÎ°ú Î∂ÑÎ•òÌï¥ÏÑú Ï†úÏãúÌïòÏÑ∏Ïöî:
    (1) Î∂àÎ¶¨ ÌäπÏïΩ(ÏûÑÏ∞®Ïù∏ Í∂åÎ¶¨ Ï†úÌïú/ÏùòÎ¨¥ Í∞ÄÏ§ë/Î©¥Ï±Ö) Í∞ÄÎä•ÏÑ± ÌÅº
    (2) Ï£ºÏùò Ï°∞Ìï≠(Î≤ïÏóêÏÑú ÏòàÏ†ïÎêú Í±∞Ï†àÏÇ¨Ïú†/Ï°∞Í±¥ Îì±ÏúºÎ°ú, ÏÇ¨ÏïàÏóê Îî∞Îùº Î∂ÑÏüÅ ÏÜåÏßÄ)
    (3) Ï†ïÎ≥¥ Î∂ÄÏ°±(Î¨∏Íµ¨ÎßåÏúºÎ°ú Î∂àÎ¶¨ Ïó¨Î∂Ä Îã®Ï†ï Ïñ¥Î†§ÏõÄ)

[Ï∂úÏ≤ò Í∑úÏπô]
- Ï∞∏Í≥† Î¨∏ÏÑúÏóê ÏóÜÎäî Î≤ïÎ†πÎ™Ö/Ï°∞Î¨∏/ÌåêÎ°ÄÎ≤àÌò∏Î•º ÎßåÎì§ÏßÄ ÎßàÏÑ∏Ïöî.
- Í∑ºÍ±∞Í∞Ä ÏûàÏúºÎ©¥ "src_title article" ÌòïÌÉúÎ°úÎßå ÌëúÍ∏∞ÌïòÏÑ∏Ïöî. ÏóÜÏúºÎ©¥ "Ï†úÍ≥µÎêú ÏûêÎ£åÏóêÏÑú Í∑ºÍ±∞ Ï°∞Î¨∏ ÌôïÏù∏ Ïïà Îê®"Ïù¥ÎùºÍ≥† Ïì∞ÏÑ∏Ïöî.

[Ï∂úÎ†• ÌòïÏãù]
## üìã Í≥ÑÏïΩÏÑú Ï°∞Ìï≠ Ï†êÍ≤Ä

Í∞Å Ìï≠Î™©ÏùÄ Î∞òÎìúÏãú Í≥ÑÏïΩÏÑú Î¨∏Íµ¨Î•º Î®ºÏ†Ä Ï†úÏãú:
**(Ï°∞Ìï≠Î™Ö/ÌäπÏïΩ) : "ÏõêÎ¨∏ Ïù∏Ïö©"**
- Î∂ÑÎ•ò: (Î∂àÎ¶¨ ÌäπÏïΩ / Ï£ºÏùò Ï°∞Ìï≠ / Ï†ïÎ≥¥ Î∂ÄÏ°±)
- Î¨∏Ï†úÏ†ê(Ïôú ÏûÑÏ∞®Ïù∏ÏóêÍ≤å Î∂àÎ¶¨/Ï£ºÏùòÏù∏ÏßÄ): 1~2Î¨∏Ïû•
- Î≤ïÏ†Å Í∑ºÍ±∞(ÏûàÏùÑ ÎïåÎßå): src_title article
- ÎåÄÏùë(Ïã§Ìñâ Í∞ÄÎä•Ìïú Í≤É 2~4Í∞ú): Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú

ÎßàÏßÄÎßâÏóê:
- Ï∂îÍ∞Ä ÌôïÏù∏ ÏßàÎ¨∏ 2~4Í∞ú(ÌïÑÏöîÌï† ÎïåÎßå)

[Ï∞∏Í≥† Î¨∏ÏÑú]
{context}
"""

SYSTEM_PROMPT_GENERAL: str = """
ÎãπÏã†ÏùÄ ÎåÄÌïúÎØºÍµ≠ ‚ÄòÏ£ºÌÉù ÏûÑÎåÄÏ∞®(Ï†ÑÏõîÏÑ∏)‚Äô Î∂ÑÏïºÏóêÏÑú ÏûÑÏ∞®Ïù∏ Î≥¥Ìò∏Î•º Í∏∞Ï§ÄÏúºÎ°ú Î≤ïÎ•† ÌåêÎã®ÏùÑ Ï†úÍ≥µÌïòÎäî AIÏûÖÎãàÎã§.

ÏïÑÎûò [Ï∞∏Í≥† Î¨∏ÏÑú]Ïóê Í∑ºÍ±∞ÌïòÏó¨ ÌåêÎã®ÌïòÏÑ∏Ïöî. Ï∞∏Í≥† Î¨∏ÏÑúÏóê ÏóÜÎäî ÎÇ¥Ïö©ÏùÄ Ï∂îÏ†ïÌïòÍ±∞ÎÇò ÏùºÎ∞òÎ°†ÏúºÎ°ú Î≥¥ÏôÑÌïòÏßÄ ÎßàÏÑ∏Ïöî.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[ÎãµÎ≥Ä ÏõêÏπô]
- ÏûÑÏ∞®Ïù∏ Î≥¥Ìò∏Î•º ÏúÑÌïú **Í∞ïÌñâÍ∑úÏ†ïÏù¥ ÏûàÏúºÎ©¥ Í≥ÑÏïΩÏÑú Î¨∏Íµ¨Î≥¥Îã§ Î≤ïÎ†πÏùÑ Ïö∞ÏÑ† Ï†ÅÏö©**Ìï©ÎãàÎã§.
- ÏßàÎ¨∏Ïù¥ Í≥ÑÏïΩÍ∏∞Í∞Ñ¬∑Ìá¥Í±∞¬∑Í∞±Ïã†Í≥º Í¥ÄÎ†®Îêú Í≤ΩÏö∞, **‚Äò2ÎÖÑ Î≥¥Ìò∏ ÏõêÏπô(Í∞ïÌñâÍ∑úÏ†ï)‚ÄôÏùÑ ÌåêÎã® Í∏∞Ï§ÄÏúºÎ°ú Î®ºÏ†Ä Í≤ÄÌÜ†**ÌïòÏÑ∏Ïöî.
- Îã®Ï†ïÏù¥ Ïñ¥Î†§Ïö¥ Í≤ΩÏö∞ÏóêÎßå ‚ÄúÏ†úÍ≥µÎêú ÏûêÎ£å Í∏∞Ï§ÄÏóêÏÑúÎäî‚ÄùÏù¥ÎùºÎäî ÌëúÌòÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[ÎãµÎ≥Ä Íµ¨Ï°∞]

A. Ìïú Ï§Ñ Í≤∞Î°†  
- Î∞òÎìúÏãú **ÌåêÎã® + Í∑∏ Í∏∞Ï§Ä(Î≤ïÏùò ÏõêÏπô)**ÏùÑ Ìï®Íªò 1~2Î¨∏Ïû•ÏúºÎ°ú Ï†úÏãúÌïòÏÑ∏Ïöî.
- ‚ÄúÏïÑÎãàÏò§.‚Äù, ‚ÄúÍ∞ÄÎä•Ìï©ÎãàÎã§.‚ÄùÏ≤òÎüº Îã®ÎãµÏúºÎ°ú ÎÅùÎÇ¥ÏßÄ ÎßàÏÑ∏Ïöî.

B. ÏßÄÍ∏à ÎãπÏû• Ìï† Ïùº  
- ÏÇ¨Ïö©ÏûêÍ∞Ä **Í∂åÎ¶¨ ÌñâÏÇ¨ ÎòêÎäî Í±∞Î∂ÄÌï† Ïàò ÏûàÎäî ÌñâÎèô**ÏùÑ Ï§ëÏã¨ÏúºÎ°ú 3~5Í∞ú Ï†úÏãúÌïòÏÑ∏Ïöî.

C. Î≤ïÏ†Å Í∑ºÍ±∞  
- Ï∞∏Í≥† Î¨∏ÏÑúÏóê Î™ÖÏãúÎêú ÌïµÏã¨ Î≤ïÎ†π¬∑Ï°∞Î¨∏ 1~2Í∞úÎßå ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.

D. Ï∂îÍ∞Ä ÌôïÏù∏ (ÌïÑÏöîÌï† ÎïåÎßå)  
- Í≤∞Î°†Ïóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎäî ÏÇ¨Ïã§Í¥ÄÍ≥ÑÎßå ÏßàÎ¨∏ÌïòÏÑ∏Ïöî.

[Ï∞∏Í≥† Î¨∏ÏÑú]
{context}
"""



# --------------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------------
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)  # type: ignore[arg-type]
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "‚Ä¶"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Ï§ëÎ≥µ Ï†úÍ±∞ (chunk_id/id Ïö∞ÏÑ†)."""
    seen: set[str] = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key: Optional[str] = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# --------------------------------------------------------------------------------------
# Tokenizers (for BM25)
# --------------------------------------------------------------------------------------
class Tokenizer(ABC):
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        raise NotImplementedError


class SimpleTokenizer(Tokenizer):
    """Regex Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä (kiwi ÎØ∏ÏÑ§Ïπò Ïãú fallback)."""

    def __init__(self, min_length: int = 1):
        self.min_length = min_length
        self._pattern = re.compile(r"[Í∞Ä-Ìû£a-zA-Z0-9]+")

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = self._pattern.findall(text.lower())
        return [t for t in tokens if len(t) >= self.min_length]


class KiwiTokenizer(Tokenizer):
    """Kiwi ÌòïÌÉúÏÜå Î∂ÑÏÑù Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä."""

    def __init__(self, pos_tags: Optional[Tuple[str, ...]] = None, min_length: int = 1):
        if not KIWI_AVAILABLE:
            raise ImportError("kiwipiepyÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: pip install kiwipiepy")
        self._kiwi = Kiwi()  # type: ignore[call-arg]
        self.pos_tags = pos_tags or ("NNG", "NNP", "VV", "VA", "SL", "SH")
        self.min_length = min_length

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens: List[str] = []
        for t in self._kiwi.tokenize(text):  # type: ignore[union-attr]
            if t.tag in self.pos_tags and len(t.form) >= self.min_length:
                tokens.append(t.form.lower())
        return tokens


# --------------------------------------------------------------------------------------
# BM25 (candidate-level) scoring
# --------------------------------------------------------------------------------------
def _bm25_lite_scores(
    query_tokens: List[str],
    docs_tokens: List[List[str]],
    *,
    k1: float = 1.5,
    b: float = 0.75,
) -> List[float]:
    """BM25Okapi-lite. Candidate-level only (N is small, e.g., 10~80)."""
    N = len(docs_tokens)
    if N == 0:
        return []
    if not query_tokens:
        return [0.0] * N

    doc_lens = [len(toks) for toks in docs_tokens]
    avgdl = (sum(doc_lens) / N) if N else 1.0
    avgdl = max(avgdl, 1e-9)

    df: Dict[str, int] = defaultdict(int)
    for toks in docs_tokens:
        for term in set(toks):
            df[term] += 1

    idf: Dict[str, float] = {}
    for term, dfi in df.items():
        idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

    qtf = Counter(query_tokens)
    scores: List[float] = []
    for toks, dl in zip(docs_tokens, doc_lens):
        tf = Counter(toks)
        score = 0.0
        norm = (1.0 - b) + b * (dl / avgdl)
        for term, qf in qtf.items():
            f = tf.get(term, 0)
            if f <= 0:
                continue
            denom = f + k1 * norm
            if denom <= 0:
                continue
            score += (idf.get(term, 0.0) * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))
        scores.append(float(score))
    return scores


def _compute_bm25_scores(
    query: str,
    docs: List[Document],
    *,
    tokenizer: Tokenizer,
    algorithm: str,
    k1: float,
    b: float,
    max_doc_chars: int,
) -> List[float]:
    """Returns BM25 scores aligned with docs (higher is better)."""
    if not docs:
        return []

    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(d.page_content or "", max_doc_chars)) for d in docs]

    if BM25_AVAILABLE:
        BM25Class = BM25Plus if algorithm == "plus" else BM25Okapi
        if BM25Class is None:
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)
        try:
            bm25 = BM25Class(docs_tokens, k1=k1, b=b)  # type: ignore[misc]
            scores = bm25.get_scores(query_tokens)
            return [float(x) for x in list(scores)]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è rank_bm25 Ïã§Ìå® ‚Üí lite BM25Î°ú Ìè¥Î∞±: {e}")
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)

    return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)



# --------------------------------------------------------------------------------------
# Global BM25 index (optional, true sparse retrieval)
# --------------------------------------------------------------------------------------
class BM25InvertedIndex:
    """Lightweight BM25 inverted index for *global* sparse retrieval.

    - Build once with a corpus (per source: law/rule/case) using build().
    - Search returns top-k Documents with BM25 scores.
    - Uses the same doc identity logic as _dedupe_docs (metadata key_fields first, fallback to content hash).

    Notes:
      * This is NOT required for the default candidate-level BM25 in _dense_sparse_fuse().
      * For large corpora, memory usage grows with the number of unique terms.
    """

    def __init__(
        self,
        *,
        tokenizer: Tokenizer,
        key_fields: Sequence[str] = ("chunk_id", "id"),
        k1: float = 1.5,
        b: float = 0.75,
        max_doc_chars: int = 4000,
    ) -> None:
        self.tokenizer = tokenizer
        self.key_fields = tuple(key_fields)
        self.k1 = float(k1)
        self.b = float(b)
        self.max_doc_chars = int(max_doc_chars)

        self._docs: List[Document] = []
        self._doc_lens: List[int] = []
        self._avgdl: float = 0.0

        # postings[term] = list of (doc_idx, tf)
        self._postings: Dict[str, List[Tuple[int, int]]] = defaultdict(list)
        self._idf: Dict[str, float] = {}
        self._built: bool = False

    def build(self, docs: Sequence[Document]) -> None:
        deduped = _dedupe_docs(docs, self.key_fields)
        self._docs = list(deduped)
        self._postings.clear()
        self._idf.clear()
        self._doc_lens = []

        df: Dict[str, int] = defaultdict(int)

        for idx, d in enumerate(self._docs):
            text = _truncate(d.page_content or "", self.max_doc_chars)
            toks = self.tokenizer.tokenize(text)
            dl = len(toks)
            self._doc_lens.append(dl)

            tf = Counter(toks)
            for term, f in tf.items():
                if not term:
                    continue
                self._postings[term].append((idx, int(f)))
            for term in tf.keys():
                df[term] += 1

        N = len(self._docs)
        self._avgdl = (sum(self._doc_lens) / N) if N else 0.0

        for term, dfi in df.items():
            self._idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

        self._built = True

    def is_built(self) -> bool:
        return self._built and bool(self._docs)

    def search(self, query: str, *, top_k: int = 20) -> List[Tuple[Document, float]]:
        if not self.is_built():
            return []
        q_tokens = self.tokenizer.tokenize(query)
        if not q_tokens:
            return []

        qtf = Counter(q_tokens)
        scores: Dict[int, float] = defaultdict(float)

        avgdl = self._avgdl or 1.0
        k1 = self.k1
        b = self.b

        for term, qf in qtf.items():
            postings = self._postings.get(term)
            if not postings:
                continue
            idf = self._idf.get(term, 0.0)
            if idf == 0.0:
                continue
            for doc_idx, f in postings:
                dl = self._doc_lens[doc_idx] or 0
                norm = (1.0 - b) + b * (dl / avgdl)
                denom = f + k1 * norm
                if denom <= 0:
                    continue
                scores[doc_idx] += (idf * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))

        if not scores:
            return []

        top = heapq.nlargest(int(top_k), scores.items(), key=lambda x: x[1])
        return [(self._docs[i], float(s)) for (i, s) in top]

# --------------------------------------------------------------------------------------
# Hybrid fusion (rank-based default)
# --------------------------------------------------------------------------------------

def _compute_bm25_scores_from_texts(
    query: str,
    texts: List[str],
    *,
    tokenizer: Tokenizer,
    algorithm: str = "okapi",
    k1: float = 1.5,
    b: float = 0.75,
    max_doc_chars: int = 1000,
) -> List[float]:
    """Compute BM25 scores over arbitrary text list (e.g., titles)."""
    if not texts:
        return []
    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(t or "", max_doc_chars)) for t in texts]

    if BM25_AVAILABLE:
        algo = (algorithm or "okapi").lower()
        if algo == "plus" and BM25Plus is not None:
            bm25 = BM25Plus(docs_tokens, k1=k1, b=b)  # type: ignore[arg-type]
        else:
            bm25 = BM25Okapi(docs_tokens, k1=k1, b=b)  # type: ignore[arg-type]
        scores = bm25.get_scores(query_tokens)
        return [float(s) for s in scores]

    # lite fallback
    doc_texts = [" ".join(toks) for toks in docs_tokens]
    return _bm25_lite_scores(query_tokens, doc_texts)


def _rank_fusion_multi(
    ranks_list: List[List[int]],
    *,
    mode: str = "rrf",
    weights: Optional[List[float]] = None,
    rrf_k: int = 60,
) -> List[float]:
    """Fuse multiple rank lists into a single score list (higher is better)."""
    if not ranks_list:
        return []
    n = len(ranks_list[0])
    if any(len(r) != n for r in ranks_list):
        raise ValueError("All rank lists must have the same length.")
    if n == 0:
        return []

    m = len(ranks_list)
    if weights is None:
        weights = [1.0] * m
    if len(weights) != m:
        raise ValueError("weights length must match ranks_list length.")

    mode = (mode or "rrf").lower()
    if mode == "rrf":
        k = max(1, int(rrf_k))
        out = [0.0] * n
        for ch in range(m):
            w = float(weights[ch])
            rr = ranks_list[ch]
            for i in range(n):
                out[i] += w / (k + int(rr[i]))
        return out

    if mode == "rank_sum":
        if n == 1:
            return [float(sum(weights))]

        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)

        out = [0.0] * n
        for ch in range(m):
            w = float(weights[ch])
            rr = ranks_list[ch]
            for i in range(n):
                out[i] += w * to_unit(int(rr[i]))
        return out

    # mode == "weighted": per-channel normalize (1/rank) then weighted sum
    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    per = []
    for ch in range(m):
        rr = ranks_list[ch]
        per.append(minmax([1.0 / max(1, int(r)) for r in rr]))

    out = [0.0] * n
    for i in range(n):
        s = 0.0
        for ch in range(m):
            s += float(weights[ch]) * per[ch][i]
        out[i] = s
    return out


def _rank_fusion(
    dense_ranks: List[int],
    sparse_ranks: List[int],
    *,
    mode: str = "rrf",          # "rrf" | "rank_sum" | "weighted"
    w_dense: float = 0.6,
    w_sparse: float = 0.4,
    rrf_k: int = 60,
) -> List[float]:
    """Return fused scores aligned with docs (higher is better)."""
    n = len(dense_ranks)
    if n == 0:
        return []

    mode = (mode or "rrf").lower()
    if mode == "rrf":
        k = max(1, int(rrf_k))
        return [(w_dense / (k + dense_ranks[i])) + (w_sparse / (k + sparse_ranks[i])) for i in range(n)]

    if mode == "rank_sum":
        if n == 1:
            return [w_dense + w_sparse]

        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)

        return [(w_dense * to_unit(dense_ranks[i])) + (w_sparse * to_unit(sparse_ranks[i])) for i in range(n)]

    # mode == "weighted": min-max normalize (dense=1/rank, sparse=1/rank) then weighted sum
    dense_scores = [1.0 / max(1, r) for r in dense_ranks]
    sparse_scores = [1.0 / max(1, r) for r in sparse_ranks]

    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    d = minmax(dense_scores)
    s = minmax(sparse_scores)
    return [(w_dense * d[i]) + (w_sparse * s[i]) for i in range(n)]


# --------------------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------------------
@dataclass
class RAGConfig:
    # -------- LLMs --------
    normalize_model: str = "solar-pro2"  # Upstage chat model
    generation_model: str = "gpt-4o-mini"  # OpenAI model
    temperature: float = 0.1
    normalize_temperature: float = 0.0

    # -------- Embeddings --------
    embedding_backend: str = "upstage"  # "upstage" | "auto" (auto keeps option for other backends if you inject)
    embedding_model: str = "solar-embedding-1-large-passage"

    # -------- Retrieval sizes --------
    k_law: int = 7
    k_rule: int = 7
    k_case: int = 3
    search_multiplier: int = 4

    # -------- Candidate-level BM25 --------
    enable_bm25: bool = True
    # ============ Sparse retrieval mode (true sparse BM25) ============
    # - candidate: BM25 reorders only dense candidates (fast, no corpus preload)
    # - global: BM25 searches a prebuilt BM25 index over the full corpus you provide via build_global_bm25()
    # - auto: use global if available, else candidate
    sparse_mode: str = "auto"  # "auto" | "candidate" | "global"
    sparse_k_law: Optional[int] = None
    sparse_k_rule: Optional[int] = None
    sparse_k_case: Optional[int] = None

    bm25_algorithm: str = "okapi"  # "okapi" | "plus"
    bm25_k1: float = 1.8
    bm25_b: float = 0.85
    bm25_use_kiwi: bool = True
    bm25_max_doc_chars: int = 4000 # 3000


    # -------- Sparse: BM25-title (metadata) --------
    enable_bm25_title: bool = True
    bm25_title_field: str = "title"
    bm25_title_max_chars: int = 512

    # title BM25 share within sparse weight (0~1). Actual weights:
    #  w_title = hybrid_sparse_weight * hybrid_sparse_title_ratio
    #  w_text  = hybrid_sparse_weight - w_title
    hybrid_sparse_title_ratio: float = 0.6
    # -------- Fusion --------
    hybrid_fusion: str = "rrf"  # "rrf" | "rank_sum" | "weighted"
    hybrid_dense_weight: float = 0.5
    hybrid_sparse_weight: float = 0.5
    rrf_k: int = 60

    # -------- Rerank (optional) --------
    enable_rerank: bool = True
    rerank_threshold: float = 0.2 # 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80 # 80
    rerank_doc_max_chars: int = 2600 # 2600

    # -------- 2-stage case expansion --------
    case_candidate_k: int = 40
    case_expand_top_n: Optional[int] = None  # None => k_case
    case_context_top_k: int = 50

    # -------- Deduping --------
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")

    def __post_init__(self) -> None:
        if not (0 <= self.temperature <= 2):
            raise ValueError("temperatureÎäî 0~2 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
        if not (0 <= self.normalize_temperature <= 2):
            raise ValueError("normalize_temperatureÎäî 0~2 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
        if self.search_multiplier < 1:
            raise ValueError("search_multiplierÎäî 1 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
        if self.case_candidate_k < 1 or self.case_context_top_k < 1:
            raise ValueError("case_* Í∞íÏùÄ 1 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")

        if self.enable_bm25:
            if self.bm25_k1 <= 0:
                raise ValueError("bm25_k1ÏùÄ 0Î≥¥Îã§ Ïª§Ïïº Ìï©ÎãàÎã§.")
            if not (0 <= self.bm25_b <= 1):
                raise ValueError("bm25_bÎäî 0~1 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
            if self.bm25_algorithm not in ("okapi", "plus"):
                raise ValueError('bm25_algorithmÏùÄ "okapi" ÎòêÎäî "plus" Ïù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.')


            if self.enable_bm25_title:
                if not (0.0 <= float(self.hybrid_sparse_title_ratio) <= 1.0):
                    raise ValueError("hybrid_sparse_title_ratioÎäî 0~1 ÏÇ¨Ïù¥Ïó¨Ïïº Ìï©ÎãàÎã§.")
                if self.bm25_title_max_chars < 32:
                    raise ValueError("bm25_title_max_charsÎäî 32 Ïù¥ÏÉÅÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.")
        if self.hybrid_fusion not in ("rrf", "rank_sum", "weighted"):
            raise ValueError('hybrid_fusionÏùÄ "rrf" | "rank_sum" | "weighted" Ï§ë ÌïòÎÇòÏó¨Ïïº Ìï©ÎãàÎã§.')
        if self.rrf_k < 1:
            raise ValueError("rrf_kÎäî 1 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
        if self.hybrid_dense_weight < 0 or self.hybrid_sparse_weight < 0:
            raise ValueError("hybrid_*_weightÎäî 0 Ïù¥ÏÉÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.")
        if self.hybrid_dense_weight == 0 and self.hybrid_sparse_weight == 0:
            raise ValueError("hybrid_dense_weightÏôÄ hybrid_sparse_weightÍ∞Ä Î™®Îëê 0Ïùº ÏàòÎäî ÏóÜÏäµÎãàÎã§.")


# --------------------------------------------------------------------------------------
# Pipeline
# --------------------------------------------------------------------------------------
class RAGPipeline:
    """Unified Hybrid RAG pipeline (no web framework integration)."""

    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        upstage_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
        embedding: Optional[object] = None,
        normalize_llm: Optional[object] = None,
        generation_llm: Optional[object] = None,
        cohere_client: Optional[object] = None,
        tokenizer: Optional[Tokenizer] = None,
    ) -> None:
        self.config = config or RAGConfig()

        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._upstage_api_key = upstage_api_key or os.getenv("UPSTAGE_API_KEY")
        self._openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")

        if not self._pc_api_key:
            raise ValueError("PINECONE_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. pc_api_key Ïù∏Ïûê ÎòêÎäî ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.")

        # ---- Embedding (dense) ----
        if embedding is not None:
            self._embedding = embedding
        else:
            backend = (self.config.embedding_backend or "auto").lower()
            if backend in ("auto", "upstage"):
                if not UPSTAGE_AVAILABLE:
                    raise ImportError("langchain_upstageÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§: pip install langchain-upstage")
                if not self._upstage_api_key:
                    raise ValueError("UPSTAGE_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§ (UpstageEmbeddings).")
                os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
                self._embedding = UpstageEmbeddings(model=self.config.embedding_model)  # type: ignore[call-arg]
            else:
                raise ValueError(
                    "ÌòÑÏû¨ unified Î™®ÎìàÏùÄ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Upstage(SOLAR) embeddingÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. "
                    "Îã§Î•∏ embeddingÏùÑ Ïì∞Î†§Î©¥ embedding Í∞ùÏ≤¥Î•º ÏßÅÏ†ë Ï£ºÏûÖÌïòÏÑ∏Ïöî."
                )

        # ---- Pinecone vector stores ----
        logger.info("üîó Pinecone 3Ï§ë Ïù∏Îç±Ïä§ Ïó∞Í≤∞ Ï§ë...")
        self._law_store = PineconeVectorStore(
            index_name=INDEX_NAMES["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=INDEX_NAMES["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=INDEX_NAMES["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        logger.info("‚úÖ [Law / Rule / Case] 3Í∞ú Ïù∏Îç±Ïä§ Î°úÎìú ÏôÑÎ£å!")

        # ---- LLMs ----
        # normalize: Upstage solar-pro2
        if normalize_llm is not None:
            self._normalize_llm = normalize_llm
        else:
            if not UPSTAGE_AVAILABLE or ChatUpstage is None:
                raise ImportError("normalize_queryÏóê Upstage chatÏùÑ Ïì∞Î†§Î©¥ langchain_upstageÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            if not self._upstage_api_key:
                raise ValueError("normalize_queryÏóê UPSTAGE_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
            self._normalize_llm = ChatUpstage(
                model=self.config.normalize_model,
                temperature=self.config.normalize_temperature,
            )

        # generation: OpenAI gpt-4o-mini
        if generation_llm is not None:
            self._generation_llm = generation_llm
        else:
            if not OPENAI_AVAILABLE or ChatOpenAI is None:
                raise ImportError("generate_answerÏóê OpenAI chatÏùÑ Ïì∞Î†§Î©¥ langchain_openaiÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            if not self._openai_api_key:
                raise ValueError("generate_answerÏóê OPENAI_API_KEYÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
            os.environ.setdefault("OPENAI_API_KEY", self._openai_api_key)
            self._generation_llm = ChatOpenAI(
                model=self.config.generation_model,
                temperature=self.config.temperature,
            )

        # ---- Tokenizer (for BM25) ----
        if tokenizer is not None:
            self._tokenizer = tokenizer
        else:
            if self.config.bm25_use_kiwi and KIWI_AVAILABLE:
                try:
                    self._tokenizer = KiwiTokenizer()
                    logger.info("‚úÖ Kiwi ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö© (BM25)")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Kiwi ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú Ïã§Ìå®, SimpleTokenizerÎ°ú ÎåÄÏ≤¥: {e}")
                    self._tokenizer = SimpleTokenizer()
            else:
                logger.info("‚ÑπÔ∏è SimpleTokenizer ÏÇ¨Ïö© (BM25)")
                self._tokenizer = SimpleTokenizer()

        # ---- Cohere rerank client (optional) ----
        self._cohere_client = None
        if self.config.enable_rerank:
            if not COHERE_AVAILABLE:
                logger.warning("‚ö†Ô∏è cohere Ìå®ÌÇ§ÏßÄÍ∞Ä ÏóÜÏñ¥ rerankÎ•º ÎπÑÌôúÏÑ±ÌôîÌï©ÎãàÎã§.")
            elif not self._cohere_api_key:
                logger.warning("‚ö†Ô∏è COHERE_API_KEYÍ∞Ä ÏóÜÏñ¥ rerankÎ•º ÎπÑÌôúÏÑ±ÌôîÌï©ÎãàÎã§.")
            else:
                self._cohere_client = cohere_client or cohere.Client(self._cohere_api_key)  # type: ignore[attr-defined]

        # ---- Global BM25 indices (optional, for true sparse retrieval) ----
        self._global_bm25: Dict[str, BM25InvertedIndex] = {}

    # ----------------------------
    # Stores
    # ----------------------------
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store

    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store

    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store

    # ----------------------------
    # Query normalization
    # ----------------------------
    def normalize_query(self, user_query: str) -> str:
        """Upstage SOLAR Pro2Î°ú ÏßàÎ¨∏ÏùÑ Î≤ïÎ•† Ïö©Ïñ¥Î°ú ÌëúÏ§ÄÌôî."""
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()

        try:
            normalized = chain.invoke({"dictionary": KEYWORD_DICT, "question": user_query})
            out = str(normalized).strip()
            return out or user_query
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå® (ÏõêÎ≥∏ ÏÇ¨Ïö©): {e}")
            return user_query

    # ----------------------------
    # Case expansion
    # ----------------------------
    def get_full_case_context(self, case_no: str) -> str:
        """ÌäπÏ†ï ÏÇ¨Í±¥Î≤àÌò∏(case_no)Ïùò ÌåêÎ°Ä Ï†ÑÎ¨∏(Ï≤≠ÌÅ¨Îì§ÏùÑ Ïó∞Í≤∞)ÏùÑ Í∞ÄÏ†∏Ïò¥."""
        try:
            results = self.case_store.similarity_search(
                query="ÌåêÎ°Ä Ï†ÑÎ¨∏ Í≤ÄÏÉâ",
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            sorted_docs = sorted(results, key=lambda x: str((x.metadata or {}).get("chunk_id", "")))
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌåêÎ°Ä Ï†ÑÎ¨∏ Î°úÎî© Ïã§Ìå® ({case_no}): {e}")
            return ""

    # ----------------------------
    # Internal helpers
    # ----------------------------
    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs

    def build_global_bm25(
        self,
        *,
        law_docs: Optional[Sequence[Document]] = None,
        rule_docs: Optional[Sequence[Document]] = None,
        case_docs: Optional[Sequence[Document]] = None,
    ) -> None:
        """(Optional) Build *global* BM25 indices for true sparse retrieval.

        Provide the same corpus you indexed into Pinecone (or a superset). Metadata should contain
        stable identifiers (e.g., chunk_id) to enable deduplication/merge with dense results.
        """
        cfg = self.config

        def _build(name: str, docs: Optional[Sequence[Document]]) -> None:
            if not docs:
                return
            idx = BM25InvertedIndex(
                tokenizer=self._tokenizer,
                key_fields=cfg.dedupe_key_fields,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_max_doc_chars,
            )
            idx.build(docs)
            if idx.is_built():
                self._global_bm25[name] = idx
                logger.info(f"‚úÖ Global BM25 index built for '{name}' (docs={len(docs)})")

        _build("law", law_docs)
        _build("rule", rule_docs)
        _build("case", case_docs)

    def _hybrid_fuse_per_source(self, source: str, query: str, dense_docs: List[Document]) -> List[Document]:
        """Choose hybrid strategy per source (candidate-level vs global BM25)."""
        cfg = self.config
        mode = (cfg.sparse_mode or "auto").lower()

        use_global = False
        if mode == "global":
            use_global = True
        elif mode == "auto":
            use_global = source in self._global_bm25 and self._global_bm25[source].is_built()

        if not use_global:
            return self._dense_sparse_fuse(query, dense_docs)

        if source == "law":
            sk = cfg.sparse_k_law or (cfg.k_law * max(1, cfg.search_multiplier))
        elif source == "rule":
            sk = cfg.sparse_k_rule or (cfg.k_rule * max(1, cfg.search_multiplier))
        else:
            sk = cfg.sparse_k_case or max(cfg.case_candidate_k, cfg.k_case * max(1, cfg.search_multiplier))

        sparse_pairs = self._global_bm25[source].search(query, top_k=int(sk))
        sparse_docs: List[Document] = []
        for rank, (d, score) in enumerate(sparse_pairs, start=1):
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__bm25_score"] = float(score)
            d.metadata["__bm25_rank"] = int(rank)
            sparse_docs.append(d)

        dense_docs = self._attach_source(dense_docs, source)
        sparse_docs = self._attach_source(sparse_docs, source)

        merged = _dedupe_docs(list(dense_docs) + list(sparse_docs), cfg.dedupe_key_fields)
        if len(merged) <= 1:
            return merged

        def _key(d: Document) -> str:
            md = d.metadata or {}
            for f in cfg.dedupe_key_fields:
                v = md.get(f)
                if v:
                    return f"{f}:{v}"
            return f"content:{hash(d.page_content)}"

        dense_rank_map: Dict[str, int] = {}
        sparse_rank_map: Dict[str, int] = {}

        for i, d in enumerate(dense_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__dense_rank", i))
            dense_rank_map[k] = min(dense_rank_map.get(k, r), r)

        for i, d in enumerate(sparse_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__bm25_rank", i))
            sparse_rank_map[k] = min(sparse_rank_map.get(k, r), r)

        max_dense = max(dense_rank_map.values()) if dense_rank_map else 1000
        max_sparse = max(sparse_rank_map.values()) if sparse_rank_map else 1000
        fill_dense = max_dense + 1000
        fill_sparse = max_sparse + 1000

        dense_ranks: List[int] = []
        sparse_ranks: List[int] = []
        for d in merged:
            k = _key(d)
            dense_ranks.append(int(dense_rank_map.get(k, fill_dense)))
            sparse_ranks.append(int(sparse_rank_map.get(k, fill_sparse)))

        fused = _rank_fusion(
            dense_ranks,
            sparse_ranks,
            mode=cfg.hybrid_fusion,
            w_dense=cfg.hybrid_dense_weight,
            w_sparse=cfg.hybrid_sparse_weight,
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(len(merged)), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, idx in enumerate(order, start=1):
            d = merged[idx]
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__hybrid_score"] = float(fused[idx])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _search_dense_candidates(self, store: PineconeVectorStore, query: str, k: int) -> List[Document]:
        """Dense retrieval via PineconeVectorStore."""
        try:
            pairs = store.similarity_search_with_score(query, k=k)  # type: ignore[attr-defined]
            docs: List[Document] = []
            for rank, (doc, score) in enumerate(pairs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_score"] = float(score)
                doc.metadata["__dense_rank"] = int(rank)
                docs.append(doc)
            return docs
        except Exception:
            docs = store.similarity_search(query, k=k)
            for rank, doc in enumerate(docs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_rank"] = int(rank)
            return docs

    def _dense_sparse_fuse(self, query: str, docs: List[Document]) -> List[Document]:

        """Dense candidatesÎ•º (BM25-text + BM25-title)Î°ú Ï†êÏàòÌôîÌïòÍ≥† 3Ï±ÑÎÑê RRFÎ°ú Í≤∞Ìï©."""
        cfg = self.config
        if not cfg.enable_bm25:
            return docs

        docs = _dedupe_docs(docs, cfg.dedupe_key_fields)
        n = len(docs)
        if n <= 1:
            return docs

        # --- dense ranks ---
        dense_ranks: List[int] = []
        for i, d in enumerate(docs, start=1):
            if d.metadata is None:
                d.metadata = {}
            dense_ranks.append(int(d.metadata.get("__dense_rank", i)))

        # --- sparse: BM25 on text ---
        bm25_text_scores = _compute_bm25_scores(
            query,
            docs,
            tokenizer=self._tokenizer,
            algorithm=cfg.bm25_algorithm,
            k1=cfg.bm25_k1,
            b=cfg.bm25_b,
            max_doc_chars=cfg.bm25_max_doc_chars,
        )
        order_text = sorted(range(n), key=lambda i: (-bm25_text_scores[i], dense_ranks[i]))
        bm25_text_ranks = [0] * n
        for r, idx in enumerate(order_text, start=1):
            bm25_text_ranks[idx] = r

        # --- sparse: BM25 on title (metadata field) ---
        bm25_title_scores: List[float] = [0.0] * n
        bm25_title_ranks: List[int] = [n + 1000] * n
        if cfg.enable_bm25_title:
            titles = [str((d.metadata or {}).get(cfg.bm25_title_field, "") or "") for d in docs]
            bm25_title_scores = _compute_bm25_scores_from_texts(
                query,
                titles,
                tokenizer=self._tokenizer,
                algorithm=cfg.bm25_algorithm,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_title_max_chars,
            )
            order_title = sorted(range(n), key=lambda i: (-bm25_title_scores[i], dense_ranks[i]))
            bm25_title_ranks = [0] * n
            for r, idx in enumerate(order_title, start=1):
                bm25_title_ranks[idx] = r

        # --- attach metadata ---
        for i, d in enumerate(docs):
            d.metadata["__bm25_text_score"] = float(bm25_text_scores[i])
            d.metadata["__bm25_text_rank"] = int(bm25_text_ranks[i])
            # legacy compatibility
            d.metadata["__bm25_score"] = float(bm25_text_scores[i])
            d.metadata["__bm25_rank"] = int(bm25_text_ranks[i])

            d.metadata["__bm25_title_score"] = float(bm25_title_scores[i])
            d.metadata["__bm25_title_rank"] = int(bm25_title_ranks[i])

        # --- 3-channel fusion (dense + bm25(text) + bm25(title)) ---
        w_dense = float(cfg.hybrid_dense_weight)
        w_title = float(cfg.hybrid_sparse_weight) * float(cfg.hybrid_sparse_title_ratio) if cfg.enable_bm25_title else 0.0
        w_text = float(cfg.hybrid_sparse_weight) - w_title

        fused = _rank_fusion_multi(
            [dense_ranks, bm25_text_ranks, bm25_title_ranks],
            mode=cfg.hybrid_fusion,
            weights=[w_dense, w_text, w_title],
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(n), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, i in enumerate(order, start=1):
            d = docs[i]
            d.metadata["__hybrid_score"] = float(fused[i])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _rerank(self, query: str, docs: List[Document]) -> Optional[List[Tuple[int, float]]]:
        """Cohere rerank Ïã§Ìñâ. Ïã§Ìå®/ÎπÑÌôúÏÑ± Ïãú None."""
        if not self._cohere_client:
            return None

        cfg = self.config
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]
        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            return [(r.index, float(r.relevance_score)) for r in rerank_results.results]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Rerank Ïã§Ìå® (skip): {e}")
            return None

    def _cap_for_rerank(self, law: List[Document], rule: List[Document], case: List[Document]) -> List[Document]:
        """rerank ÏûÖÎ†• Î¨∏ÏÑú Ïàò Ï†úÌïú: law/rule Ïö∞ÏÑ†, caseÎäî ÎÇ®Îäî Ïä¨Î°ØÎßå."""
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)

        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[: cfg.rerank_max_documents]
        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]

    # ----------------------------
    # Retrieval: triple index + hybrid + optional rerank + 2-stage case expansion
    # ----------------------------
    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        cfg = self.config
        mult = cfg.search_multiplier

        logger.info(f"üîç [Hybrid Retrieval] query='{query}'")

        docs_law = self._attach_source(
            self._search_dense_candidates(self.law_store, query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self._search_dense_candidates(self.rule_store, query, k=cfg.k_rule * mult),
            "rule",
        )
        docs_case_chunks = self._attach_source(
            self._search_dense_candidates(self.case_store, query, k=cfg.case_candidate_k),
            "case",
        )

        # candidate-level BM25 fusion per index
        docs_law = self._hybrid_fuse_per_source("law", query, docs_law)
        docs_rule = self._hybrid_fuse_per_source("rule", query, docs_rule)
        docs_case_chunks = self._hybrid_fuse_per_source("case", query, docs_case_chunks)

        combined_for_rerank = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)

        ranked = self._rerank(query, combined_for_rerank) if cfg.enable_rerank else None
        if ranked:
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]
            selected_docs = [combined_for_rerank[i] for (i, _s) in filtered]
            logger.info(f"üìå Rerank selected={len(selected_docs)} (threshold={cfg.rerank_threshold})")
        else:
            selected_docs = combined_for_rerank

        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)

        law_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "rule"]
        case_ranked_chunks = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "case"]

        final_law = law_ranked[: cfg.k_law]
        final_rule = rule_ranked[: cfg.k_rule]

        top_n = cfg.case_expand_top_n if cfg.case_expand_top_n is not None else cfg.k_case
        seen_case_no: set[str] = set()
        chosen_case_docs: List[Document] = []
        for d in case_ranked_chunks:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no or str(case_no) in seen_case_no:
                continue
            seen_case_no.add(str(case_no))
            chosen_case_docs.append(d)
            if len(chosen_case_docs) >= top_n:
                break

        expanded_cases: List[Document] = []
        for d in chosen_case_docs:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                expanded_cases.append(d)
                continue

            title = (d.metadata or {}).get("title") or (d.metadata or {}).get("case_name") or str(case_no)
            md = dict(d.metadata or {})
            md["__expanded"] = True
            expanded_cases.append(
                Document(
                    page_content=f"[ÌåêÎ°Ä Ï†ÑÎ¨∏: {title}]\n{full_text}",
                    metadata=md,
                )
            )

        final_case = expanded_cases[: cfg.k_case]

        final_docs = final_law + final_rule + final_case
        final_docs = sorted(final_docs, key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99))
        return final_docs

    # ----------------------------
    # Context formatting (Í≤∞Ìï© Î≤ÑÏ†Ñ: SECTION Íµ¨Î∂Ñ + Í∞ÑÍ≤∞Ìïú Ìè¨Îß∑)
    # ----------------------------
    @staticmethod
    def format_reference_line(doc: Document, *, text_max_chars: int = 2500) -> str:
        """
        Îã®Ïùº Î¨∏ÏÑúÎ•º '{src_title} {article} - {text}' ÌòïÏãùÏúºÎ°ú Ìè¨Îß∑.
        ÌåêÎ°ÄÏùò Í≤ΩÏö∞ ÏÇ¨Í±¥Î≤àÌò∏Î•º article ÏúÑÏπòÏóê ÌëúÏãú.
        """
        md = doc.metadata or {}
        
        # src_title Ï∂îÏ∂ú (Î≤ïÎ†πÎ™Ö/ÌåêÎ°ÄÎ™Ö Îì±)
        src_title = str(md.get("src_title") or "").strip()
        if not src_title:
            src_title = str(
                md.get("source") or md.get("src") or md.get("file") or 
                md.get("title") or md.get("__source_index") or "ÏûêÎ£å"
            ).strip()
        
        # article Ï∂îÏ∂ú (Ï°∞Î¨∏Î≤àÌò∏ ÎòêÎäî ÏÇ¨Í±¥Î≤àÌò∏)
        article = str(md.get("article") or "").strip()
        if not article:
            # ÌåêÎ°ÄÏù∏ Í≤ΩÏö∞ case_no ÏÇ¨Ïö©
            case_no = str(md.get("case_no") or "").strip()
            if case_no:
                article = case_no
        
        # Î≥∏Î¨∏ ÌÖçÏä§Ìä∏ (Ï§ÑÎ∞îÍøà Ï†úÍ±∞, Í∏∏Ïù¥ Ï†úÌïú)
        text = _truncate(
            (doc.page_content or "").strip().replace("\n", " "), 
            int(text_max_chars)
        ).strip()
        
        # Ï°∞Ìï©: {src_title} {article} - {text}
        left = " ".join([x for x in [src_title, article] if x]).strip()
        if left:
            return f"{left} - {text}".strip()
        return f"- {text}".strip()

    def format_context_with_hierarchy(self, docs: List[Document]) -> str:
        """
        Î¨∏ÏÑúÎ•º Î≤ïÏ†Å ÏúÑÍ≥Ñ(SECTION 1/2/3)Î°ú Íµ¨Î∂ÑÌïòÍ≥†,
        Í∞Å Ìï≠Î™©ÏùÑ '{src_title} {article} - {text}' ÌòïÏãùÏúºÎ°ú Ìè¨Îß∑.
        
        SECTION Î∂ÑÎ•ò Í∏∞Ï§Ä (priority Í∞í):
        - SECTION 1 (ÌïµÏã¨ Î≤ïÎ†π): priority 1, 2, 4, 5
        - SECTION 2 (Í¥ÄÎ†® Í∑úÏ†ï): priority 3, 6, 7, 8, 11
        - SECTION 3 (ÌåêÎ°Ä/ÏÇ¨Î°Ä): Í∑∏ Ïô∏ (Ï£ºÎ°ú case Ïù∏Îç±Ïä§)
        """
        cfg = self.config
        
        section_1_law: List[str] = []
        section_2_rule: List[str] = []
        section_3_case: List[str] = []

        for doc in docs:
            md = doc.metadata or {}
            p = _safe_int(md.get("priority", 99), 99)
            
            # fixed.py Ïä§ÌÉÄÏùºÏùò Í∞ÑÍ≤∞Ìïú Ìè¨Îß∑ Ï†ÅÏö©
            entry = self.format_reference_line(doc, text_max_chars=cfg.rerank_doc_max_chars)

            # priorityÏóê Îî∞Î•∏ SECTION Î∂ÑÎ•ò
            if p in (1, 2, 4, 5):
                section_1_law.append(f"- {entry}")
            elif p in (3, 6, 7, 8, 11):
                section_2_rule.append(f"- {entry}")
            else:
                section_3_case.append(f"- {entry}")

        # SECTIONÎ≥ÑÎ°ú Ï°∞Ìï©
        parts: List[str] = []
        if section_1_law:
            parts.append(
                "## [SECTION 1: ÌïµÏã¨ Î≤ïÎ†π (ÏµúÏö∞ÏÑ† Î≤ïÏ†Å Í∑ºÍ±∞)]\n" + 
                "\n".join(section_1_law)
            )
        if section_2_rule:
            parts.append(
                "## [SECTION 2: Í¥ÄÎ†® Í∑úÏ†ï Î∞è Ï†àÏ∞® (ÏÑ∏Î∂Ä Í∏∞Ï§Ä)]\n" + 
                "\n".join(section_2_rule)
            )
        if section_3_case:
            parts.append(
                "## [SECTION 3: ÌåêÎ°Ä Î∞è Ìï¥ÏÑù ÏÇ¨Î°Ä (Ï†ÅÏö© ÏòàÏãú)]\n" + 
                "\n".join(section_3_case)
            )

        return "\n\n".join(parts).strip()

    def format_context(self, docs: List[Document]) -> str:
        """
        SYSTEM_PROMPTÏùò {context}Ïóê Îì§Ïñ¥Í∞à Î≥∏Î¨∏.
        SECTION Íµ¨Î∂ÑÏùÑ Ìè¨Ìï®Ìïú Í≥ÑÏ∏µÏ†Å Ìè¨Îß∑ ÏÇ¨Ïö©.
        """
        return self.format_context_with_hierarchy(docs)

    def format_references(self, docs: List[Document]) -> List[str]:
        """UI ÌëúÏãúÏö© Ï∞∏Ï°∞ Î™©Î°ù (src_title + articleÎßå, ÌÖçÏä§Ìä∏ ÏóÜÏù¥)."""
        return [self.format_reference_short(d) for d in docs]

    @staticmethod
    def format_reference_short(doc: Document) -> str:
        """
        Îã®Ïùº Î¨∏ÏÑúÎ•º '{src_title} {article}' ÌòïÏãùÏúºÎ°ú Ìè¨Îß∑ (ÌÖçÏä§Ìä∏ Ï†úÏô∏).
        Ïòà: 'Ï£ºÌÉùÏûÑÎåÄÏ∞®Î≥¥Ìò∏Î≤ï(Î≤ïÎ•†)(Ï†ú21065Ìò∏)(20260102) Ï†ú3Ï°∞Ïùò7(ÏûÑÎåÄÏù∏Ïùò Ï†ïÎ≥¥ Ï†úÏãú ÏùòÎ¨¥)'
        """
        md = doc.metadata or {}
        
        # src_title Ï∂îÏ∂ú (Î≤ïÎ†πÎ™Ö/ÌåêÎ°ÄÎ™Ö Îì±)
        src_title = str(md.get("src_title") or "").strip()
        if not src_title:
            src_title = str(
                md.get("source") or md.get("src") or md.get("file") or 
                md.get("title") or md.get("__source_index") or "ÏûêÎ£å"
            ).strip()
        
        # article Ï∂îÏ∂ú (Ï°∞Î¨∏Î≤àÌò∏ ÎòêÎäî ÏÇ¨Í±¥Î≤àÌò∏)
        article = str(md.get("article") or "").strip()
        if not article:
            # ÌåêÎ°ÄÏù∏ Í≤ΩÏö∞ case_no ÏÇ¨Ïö©
            case_no = str(md.get("case_no") or "").strip()
            if case_no:
                article = case_no
        
        # Ï°∞Ìï©: {src_title} {article}
        return " ".join([x for x in [src_title, article] if x]).strip() or "ÏûêÎ£å"

    # ----------------------------
    # User-provided contract context (OCR)
    # ----------------------------
    @staticmethod
    def _format_user_contract_context(contract_text: Optional[str], *, max_chars: int = 12000) -> str:
        """ÏÇ¨Ïö©ÏûêÍ∞Ä ÏóÖÎ°úÎìúÌïú Í≥ÑÏïΩÏÑú(OCR) ÌÖçÏä§Ìä∏Î•º Ïª®ÌÖçÏä§Ìä∏Ïóê ÏïàÏ†ÑÌïòÍ≤å ÏÇΩÏûÖ.
        - ÎÑàÎ¨¥ Í∏∏Î©¥ ÏûòÎùºÏÑú(Ï†ïÎ≥¥ Í≥ºÎã§/ÌÜ†ÌÅ∞ Ìè≠Î∞ú Î∞©ÏßÄ) ÏïûÎ∂ÄÎ∂Ñ Ï§ëÏã¨ÏúºÎ°ú Ìè¨Ìï®Ìï©ÎãàÎã§.
        """
        if not contract_text:
            return ""
        t = str(contract_text).strip()
        if not t:
            return ""
        if len(t) > max_chars:
            t = t[: max_chars - 1] + "‚Ä¶"
        return "## [SECTION 0: ÏÇ¨Ïö©Ïûê Í≥ÑÏïΩÏÑú OCR (ÏµúÏö∞ÏÑ† Ï∞∏Í≥†)]\n" + t.strip()

    # ----------------------------
    # Answer generation
    # ----------------------------
    def answer_with_trace(
        self,
        user_input: str,
        *,
        skip_normalization: bool = False,
        extra_context: Optional[str] = None,
        use_contract_mode: bool = False,  # ‚úÖ Explicit flag for contract mode
    ) -> Dict[str, Any]:
        """UIÏö©: normalized_query, references, answerÎ•º Ìï®Íªò Î∞òÌôò.

        
        Args:
            user_input: ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏
            skip_normalization: TrueÏù¥Î©¥ ÏßàÎ¨∏ ÌëúÏ§ÄÌôî Í±¥ÎÑàÎõ∞Í∏∞
            extra_context: ÏÇ¨Ïö©ÏûêÍ∞Ä ÏóÖÎ°úÎìúÌïú Í≥ÑÏïΩÏÑú OCR ÌÖçÏä§Ìä∏ (SECTION 0Î°ú ÏÇΩÏûÖÎê®)
            use_contract_mode: TrueÏù¥Î©¥ Í≥ÑÏïΩÏÑú Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ ÏÇ¨Ïö© (ÌååÏùº ÏóÖÎ°úÎìú ÏãúÏóêÎßå True)
        """
        normalized_query = user_input if skip_normalization else self.normalize_query(user_input)
        if not skip_normalization:
            logger.info(f"üîÑ ÌëúÏ§ÄÌôîÎêú ÏßàÎ¨∏: {normalized_query}")

        docs = self.triple_hybrid_retrieval(normalized_query)
        if not docs:
            return {
                "normalized_query": normalized_query,
                "references": [],
                "answer": "Ï£ÑÏÜ°Ìï©ÎãàÎã§. Í¥ÄÎ†® Î≤ïÎ†πÏù¥ÎÇò ÌåêÎ°ÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.",
                "docs": [],
            }

        # SECTION Íµ¨Î∂ÑÏù¥ Ìè¨Ìï®Îêú context ÏÉùÏÑ±
        context_main = self.format_context(docs)
        
        # OCR Í≥ÑÏïΩÏÑú Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏûàÏúºÎ©¥ SECTION 0ÏúºÎ°ú Îß® ÏïûÏóê Ï∂îÍ∞Ä
        context_contract = self._format_user_contract_context(extra_context)
        context = (context_contract + "\n\n" + context_main).strip() if context_contract else context_main

        # ‚úÖ ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Î∂ÑÍ∏∞: use_contract_mode ÌîåÎûòÍ∑∏ ÏÇ¨Ïö©
        # - use_contract_mode=True: ÌååÏùºÏù¥ Ïù¥Î≤à ÏöîÏ≤≠ÏóêÏÑú ÏóÖÎ°úÎìúÎê® ‚Üí Í≥ÑÏïΩÏÑú Î∂ÑÏÑù Î™®Îìú
        # - use_contract_mode=False: ÏùºÎ∞ò ÏßàÎ¨∏ ÎòêÎäî follow-up ÏßàÎ¨∏ ‚Üí ÏùºÎ∞ò Î™®Îìú
        system_prompt_to_use = SYSTEM_PROMPT_WITH_CONTRACT if use_contract_mode else SYSTEM_PROMPT_GENERAL
        logger.info(f"üìù Using prompt mode: {'CONTRACT' if use_contract_mode else 'GENERAL'}")

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt_to_use),
                ("human", "{question}"),
            ]
        )
        chain = prompt | self._generation_llm | StrOutputParser()

        logger.info("ü§ñ ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë...")
        try:
            answer = str(chain.invoke({"context": context, "question": normalized_query})).strip()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®: {e}")
            answer = "Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."

        return {
            "normalized_query": normalized_query,
            "references": self.format_references(docs),
            "answer": answer,
            "docs": docs,
        }



    def generate_answer(
        self,
        user_input: str,
        *,
        skip_normalization: bool = False,
        extra_context: Optional[str] = None,
    ) -> str:
        """Ìò∏ÌôòÏö©: ÎãµÎ≥Ä Î¨∏ÏûêÏó¥Îßå Î∞òÌôò.
        
        Args:
            user_input: ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏
            skip_normalization: TrueÏù¥Î©¥ ÏßàÎ¨∏ ÌëúÏ§ÄÌôî Í±¥ÎÑàÎõ∞Í∏∞
            extra_context: ÏÇ¨Ïö©ÏûêÍ∞Ä ÏóÖÎ°úÎìúÌïú Í≥ÑÏïΩÏÑú OCR ÌÖçÏä§Ìä∏ (SECTION 0Î°ú ÏÇΩÏûÖÎê®)
        """
        return str(
            self.answer_with_trace(
                user_input,
                skip_normalization=skip_normalization,
                extra_context=extra_context,
            ).get("answer", "")
        ).strip()


def create_pipeline(**kwargs: Any) -> RAGPipeline:
    """Convenience helper."""
    return RAGPipeline(**kwargs)


__all__ = [
    "RAGConfig",
    "RAGPipeline",
    "create_pipeline",
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
]
