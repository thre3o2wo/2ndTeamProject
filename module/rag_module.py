"""
rag_module.py

ì£¼íƒ ì„ëŒ€ì°¨ ë²•ë¥  ìƒë‹´ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG ëª¨ë“ˆì…ë‹ˆë‹¤
Dense ê²€ìƒ‰ê³¼ Sparse ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ ì •í™•í•œ ë²•ë¥  ë¬¸ì„œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤

ì‚¬ìš© ëª¨ë¸
ì§ˆë¬¸ í‘œì¤€í™”   Upstage SOLAR Pro2
ë‹µë³€ ìƒì„±     OpenAI GPT-4o-mini
ì„ë² ë”©        Upstage SOLAR embedding

ê²€ìƒ‰ ë°©ì‹
Dense ê²€ìƒ‰   Pinecone ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤
Sparse ê²€ìƒ‰  BM25 ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
Fusion       RRF ë°©ì‹ìœ¼ë¡œ ë‘ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ ê²°í•©í•©ë‹ˆë‹¤

í•„ìš” í™˜ê²½ë³€ìˆ˜
PINECONE_API_KEY   Pinecone ë²¡í„° DB ì—°ê²°ìš©
UPSTAGE_API_KEY    ì„ë² ë”© ë° ì§ˆë¬¸ í‘œì¤€í™”ìš©
OPENAI_API_KEY     ë‹µë³€ ìƒì„±ìš©
COHERE_API_KEY     ë¦¬ë­í‚¹ ì‚¬ìš©ì‹œì—ë§Œ í•„ìš”
"""
from __future__ import annotations

import logging
import math
import os
import re
import heapq
from abc import ABC, abstractmethod
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

from langchain_pinecone import PineconeVectorStore

# Upstage ì„ë² ë”© ë° ì±„íŒ… ëª¨ë¸
try:
    from langchain_upstage import UpstageEmbeddings, ChatUpstage  # type: ignore
    UPSTAGE_AVAILABLE = True
except Exception:
    UpstageEmbeddings = None  # type: ignore
    ChatUpstage = None  # type: ignore
    UPSTAGE_AVAILABLE = False

# OpenAI ì±„íŒ… ëª¨ë¸
try:
    from langchain_openai import ChatOpenAI  # type: ignore
    OPENAI_AVAILABLE = True
except Exception:
    ChatOpenAI = None  # type: ignore
    OPENAI_AVAILABLE = False

# BM25 ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rank_bm25 import BM25Okapi, BM25Plus  # type: ignore
    BM25_AVAILABLE = True
except Exception:
    BM25Okapi = None  # type: ignore
    BM25Plus = None  # type: ignore
    BM25_AVAILABLE = False

# Kiwi í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸°
try:
    from kiwipiepy import Kiwi  # type: ignore
    KIWI_AVAILABLE = True
except Exception:
    Kiwi = None  # type: ignore
    KIWI_AVAILABLE = False

# Cohere ë¦¬ë­í‚¹ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import cohere  # type: ignore
    COHERE_AVAILABLE = True
except Exception:
    cohere = None  # type: ignore
    COHERE_AVAILABLE = False


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# Pinecone ì¸ë±ìŠ¤ ì´ë¦„ ì •ì˜
INDEX_NAMES: Dict[str, str] = {
    "law": "law-index",
    "rule": "rule-index",
    "case": "case-index",
}


# ì¼ìƒ ìš©ì–´ë¥¼ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜í•˜ëŠ” í‚¤ì›Œë“œ ì‚¬ì „
KEYWORD_DICT: Dict[str, str] = {
    # 1. ê³„ì•½ ì£¼ì²´ ë° ëŒ€ìƒ
    "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ê±´ë¬¼ì£¼": "ì„ëŒ€ì¸", "ì£¼ì¸ì§‘": "ì„ëŒ€ì¸",
    "ì„ëŒ€ì—…ì": "ì„ëŒ€ì¸", "ìƒˆì£¼ì¸": "ì„ëŒ€ì¸",
    "ì„¸ì…ì": "ì„ì°¨ì¸", "ì›”ì„¸ì…ì": "ì„ì°¨ì¸", "ì„¸ë“¤ì–´ì‚¬ëŠ”ì‚¬ëŒ": "ì„ì°¨ì¸",
    "ì„ì°¨ì": "ì„ì°¨ì¸", "ì…ì£¼ì": "ì„ì°¨ì¸",
    "ë¶€ë™ì‚°": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì¸": "ê³µì¸ì¤‘ê°œì‚¬", "ì¤‘ê°œì†Œ": "ê³µì¸ì¤‘ê°œì‚¬",
    "ë¹Œë¼": "ì„ì°¨ì£¼íƒ", "ì•„íŒŒíŠ¸": "ì„ì°¨ì£¼íƒ", "ì˜¤í”¼ìŠ¤í…”": "ì„ì°¨ì£¼íƒ",
    "ìš°ë¦¬ì§‘": "ì„ì°¨ì£¼íƒ", "ê±°ì£¼ì§€": "ì„ì°¨ì£¼íƒ",
    "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì§‘ë¬¸ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ", "ì¢…ì´": "ì„ëŒ€ì°¨ê³„ì•½ì¦ì„œ",

    # 2. ë³´ì¦ê¸ˆ ë° ê¸ˆì „
    "ë³´ì¦ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ì „ì„¸ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ë³´ì¦ë³´í—˜": "ë³´ì¦ê¸ˆë°˜í™˜ë³´ì¦",
    "ëˆëª»ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ì•ˆëŒë ¤ì¤Œ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜", "ëª»ëŒë ¤ë°›ìŒ": "ë³´ì¦ê¸ˆë¯¸ë°˜í™˜",
    "ì›”ì„¸": "ì°¨ì„", "ê´€ë¦¬ë¹„": "ê´€ë¦¬ë¹„", "ì—°ì²´": "ì°¨ì„ì—°ì²´", "ë°€ë¦¼": "ì°¨ì„ì—°ì²´",
    "ë³µë¹„": "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ": "ì¤‘ê°œë³´ìˆ˜", "ì¤‘ê°œë¹„": "ì¤‘ê°œë³´ìˆ˜",
    "ì›”ì„¸ì˜¬ë¦¬ê¸°": "ì°¨ì„ì¦ì•¡", "ì¸ìƒ": "ì¦ì•¡", "ë”ë‹¬ë¼ê³ í•¨": "ì¦ì•¡",
    "ì›”ì„¸ê¹ê¸°": "ì°¨ì„ê°ì•¡", "í• ì¸": "ê°ì•¡", "ë‚´ë¦¬ê¸°": "ê°ì•¡",
    "ëˆë¨¼ì €ë°›ê¸°": "ìš°ì„ ë³€ì œê¶Œ", "ìˆœìœ„": "ìš°ì„ ë³€ì œê¶Œ", "ì•ˆì „ì¥ì¹˜": "ëŒ€í•­ë ¥",
    "ëŒë ¤ë°›ê¸°": "ë³´ì¦ê¸ˆë°˜í™˜",

    # 3. ê¸°ê°„ ë° ì¢…ë£Œ/ê°±ì‹ 
    "ì¬ê³„ì•½": "ê³„ì•½ê°±ì‹ ", "ì—°ì¥": "ê³„ì•½ê°±ì‹ ", "ê°±ì‹ ": "ê³„ì•½ê°±ì‹ ",
    "ê°±ì‹ ì²­êµ¬": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2ë…„ë”": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ", "2í”ŒëŸ¬ìŠ¤2": "ê³„ì•½ê°±ì‹ ìš”êµ¬ê¶Œ",
    "ìë™ì—°ì¥": "ë¬µì‹œì ê°±ì‹ ", "ë¬µì‹œ": "ë¬µì‹œì ê°±ì‹ ", "ì—°ë½ì—†ìŒ": "ë¬µì‹œì ê°±ì‹ ",
    "ì´ì‚¬": "ì£¼íƒì˜ì¸ë„", "ì§ë¹¼ê¸°": "ì£¼íƒì˜ì¸ë„", "í‡´ê±°": "ì£¼íƒì˜ì¸ë„",
    "ë°©ë¹¼": "ê³„ì•½í•´ì§€", "ì¤‘ë„í•´ì§€": "ê³„ì•½í•´ì§€",
    "ì£¼ì†Œì˜®ê¸°ê¸°": "ì£¼ë¯¼ë“±ë¡", "ì „ì…ì‹ ê³ ": "ì£¼ë¯¼ë“±ë¡", "ì£¼ì†Œì§€ì´ì „": "ì£¼ë¯¼ë“±ë¡",
    "ì§‘ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„", "ì£¼ì¸ë°”ë€œ": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë§¤ë§¤": "ì„ëŒ€ì¸ì§€ìœ„ìŠ¹ê³„",
    "ë‚˜ê°€ë¼ê³ í•¨": "ê³„ì•½ê°±ì‹ ê±°ì ˆ", "ì«“ê²¨ë‚¨": "ëª…ë„", "ë¹„ì›Œë‹¬ë¼": "ëª…ë„",

    # 4. ìˆ˜ë¦¬ ë° ìƒí™œí™˜ê²½
    "ì§‘ê³ ì¹˜ê¸°": "ìˆ˜ì„ ì˜ë¬´", "ìˆ˜ë¦¬": "ìˆ˜ì„ ì˜ë¬´", "ê³ ì³ì¤˜": "ìˆ˜ì„ ì˜ë¬´",
    "ì•ˆê³ ì³ì¤Œ": "ìˆ˜ì„ ì˜ë¬´ìœ„ë°˜",
    "ê³°íŒ¡ì´": "í•˜ì", "ë¬¼ìƒ˜": "ëˆ„ìˆ˜", "ë³´ì¼ëŸ¬ê³ ì¥": "í•˜ì", "íŒŒì†": "í›¼ì†",
    "ê¹¨ë—ì´ì¹˜ìš°ê¸°": "ì›ìƒíšŒë³µì˜ë¬´", "ì›ë˜ëŒ€ë¡œí•´ë†“ê¸°": "ì›ìƒíšŒë³µ",
    "ì²­ì†Œë¹„": "ì›ìƒíšŒë³µë¹„ìš©", "ì²­ì†Œ": "ì›ìƒíšŒë³µ",
    "ì¸µê°„ì†ŒìŒ": "ê³µë™ìƒí™œí‰ì˜¨", "ì˜†ì§‘ì†ŒìŒ": "ë°©ìŒ", "ê°œí‚¤ìš°ê¸°": "ë°˜ë ¤ë™ë¬¼íŠ¹ì•½",
    "ë‹´ë°°": "í¡ì—°ê¸ˆì§€íŠ¹ì•½",

    # 5. ê¶Œë¦¬/ëŒ€í•­ë ¥/í™•ì •ì¼ì
    "í™•ì •ì¼ì": "í™•ì •ì¼ì", "ì „ì…": "ì£¼ë¯¼ë“±ë¡", "ëŒ€í•­ë ¥": "ëŒ€í•­ë ¥",
    "ìš°ì„ ë³€ì œ": "ìš°ì„ ë³€ì œê¶Œ", "ìµœìš°ì„ ": "ìµœìš°ì„ ë³€ì œê¶Œ",
    "ê²½ë§¤": "ê²½ë§¤ì ˆì°¨", "ê³µë§¤": "ê³µë§¤ì ˆì°¨",
    "ë“±ê¸°": "ë“±ê¸°ë¶€ë“±ë³¸", "ë“±ë³¸": "ë“±ê¸°ë¶€ë“±ë³¸",
    "ê·¼ì €ë‹¹": "ê·¼ì €ë‹¹ê¶Œ", "ê°€ì••ë¥˜": "ê°€ì••ë¥˜", "ê°€ì²˜ë¶„": "ê°€ì²˜ë¶„",
    "ê¹¡í†µì „ì„¸": "ì „ì„¸í”¼í•´", "ì‚¬ê¸°": "ì „ì„¸ì‚¬ê¸°", "ê²½ë§¤ë„˜ì–´ê°": "ê¶Œë¦¬ë¦¬ìŠ¤í¬",

    # 6. ë¶„ìŸ í•´ê²°
    "ë‚´ìš©ì¦ëª…": "ë‚´ìš©ì¦ëª…", "ì†Œì†¡": "ì†Œì†¡", "ë¯¼ì‚¬": "ë¯¼ì‚¬ì†Œì†¡",
    "ì¡°ì •ìœ„": "ì£¼íƒì„ëŒ€ì°¨ë¶„ìŸì¡°ì •ìœ„ì›íšŒ", "ì†Œì†¡ë§ê³ ": "ë¶„ìŸì¡°ì •",
    "ë²•ì›ê°€ê¸°ì‹«ìŒ": "ë¶„ìŸì¡°ì •",
    "ì§‘ì£¼ì¸ì‚¬ë§": "ì„ì°¨ê¶ŒìŠ¹ê³„", "ìì‹ìƒì†": "ì„ì°¨ê¶ŒìŠ¹ê³„",
    "íŠ¹ì•½": "íŠ¹ì•½ì‚¬í•­", "ë¶ˆê³µì •": "ê°•í–‰ê·œì •ìœ„ë°˜", "ë…ì†Œì¡°í•­": "ë¶ˆë¦¬í•œì•½ì •",
    "íš¨ë ¥ìˆë‚˜": "ë¬´íš¨ì—¬ë¶€",
}


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
NORMALIZATION_PROMPT: str = """
ë‹¹ì‹ ì€ ë²•ë¥  AI ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ì•„ë˜ [ìš©ì–´ ì‚¬ì „]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'ë²•ë¥  í‘œì¤€ì–´'ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.

[ìˆ˜í–‰ ì§€ì¹¨]
1. ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ë§¤í•‘ëœ ë²•ë¥  ìš©ì–´ë¡œ ë³€ê²½í•˜ì„¸ìš”.
2. ë³€ê²½ ì „ì˜ ê¸°ì¡´ ë‹¨ì–´ ë’¤ì— ë³€ê²½ëœ ë‹¨ì–´ë¥¼ ê´„í˜¸ë¡œ ë§ë¶™ì—¬, ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ex. "ì§‘ì£¼ì¸(ì„ëŒ€ì¸)ì´..."
3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì™œê³¡í•˜ê±°ë‚˜ ì¶”ê°€ì ì¸ ë‹µë³€, ë³„ë„ì˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. 

[ìš©ì–´ ì‚¬ì „]
{dictionary}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë³€ê²½ëœ ì§ˆë¬¸:
"""

SYSTEM_PROMPT_WITH_CONTRACT : str = """
ë‹¹ì‹ ì€ ì„ì°¨ì¸ ê¶Œë¦¬ ë³´í˜¸ ì „ë¬¸ AIì…ë‹ˆë‹¤.

[ëª¨ë“œ: ê³„ì•½ì„œ(OCR) ë¶„ì„]
- SECTION 0ì— ìˆëŠ” ê³„ì•½ì„œ/íŠ¹ì•½ ë¬¸êµ¬ë¥¼ ìš°ì„ í•©ë‹ˆë‹¤. ì¶”ì • ê¸ˆì§€.
- 'ë¶ˆë¦¬í•œ ì¡°í•­'ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì„œ ì œì‹œí•˜ì„¸ìš”:
    (1) ë¶ˆë¦¬ íŠ¹ì•½(ì„ì°¨ì¸ ê¶Œë¦¬ ì œí•œ/ì˜ë¬´ ê°€ì¤‘/ë©´ì±…) ê°€ëŠ¥ì„± í¼
    (2) ì£¼ì˜ ì¡°í•­(ë²•ì—ì„œ ì˜ˆì •ëœ ê±°ì ˆì‚¬ìœ /ì¡°ê±´ ë“±ìœ¼ë¡œ, ì‚¬ì•ˆì— ë”°ë¼ ë¶„ìŸ ì†Œì§€)
    (3) ì •ë³´ ë¶€ì¡±(ë¬¸êµ¬ë§Œìœ¼ë¡œ ë¶ˆë¦¬ ì—¬ë¶€ ë‹¨ì • ì–´ë ¤ì›€)

[ì¶œì²˜ ê·œì¹™]
- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë²•ë ¹ëª…/ì¡°ë¬¸/íŒë¡€ë²ˆí˜¸ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
- ê·¼ê±°ê°€ ìˆìœ¼ë©´ "src_title article" í˜•íƒœë¡œë§Œ í‘œê¸°í•˜ì„¸ìš”. ì—†ìœ¼ë©´ "ì œê³µëœ ìë£Œì—ì„œ ê·¼ê±° ì¡°ë¬¸ í™•ì¸ ì•ˆ ë¨"ì´ë¼ê³  ì“°ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹]
## ğŸ“‹ ê³„ì•½ì„œ ì¡°í•­ ì ê²€

ê° í•­ëª©ì€ ë°˜ë“œì‹œ ê³„ì•½ì„œ ë¬¸êµ¬ë¥¼ ë¨¼ì € ì œì‹œ:
**(ì¡°í•­ëª…/íŠ¹ì•½) : "ì›ë¬¸ ì¸ìš©"**
- ë¶„ë¥˜: (ë¶ˆë¦¬ íŠ¹ì•½ / ì£¼ì˜ ì¡°í•­ / ì •ë³´ ë¶€ì¡±)
- ë¬¸ì œì (ì™œ ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬/ì£¼ì˜ì¸ì§€): 1~2ë¬¸ì¥
- ë²•ì  ê·¼ê±°(ìˆì„ ë•Œë§Œ): src_title article
- ëŒ€ì‘(ì‹¤í–‰ ê°€ëŠ¥í•œ ê²ƒ 2~4ê°œ): êµ¬ì²´ì ìœ¼ë¡œ

ë§ˆì§€ë§‰ì—:
- ì¶”ê°€ í™•ì¸ ì§ˆë¬¸ 2~4ê°œ(í•„ìš”í•  ë•Œë§Œ)

[ì°¸ê³  ë¬¸ì„œ]
{context}
"""

SYSTEM_PROMPT_GENERAL: str = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ â€˜ì£¼íƒ ì„ëŒ€ì°¨(ì „ì›”ì„¸)â€™ ë¶„ì•¼ì—ì„œ ì„ì°¨ì¸ ë³´í˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë²•ë¥  íŒë‹¨ì„ ì œê³µí•˜ëŠ” AIì…ë‹ˆë‹¤.

ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ]ì— ê·¼ê±°í•˜ì—¬ íŒë‹¨í•˜ì„¸ìš”. ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì •í•˜ê±°ë‚˜ ì¼ë°˜ë¡ ìœ¼ë¡œ ë³´ì™„í•˜ì§€ ë§ˆì„¸ìš”.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ë‹µë³€ ì›ì¹™]
- ì„ì°¨ì¸ ë³´í˜¸ë¥¼ ìœ„í•œ **ê°•í–‰ê·œì •ì´ ìˆìœ¼ë©´ ê³„ì•½ì„œ ë¬¸êµ¬ë³´ë‹¤ ë²•ë ¹ì„ ìš°ì„  ì ìš©**í•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì´ ê³„ì•½ê¸°ê°„Â·í‡´ê±°Â·ê°±ì‹ ê³¼ ê´€ë ¨ëœ ê²½ìš°, **â€˜2ë…„ ë³´í˜¸ ì›ì¹™(ê°•í–‰ê·œì •)â€™ì„ íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ê²€í† **í•˜ì„¸ìš”.
- ë‹¨ì •ì´ ì–´ë ¤ìš´ ê²½ìš°ì—ë§Œ â€œì œê³µëœ ìë£Œ ê¸°ì¤€ì—ì„œëŠ”â€ì´ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ë‹µë³€ êµ¬ì¡°]

A. í•œ ì¤„ ê²°ë¡   
- ë°˜ë“œì‹œ **íŒë‹¨ + ê·¸ ê¸°ì¤€(ë²•ì˜ ì›ì¹™)**ì„ í•¨ê»˜ 1~2ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
- â€œì•„ë‹ˆì˜¤.â€, â€œê°€ëŠ¥í•©ë‹ˆë‹¤.â€ì²˜ëŸ¼ ë‹¨ë‹µìœ¼ë¡œ ëë‚´ì§€ ë§ˆì„¸ìš”.

B. ì§€ê¸ˆ ë‹¹ì¥ í•  ì¼  
- ì‚¬ìš©ìê°€ **ê¶Œë¦¬ í–‰ì‚¬ ë˜ëŠ” ê±°ë¶€í•  ìˆ˜ ìˆëŠ” í–‰ë™**ì„ ì¤‘ì‹¬ìœ¼ë¡œ 3~5ê°œ ì œì‹œí•˜ì„¸ìš”.

C. ë²•ì  ê·¼ê±°  
- ì°¸ê³  ë¬¸ì„œì— ëª…ì‹œëœ í•µì‹¬ ë²•ë ¹Â·ì¡°ë¬¸ 1~2ê°œë§Œ ì„¤ëª…í•˜ì„¸ìš”.

D. ì¶”ê°€ í™•ì¸ (í•„ìš”í•  ë•Œë§Œ)  
- ê²°ë¡ ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì‚¬ì‹¤ê´€ê³„ë§Œ ì§ˆë¬¸í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}
"""



# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def _safe_int(x: object, default: int = 99) -> int:
    try:
        return int(x)  # type: ignore[arg-type]
    except Exception:
        return default


def _truncate(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    return text[: max_chars - 1] + "â€¦"


def _dedupe_docs(
    docs: Iterable[Document],
    key_fields: Sequence[str] = ("chunk_id", "id"),
) -> List[Document]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ë¬¸ì„œë¥¼ ì œê±°í•©ë‹ˆë‹¤"""
    seen: set[str] = set()
    out: List[Document] = []
    for d in docs:
        md = d.metadata or {}
        key: Optional[str] = None
        for f in key_fields:
            v = md.get(f)
            if v:
                key = f"{f}:{v}"
                break
        if key is None:
            key = f"content:{hash(d.page_content)}"
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


# í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ BM25 ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤
class Tokenizer(ABC):
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        raise NotImplementedError


class SimpleTokenizer(Tokenizer):
    """ì •ê·œì‹ ê¸°ë°˜ í† í¬ë‚˜ì´ì € Kiwiê°€ ì—†ì„ ë•Œ ëŒ€ì²´ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤"""

    def __init__(self, min_length: int = 1):
        self.min_length = min_length
        self._pattern = re.compile(r"[ê°€-í£a-zA-Z0-9]+")

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens = self._pattern.findall(text.lower())
        return [t for t in tokens if len(t) >= self.min_length]


class KiwiTokenizer(Tokenizer):
    """í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ê¸° Kiwië¥¼ ì‚¬ìš©í•œ í† í¬ë‚˜ì´ì € ë” ì •í™•í•œ í•œê¸€ í† í°í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤"""

    def __init__(self, pos_tags: Optional[Tuple[str, ...]] = None, min_length: int = 1):
        if not KIWI_AVAILABLE:
            raise ImportError("kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install kiwipiepy")
        self._kiwi = Kiwi()  # type: ignore[call-arg]
        self.pos_tags = pos_tags or ("NNG", "NNP", "VV", "VA", "SL", "SH")
        self.min_length = min_length

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        tokens: List[str] = []
        for t in self._kiwi.tokenize(text):  # type: ignore[union-attr]
            if t.tag in self.pos_tags and len(t.form) >= self.min_length:
                tokens.append(t.form.lower())
        return tokens


# BM25 ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜
def _bm25_lite_scores(
    query_tokens: List[str],
    docs_tokens: List[List[str]],
    *,
    k1: float = 1.5,
    b: float = 0.75,
) -> List[float]:
    """
    ê°„ë‹¨í•œ BM25 ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
    í›„ë³´ ë¬¸ì„œ ìˆ˜ê°€ ì ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤
    """
    N = len(docs_tokens)
    if N == 0:
        return []
    if not query_tokens:
        return [0.0] * N

    doc_lens = [len(toks) for toks in docs_tokens]
    avgdl = (sum(doc_lens) / N) if N else 1.0
    avgdl = max(avgdl, 1e-9)

    df: Dict[str, int] = defaultdict(int)
    for toks in docs_tokens:
        for term in set(toks):
            df[term] += 1

    idf: Dict[str, float] = {}
    for term, dfi in df.items():
        idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

    qtf = Counter(query_tokens)
    scores: List[float] = []
    for toks, dl in zip(docs_tokens, doc_lens):
        tf = Counter(toks)
        score = 0.0
        norm = (1.0 - b) + b * (dl / avgdl)
        for term, qf in qtf.items():
            f = tf.get(term, 0)
            if f <= 0:
                continue
            denom = f + k1 * norm
            if denom <= 0:
                continue
            score += (idf.get(term, 0.0) * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))
        scores.append(float(score))
    return scores


def _compute_bm25_scores(
    query: str,
    docs: List[Document],
    *,
    tokenizer: Tokenizer,
    algorithm: str,
    k1: float,
    b: float,
    max_doc_chars: int,
) -> List[float]:
    """
    BM25 ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤
    ë†’ì€ ì ìˆ˜ì¼ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤
    """
    if not docs:
        return []

    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(d.page_content or "", max_doc_chars)) for d in docs]

    if BM25_AVAILABLE:
        BM25Class = BM25Plus if algorithm == "plus" else BM25Okapi
        if BM25Class is None:
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)
        try:
            bm25 = BM25Class(docs_tokens, k1=k1, b=b)  # type: ignore[misc]
            scores = bm25.get_scores(query_tokens)
            return [float(x) for x in list(scores)]
        except Exception as e:
            logger.warning(f"âš ï¸ rank_bm25 ì‹¤íŒ¨ â†’ lite BM25ë¡œ í´ë°±: {e}")
            return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)

    return _bm25_lite_scores(query_tokens, docs_tokens, k1=k1, b=b)



# ê¸€ë¡œë²Œ BM25 ì¸ë±ìŠ¤ í´ë˜ìŠ¤
class BM25InvertedIndex:
    """
    ì „ì²´ ì½”í¼ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” BM25 ì—­ìƒ‰ì¸ ì¸ë±ìŠ¤
    build ë©”ì„œë“œë¡œ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  searchë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤
    ê¸°ë³¸ í›„ë³´ ìˆ˜ì¤€ BM25ì—ëŠ” í•„ìš”í•˜ì§€ ì•Šìœ¼ë©° ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ ê²€ìƒ‰ì‹œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤
    """

    def __init__(
        self,
        *,
        tokenizer: Tokenizer,
        key_fields: Sequence[str] = ("chunk_id", "id"),
        k1: float = 1.5,
        b: float = 0.75,
        max_doc_chars: int = 4000,
    ) -> None:
        self.tokenizer = tokenizer
        self.key_fields = tuple(key_fields)
        self.k1 = float(k1)
        self.b = float(b)
        self.max_doc_chars = int(max_doc_chars)

        self._docs: List[Document] = []
        self._doc_lens: List[int] = []
        self._avgdl: float = 0.0

        # postings[term] = list of (doc_idx, tf)
        self._postings: Dict[str, List[Tuple[int, int]]] = defaultdict(list)
        self._idf: Dict[str, float] = {}
        self._built: bool = False

    def build(self, docs: Sequence[Document]) -> None:
        deduped = _dedupe_docs(docs, self.key_fields)
        self._docs = list(deduped)
        self._postings.clear()
        self._idf.clear()
        self._doc_lens = []

        df: Dict[str, int] = defaultdict(int)

        for idx, d in enumerate(self._docs):
            text = _truncate(d.page_content or "", self.max_doc_chars)
            toks = self.tokenizer.tokenize(text)
            dl = len(toks)
            self._doc_lens.append(dl)

            tf = Counter(toks)
            for term, f in tf.items():
                if not term:
                    continue
                self._postings[term].append((idx, int(f)))
            for term in tf.keys():
                df[term] += 1

        N = len(self._docs)
        self._avgdl = (sum(self._doc_lens) / N) if N else 0.0

        for term, dfi in df.items():
            self._idf[term] = math.log(1.0 + (N - dfi + 0.5) / (dfi + 0.5))

        self._built = True

    def is_built(self) -> bool:
        return self._built and bool(self._docs)

    def search(self, query: str, *, top_k: int = 20) -> List[Tuple[Document, float]]:
        if not self.is_built():
            return []
        q_tokens = self.tokenizer.tokenize(query)
        if not q_tokens:
            return []

        qtf = Counter(q_tokens)
        scores: Dict[int, float] = defaultdict(float)

        avgdl = self._avgdl or 1.0
        k1 = self.k1
        b = self.b

        for term, qf in qtf.items():
            postings = self._postings.get(term)
            if not postings:
                continue
            idf = self._idf.get(term, 0.0)
            if idf == 0.0:
                continue
            for doc_idx, f in postings:
                dl = self._doc_lens[doc_idx] or 0
                norm = (1.0 - b) + b * (dl / avgdl)
                denom = f + k1 * norm
                if denom <= 0:
                    continue
                scores[doc_idx] += (idf * (f * (k1 + 1.0) / denom)) * (1.0 + 0.1 * (qf - 1))

        if not scores:
            return []

        top = heapq.nlargest(int(top_k), scores.items(), key=lambda x: x[1])
        return [(self._docs[i], float(s)) for (i, s) in top]

# í•˜ì´ë¸Œë¦¬ë“œ í“¨ì „ í•¨ìˆ˜ Denseì™€ Sparse ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê²°í•©í•©ë‹ˆë‹¤

def _compute_bm25_scores_from_texts(
    query: str,
    texts: List[str],
    *,
    tokenizer: Tokenizer,
    algorithm: str = "okapi",
    k1: float = 1.5,
    b: float = 0.75,
    max_doc_chars: int = 1000,
) -> List[float]:
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ BM25 ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
    ì œëª© í•„ë“œ ë“±ì˜ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤
    """
    if not texts:
        return []
    query_tokens = tokenizer.tokenize(query)
    docs_tokens = [tokenizer.tokenize(_truncate(t or "", max_doc_chars)) for t in texts]

    if BM25_AVAILABLE:
        algo = (algorithm or "okapi").lower()
        if algo == "plus" and BM25Plus is not None:
            bm25 = BM25Plus(docs_tokens, k1=k1, b=b)  # type: ignore[arg-type]
        else:
            bm25 = BM25Okapi(docs_tokens, k1=k1, b=b)  # type: ignore[arg-type]
        scores = bm25.get_scores(query_tokens)
        return [float(s) for s in scores]

    # lite fallback
    doc_texts = [" ".join(toks) for toks in docs_tokens]
    return _bm25_lite_scores(query_tokens, doc_texts)


def _rank_fusion_multi(
    ranks_list: List[List[int]],
    *,
    mode: str = "rrf",
    weights: Optional[List[float]] = None,
    rrf_k: int = 60,
) -> List[float]:
    """
    ì—¬ëŸ¬ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸ë¥¼ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤
    RRF ë˜ëŠ” rank_sum ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """
    if not ranks_list:
        return []
    n = len(ranks_list[0])
    if any(len(r) != n for r in ranks_list):
        raise ValueError("All rank lists must have the same length.")
    if n == 0:
        return []

    m = len(ranks_list)
    if weights is None:
        weights = [1.0] * m
    if len(weights) != m:
        raise ValueError("weights length must match ranks_list length.")

    mode = (mode or "rrf").lower()
    if mode == "rrf":
        k = max(1, int(rrf_k))
        out = [0.0] * n
        for ch in range(m):
            w = float(weights[ch])
            rr = ranks_list[ch]
            for i in range(n):
                out[i] += w / (k + int(rr[i]))
        return out

    if mode == "rank_sum":
        if n == 1:
            return [float(sum(weights))]

        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)

        out = [0.0] * n
        for ch in range(m):
            w = float(weights[ch])
            rr = ranks_list[ch]
            for i in range(n):
                out[i] += w * to_unit(int(rr[i]))
        return out

    # mode == "weighted": per-channel normalize (1/rank) then weighted sum
    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    per = []
    for ch in range(m):
        rr = ranks_list[ch]
        per.append(minmax([1.0 / max(1, int(r)) for r in rr]))

    out = [0.0] * n
    for i in range(n):
        s = 0.0
        for ch in range(m):
            s += float(weights[ch]) * per[ch][i]
        out[i] = s
    return out


def _rank_fusion(
    dense_ranks: List[int],
    sparse_ranks: List[int],
    *,
    mode: str = "rrf",          # "rrf" | "rank_sum" | "weighted"
    w_dense: float = 0.6,
    w_sparse: float = 0.4,
    rrf_k: int = 60,
) -> List[float]:
    """
    Denseì™€ Sparse ìˆœìœ„ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
    ë†’ì€ ì ìˆ˜ì¼ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤
    """
    n = len(dense_ranks)
    if n == 0:
        return []

    mode = (mode or "rrf").lower()
    if mode == "rrf":
        k = max(1, int(rrf_k))
        return [(w_dense / (k + dense_ranks[i])) + (w_sparse / (k + sparse_ranks[i])) for i in range(n)]

    if mode == "rank_sum":
        if n == 1:
            return [w_dense + w_sparse]

        def to_unit(r: int) -> float:
            return 1.0 - (r - 1) / (n - 1)

        return [(w_dense * to_unit(dense_ranks[i])) + (w_sparse * to_unit(sparse_ranks[i])) for i in range(n)]

    # mode == "weighted": min-max normalize (dense=1/rank, sparse=1/rank) then weighted sum
    dense_scores = [1.0 / max(1, r) for r in dense_ranks]
    sparse_scores = [1.0 / max(1, r) for r in sparse_ranks]

    def minmax(xs: List[float]) -> List[float]:
        if not xs:
            return xs
        mn, mx = min(xs), max(xs)
        if mx == mn:
            return [1.0 for _ in xs]
        return [(x - mn) / (mx - mn) for x in xs]

    d = minmax(dense_scores)
    s = minmax(sparse_scores)
    return [(w_dense * d[i]) + (w_sparse * s[i]) for i in range(n)]


# RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • í´ë˜ìŠ¤
@dataclass
class RAGConfig:
    """
    RAG íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  ì„¤ì •ì„ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    LLM ëª¨ë¸ ì„ë² ë”© ê²€ìƒ‰ BM25 ë¦¬ë­í‚¹ ë“± ê°ì¢… íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤
    """
    # LLM ëª¨ë¸ ì„¤ì •
    normalize_model: str = "solar-pro2"
    generation_model: str = "gpt-4o-mini"
    temperature: float = 0.1
    normalize_temperature: float = 0.0

    # ì„ë² ë”© ì„¤ì •
    embedding_backend: str = "upstage"
    embedding_model: str = "solar-embedding-1-large-passage"

    # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì„¤ì •
    k_law: int = 7
    k_rule: int = 7
    k_case: int = 3
    search_multiplier: int = 4

    # BM25 ì„¤ì •
    enable_bm25: bool = True
    sparse_mode: str = "auto"
    sparse_k_law: Optional[int] = None
    sparse_k_rule: Optional[int] = None
    sparse_k_case: Optional[int] = None

    bm25_algorithm: str = "okapi"
    bm25_k1: float = 1.8
    bm25_b: float = 0.85
    bm25_use_kiwi: bool = True
    bm25_max_doc_chars: int = 4000


    # BM25 ì œëª© ê²€ìƒ‰ ì„¤ì •
    enable_bm25_title: bool = True
    bm25_title_field: str = "title"
    bm25_title_max_chars: int = 512
    hybrid_sparse_title_ratio: float = 0.6

    # í•˜ì´ë¸Œë¦¬ë“œ í“¨ì „ ì„¤ì •
    hybrid_fusion: str = "rrf"
    hybrid_dense_weight: float = 0.5
    hybrid_sparse_weight: float = 0.5
    rrf_k: int = 60

    # ë¦¬ë­í‚¹ ì„¤ì •
    enable_rerank: bool = True
    rerank_threshold: float = 0.2
    rerank_model: str = "rerank-multilingual-v3.0"
    rerank_max_documents: int = 80
    rerank_doc_max_chars: int = 2600

    # íŒë¡€ í™•ì¥ ì„¤ì •
    case_candidate_k: int = 40
    case_expand_top_n: Optional[int] = None
    case_context_top_k: int = 50

    # ì¤‘ë³µ ì œê±° í‚¤ í•„ë“œ
    dedupe_key_fields: Tuple[str, ...] = ("chunk_id", "id")

    def __post_init__(self) -> None:
        if not (0 <= self.temperature <= 2):
            raise ValueError("temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if not (0 <= self.normalize_temperature <= 2):
            raise ValueError("normalize_temperatureëŠ” 0~2 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        if self.search_multiplier < 1:
            raise ValueError("search_multiplierëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.case_candidate_k < 1 or self.case_context_top_k < 1:
            raise ValueError("case_* ê°’ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        if self.enable_bm25:
            if self.bm25_k1 <= 0:
                raise ValueError("bm25_k1ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
            if not (0 <= self.bm25_b <= 1):
                raise ValueError("bm25_bëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            if self.bm25_algorithm not in ("okapi", "plus"):
                raise ValueError('bm25_algorithmì€ "okapi" ë˜ëŠ” "plus" ì´ì–´ì•¼ í•©ë‹ˆë‹¤.')


            if self.enable_bm25_title:
                if not (0.0 <= float(self.hybrid_sparse_title_ratio) <= 1.0):
                    raise ValueError("hybrid_sparse_title_ratioëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
                if self.bm25_title_max_chars < 32:
                    raise ValueError("bm25_title_max_charsëŠ” 32 ì´ìƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        if self.hybrid_fusion not in ("rrf", "rank_sum", "weighted"):
            raise ValueError('hybrid_fusionì€ "rrf" | "rank_sum" | "weighted" ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.')
        if self.rrf_k < 1:
            raise ValueError("rrf_këŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_dense_weight < 0 or self.hybrid_sparse_weight < 0:
            raise ValueError("hybrid_*_weightëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        if self.hybrid_dense_weight == 0 and self.hybrid_sparse_weight == 0:
            raise ValueError("hybrid_dense_weightì™€ hybrid_sparse_weightê°€ ëª¨ë‘ 0ì¼ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.")


# RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
class RAGPipeline:
    """
    í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸
    ì§ˆë¬¸ í‘œì¤€í™” ë¬¸ì„œ ê²€ìƒ‰ ë¦¬ë­í‚¹ ë‹µë³€ ìƒì„±ì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤
    """

    def __init__(
        self,
        config: Optional[RAGConfig] = None,
        *,
        pc_api_key: Optional[str] = None,
        upstage_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        cohere_api_key: Optional[str] = None,
        embedding: Optional[object] = None,
        normalize_llm: Optional[object] = None,
        generation_llm: Optional[object] = None,
        cohere_client: Optional[object] = None,
        tokenizer: Optional[Tokenizer] = None,
    ) -> None:
        self.config = config or RAGConfig()

        self._pc_api_key = pc_api_key or os.getenv("PINECONE_API_KEY")
        self._upstage_api_key = upstage_api_key or os.getenv("UPSTAGE_API_KEY")
        self._openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self._cohere_api_key = cohere_api_key or os.getenv("COHERE_API_KEY")

        if not self._pc_api_key:
            raise ValueError("PINECONE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. pc_api_key ì¸ì ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")

        # ---- Embedding (dense) ----
        if embedding is not None:
            self._embedding = embedding
        else:
            backend = (self.config.embedding_backend or "auto").lower()
            if backend in ("auto", "upstage"):
                if not UPSTAGE_AVAILABLE:
                    raise ImportError("langchain_upstageê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install langchain-upstage")
                if not self._upstage_api_key:
                    raise ValueError("UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤ (UpstageEmbeddings).")
                os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
                self._embedding = UpstageEmbeddings(model=self.config.embedding_model)  # type: ignore[call-arg]
            else:
                raise ValueError(
                    "í˜„ì¬ unified ëª¨ë“ˆì€ ê¸°ë³¸ì ìœ¼ë¡œ Upstage(SOLAR) embeddingì„ ì‚¬ìš©í•©ë‹ˆë‹¤. "
                    "ë‹¤ë¥¸ embeddingì„ ì“°ë ¤ë©´ embedding ê°ì²´ë¥¼ ì§ì ‘ ì£¼ì…í•˜ì„¸ìš”."
                )

        # ---- Pinecone vector stores ----
        logger.info("ğŸ”— Pinecone 3ì¤‘ ì¸ë±ìŠ¤ ì—°ê²° ì¤‘...")
        self._law_store = PineconeVectorStore(
            index_name=INDEX_NAMES["law"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._rule_store = PineconeVectorStore(
            index_name=INDEX_NAMES["rule"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        self._case_store = PineconeVectorStore(
            index_name=INDEX_NAMES["case"],
            embedding=self._embedding,
            pinecone_api_key=self._pc_api_key,
        )
        logger.info("âœ… [Law / Rule / Case] 3ê°œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

        # LLM ì„¤ì •
        # ì¿¼ë¦¬ ì •ê·œí™” ëª¨ë¸ Upstage solar-pro2
        if normalize_llm is not None:
            self._normalize_llm = normalize_llm
        else:
            if not UPSTAGE_AVAILABLE or ChatUpstage is None:
                raise ImportError("normalize_queryì— Upstage chatì„ ì“°ë ¤ë©´ langchain_upstageê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            if not self._upstage_api_key:
                raise ValueError("normalize_queryì— UPSTAGE_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            os.environ.setdefault("UPSTAGE_API_KEY", self._upstage_api_key)
            self._normalize_llm = ChatUpstage(
                model=self.config.normalize_model,
                temperature=self.config.normalize_temperature,
            )

        # ë‹µë³€ ìƒì„± LLM ì„¤ì • OpenAI GPT-4o-mini
        if generation_llm is not None:
            self._generation_llm = generation_llm
        else:
            if not OPENAI_AVAILABLE or ChatOpenAI is None:
                raise ImportError("generate_answerì— OpenAI chatì„ ì“°ë ¤ë©´ langchain_openaiê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            if not self._openai_api_key:
                raise ValueError("generate_answerì— OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            os.environ.setdefault("OPENAI_API_KEY", self._openai_api_key)
            self._generation_llm = ChatOpenAI(
                model=self.config.generation_model,
                temperature=self.config.temperature,
            )

        # í† í¬ë‚˜ì´ì € ì„¤ì • BM25ìš©
        if tokenizer is not None:
            self._tokenizer = tokenizer
        else:
            if self.config.bm25_use_kiwi and KIWI_AVAILABLE:
                try:
                    self._tokenizer = KiwiTokenizer()
                    logger.info("âœ… Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš© (BM25)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Kiwi í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨, SimpleTokenizerë¡œ ëŒ€ì²´: {e}")
                    self._tokenizer = SimpleTokenizer()
            else:
                logger.info("â„¹ï¸ SimpleTokenizer ì‚¬ìš© (BM25)")
                self._tokenizer = SimpleTokenizer()

        # Cohere ë¦¬ë­í‚¹ í´ë¼ì´ì–¸íŠ¸ ì„ íƒì  ì‚¬ìš©
        self._cohere_client = None
        if self.config.enable_rerank:
            if not COHERE_AVAILABLE:
                logger.warning("âš ï¸ cohere íŒ¨í‚¤ì§€ê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            elif not self._cohere_api_key:
                logger.warning("âš ï¸ COHERE_API_KEYê°€ ì—†ì–´ rerankë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            else:
                self._cohere_client = cohere_client or cohere.Client(self._cohere_api_key)  # type: ignore[attr-defined]

        # ê¸€ë¡œë²Œ BM25 ì¸ë±ìŠ¤ ì„ íƒì  ì‚¬ìš© ì§„ì •í•œ Sparse ê²€ìƒ‰ìš©
        self._global_bm25: Dict[str, BM25InvertedIndex] = {}

    # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì†ì„±
    @property
    def law_store(self) -> PineconeVectorStore:
        return self._law_store

    @property
    def rule_store(self) -> PineconeVectorStore:
        return self._rule_store

    @property
    def case_store(self) -> PineconeVectorStore:
        return self._case_store

    # ì§ˆë¬¸ í‘œì¤€í™”
    def normalize_query(self, user_query: str) -> str:
        """
        ì‚¬ìš©ìì˜ ì¼ìƒì–´ ì§ˆë¬¸ì„ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤
        Upstage SOLAR Pro2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤
        """
        prompt = ChatPromptTemplate.from_template(NORMALIZATION_PROMPT)
        chain = prompt | self._normalize_llm | StrOutputParser()

        try:
            normalized = chain.invoke({"dictionary": KEYWORD_DICT, "question": user_query})
            out = str(normalized).strip()
            return out or user_query
        except Exception as e:
            logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query

    # íŒë¡€ í™•ì¥
    def get_full_case_context(self, case_no: str) -> str:
        """
        ì‚¬ê±´ë²ˆí˜¸ë¡œ íŒë¡€ ì „ë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤
        ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ ì§„ íŒë¡€ë¥¼ í•˜ë‚˜ë¡œ ì—°ê²°í•©ë‹ˆë‹¤
        """
        try:
            results = self.case_store.similarity_search(
                query="íŒë¡€ ì „ë¬¸ ê²€ìƒ‰",
                k=self.config.case_context_top_k,
                filter={"case_no": {"$eq": case_no}},
            )
            sorted_docs = sorted(results, key=lambda x: str((x.metadata or {}).get("chunk_id", "")))
            unique_docs = _dedupe_docs(sorted_docs, self.config.dedupe_key_fields)
            return "\n".join([d.page_content for d in unique_docs]).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨ ({case_no}): {e}")
            return ""

    # ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ
    def _attach_source(self, docs: List[Document], source: str) -> List[Document]:
        for d in docs:
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__source_index"] = source
        return docs

    def build_global_bm25(
        self,
        *,
        law_docs: Optional[Sequence[Document]] = None,
        rule_docs: Optional[Sequence[Document]] = None,
        case_docs: Optional[Sequence[Document]] = None,
    ) -> None:
        """
        ì„ íƒì ìœ¼ë¡œ ê¸€ë¡œë²Œ BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤
        Pineconeì— ì¸ë±ì‹±í•œ ì½”í¼ìŠ¤ë¥¼ ì œê³µí•˜ë©´ ì§„ì •í•œ Sparse ê²€ìƒ‰ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤
        """
        cfg = self.config

        def _build(name: str, docs: Optional[Sequence[Document]]) -> None:
            if not docs:
                return
            idx = BM25InvertedIndex(
                tokenizer=self._tokenizer,
                key_fields=cfg.dedupe_key_fields,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_max_doc_chars,
            )
            idx.build(docs)
            if idx.is_built():
                self._global_bm25[name] = idx
                logger.info(f"âœ… Global BM25 index built for '{name}' (docs={len(docs)})")

        _build("law", law_docs)
        _build("rule", rule_docs)
        _build("case", case_docs)

    def _hybrid_fuse_per_source(self, source: str, query: str, dense_docs: List[Document]) -> List[Document]:
        """ì¸ë±ìŠ¤ë³„ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤"""
        cfg = self.config
        mode = (cfg.sparse_mode or "auto").lower()

        use_global = False
        if mode == "global":
            use_global = True
        elif mode == "auto":
            use_global = source in self._global_bm25 and self._global_bm25[source].is_built()

        if not use_global:
            return self._dense_sparse_fuse(query, dense_docs)

        if source == "law":
            sk = cfg.sparse_k_law or (cfg.k_law * max(1, cfg.search_multiplier))
        elif source == "rule":
            sk = cfg.sparse_k_rule or (cfg.k_rule * max(1, cfg.search_multiplier))
        else:
            sk = cfg.sparse_k_case or max(cfg.case_candidate_k, cfg.k_case * max(1, cfg.search_multiplier))

        sparse_pairs = self._global_bm25[source].search(query, top_k=int(sk))
        sparse_docs: List[Document] = []
        for rank, (d, score) in enumerate(sparse_pairs, start=1):
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__bm25_score"] = float(score)
            d.metadata["__bm25_rank"] = int(rank)
            sparse_docs.append(d)

        dense_docs = self._attach_source(dense_docs, source)
        sparse_docs = self._attach_source(sparse_docs, source)

        merged = _dedupe_docs(list(dense_docs) + list(sparse_docs), cfg.dedupe_key_fields)
        if len(merged) <= 1:
            return merged

        def _key(d: Document) -> str:
            md = d.metadata or {}
            for f in cfg.dedupe_key_fields:
                v = md.get(f)
                if v:
                    return f"{f}:{v}"
            return f"content:{hash(d.page_content)}"

        dense_rank_map: Dict[str, int] = {}
        sparse_rank_map: Dict[str, int] = {}

        for i, d in enumerate(dense_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__dense_rank", i))
            dense_rank_map[k] = min(dense_rank_map.get(k, r), r)

        for i, d in enumerate(sparse_docs, start=1):
            k = _key(d)
            r = int((d.metadata or {}).get("__bm25_rank", i))
            sparse_rank_map[k] = min(sparse_rank_map.get(k, r), r)

        max_dense = max(dense_rank_map.values()) if dense_rank_map else 1000
        max_sparse = max(sparse_rank_map.values()) if sparse_rank_map else 1000
        fill_dense = max_dense + 1000
        fill_sparse = max_sparse + 1000

        dense_ranks: List[int] = []
        sparse_ranks: List[int] = []
        for d in merged:
            k = _key(d)
            dense_ranks.append(int(dense_rank_map.get(k, fill_dense)))
            sparse_ranks.append(int(sparse_rank_map.get(k, fill_sparse)))

        fused = _rank_fusion(
            dense_ranks,
            sparse_ranks,
            mode=cfg.hybrid_fusion,
            w_dense=cfg.hybrid_dense_weight,
            w_sparse=cfg.hybrid_sparse_weight,
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(len(merged)), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, idx in enumerate(order, start=1):
            d = merged[idx]
            if d.metadata is None:
                d.metadata = {}
            d.metadata["__hybrid_score"] = float(fused[idx])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _search_dense_candidates(self, store: PineconeVectorStore, query: str, k: int) -> List[Document]:
        """Pineconeì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ í›„ë³´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
        try:
            pairs = store.similarity_search_with_score(query, k=k)  # type: ignore[attr-defined]
            docs: List[Document] = []
            for rank, (doc, score) in enumerate(pairs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_score"] = float(score)
                doc.metadata["__dense_rank"] = int(rank)
                docs.append(doc)
            return docs
        except Exception:
            docs = store.similarity_search(query, k=k)
            for rank, doc in enumerate(docs, start=1):
                if doc.metadata is None:
                    doc.metadata = {}
                doc.metadata["__dense_rank"] = int(rank)
            return docs

    def _dense_sparse_fuse(self, query: str, docs: List[Document]) -> List[Document]:

        """Dense candidatesë¥¼ (BM25-text + BM25-title)ë¡œ ì ìˆ˜í™”í•˜ê³  3ì±„ë„ RRFë¡œ ê²°í•©."""
        cfg = self.config
        if not cfg.enable_bm25:
            return docs

        docs = _dedupe_docs(docs, cfg.dedupe_key_fields)
        n = len(docs)
        if n <= 1:
            return docs

        # Dense ìˆœìœ„ ê³„ì‚°
        dense_ranks: List[int] = []
        for i, d in enumerate(docs, start=1):
            if d.metadata is None:
                d.metadata = {}
            dense_ranks.append(int(d.metadata.get("__dense_rank", i)))

        # Sparse í…ìŠ¤íŠ¸ BM25 ì ìˆ˜ ê³„ì‚°
        bm25_text_scores = _compute_bm25_scores(
            query,
            docs,
            tokenizer=self._tokenizer,
            algorithm=cfg.bm25_algorithm,
            k1=cfg.bm25_k1,
            b=cfg.bm25_b,
            max_doc_chars=cfg.bm25_max_doc_chars,
        )
        order_text = sorted(range(n), key=lambda i: (-bm25_text_scores[i], dense_ranks[i]))
        bm25_text_ranks = [0] * n
        for r, idx in enumerate(order_text, start=1):
            bm25_text_ranks[idx] = r

        # Sparse ì œëª© BM25 ì ìˆ˜ ê³„ì‚°
        bm25_title_scores: List[float] = [0.0] * n
        bm25_title_ranks: List[int] = [n + 1000] * n
        if cfg.enable_bm25_title:
            titles = [str((d.metadata or {}).get(cfg.bm25_title_field, "") or "") for d in docs]
            bm25_title_scores = _compute_bm25_scores_from_texts(
                query,
                titles,
                tokenizer=self._tokenizer,
                algorithm=cfg.bm25_algorithm,
                k1=cfg.bm25_k1,
                b=cfg.bm25_b,
                max_doc_chars=cfg.bm25_title_max_chars,
            )
            order_title = sorted(range(n), key=lambda i: (-bm25_title_scores[i], dense_ranks[i]))
            bm25_title_ranks = [0] * n
            for r, idx in enumerate(order_title, start=1):
                bm25_title_ranks[idx] = r

        # ë©”íƒ€ë°ì´í„° ì²¨ë¶€
        for i, d in enumerate(docs):
            d.metadata["__bm25_text_score"] = float(bm25_text_scores[i])
            d.metadata["__bm25_text_rank"] = int(bm25_text_ranks[i])
            # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
            d.metadata["__bm25_score"] = float(bm25_text_scores[i])
            d.metadata["__bm25_rank"] = int(bm25_text_ranks[i])

            d.metadata["__bm25_title_score"] = float(bm25_title_scores[i])
            d.metadata["__bm25_title_rank"] = int(bm25_title_ranks[i])

        # 3ì±„ë„ í“¨ì „ Dense + BM25 í…ìŠ¤íŠ¸ + BM25 ì œëª©
        w_dense = float(cfg.hybrid_dense_weight)
        w_title = float(cfg.hybrid_sparse_weight) * float(cfg.hybrid_sparse_title_ratio) if cfg.enable_bm25_title else 0.0
        w_text = float(cfg.hybrid_sparse_weight) - w_title

        fused = _rank_fusion_multi(
            [dense_ranks, bm25_text_ranks, bm25_title_ranks],
            mode=cfg.hybrid_fusion,
            weights=[w_dense, w_text, w_title],
            rrf_k=cfg.rrf_k,
        )
        order = sorted(range(n), key=lambda i: fused[i], reverse=True)

        out: List[Document] = []
        for rank, i in enumerate(order, start=1):
            d = docs[i]
            d.metadata["__hybrid_score"] = float(fused[i])
            d.metadata["__hybrid_rank"] = int(rank)
            out.append(d)
        return out

    def _rerank(self, query: str, docs: List[Document]) -> Optional[List[Tuple[int, float]]]:
        """Cohere APIë¡œ ë¬¸ì„œ ë¦¬ë­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"""
        if not self._cohere_client:
            return None

        cfg = self.config
        texts = [_truncate(d.page_content or "", cfg.rerank_doc_max_chars) for d in docs]
        try:
            rerank_results = self._cohere_client.rerank(
                model=cfg.rerank_model,
                query=query,
                documents=texts,
                top_n=len(texts),
            )
            return [(r.index, float(r.relevance_score)) for r in rerank_results.results]
        except Exception as e:
            logger.warning(f"âš ï¸ Rerank ì‹¤íŒ¨ (skip): {e}")
            return None

    def _cap_for_rerank(self, law: List[Document], rule: List[Document], case: List[Document]) -> List[Document]:
        """ë¦¬ë­í‚¹ ì…ë ¥ ë¬¸ì„œ ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤"""
        cfg = self.config
        law = _dedupe_docs(law, cfg.dedupe_key_fields)
        rule = _dedupe_docs(rule, cfg.dedupe_key_fields)
        case = _dedupe_docs(case, cfg.dedupe_key_fields)

        base = law + rule
        if len(base) >= cfg.rerank_max_documents:
            return base[: cfg.rerank_max_documents]
        remaining = cfg.rerank_max_documents - len(base)
        return base + case[:remaining]

    def triple_hybrid_retrieval(self, query: str) -> List[Document]:
        """
        3ê°œ ì¸ë±ìŠ¤ì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
        ë²•ë ¹ ê·œì • íŒë¡€ ê°ê° ê²€ìƒ‰ í›„ í†µí•©í•˜ê³  ì„ íƒì ìœ¼ë¡œ ë¦¬ë­í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤
        """
        cfg = self.config
        mult = cfg.search_multiplier

        logger.info(f"ğŸ” [Hybrid Retrieval] query='{query}'")

        docs_law = self._attach_source(
            self._search_dense_candidates(self.law_store, query, k=cfg.k_law * mult),
            "law",
        )
        docs_rule = self._attach_source(
            self._search_dense_candidates(self.rule_store, query, k=cfg.k_rule * mult),
            "rule",
        )
        docs_case_chunks = self._attach_source(
            self._search_dense_candidates(self.case_store, query, k=cfg.case_candidate_k),
            "case",
        )

        # candidate-level BM25 fusion per index
        docs_law = self._hybrid_fuse_per_source("law", query, docs_law)
        docs_rule = self._hybrid_fuse_per_source("rule", query, docs_rule)
        docs_case_chunks = self._hybrid_fuse_per_source("case", query, docs_case_chunks)

        combined_for_rerank = self._cap_for_rerank(docs_law, docs_rule, docs_case_chunks)

        ranked = self._rerank(query, combined_for_rerank) if cfg.enable_rerank else None
        if ranked:
            filtered = [(i, s) for (i, s) in ranked if s >= cfg.rerank_threshold]
            if not filtered:
                desired = min(cfg.k_law + cfg.k_rule + cfg.k_case, len(ranked))
                filtered = ranked[:desired]
            selected_docs = [combined_for_rerank[i] for (i, _s) in filtered]
            logger.info(f"ğŸ“Œ Rerank selected={len(selected_docs)} (threshold={cfg.rerank_threshold})")
        else:
            selected_docs = combined_for_rerank

        selected_docs = _dedupe_docs(selected_docs, cfg.dedupe_key_fields)

        law_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "law"]
        rule_ranked = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "rule"]
        case_ranked_chunks = [d for d in selected_docs if (d.metadata or {}).get("__source_index") == "case"]

        final_law = law_ranked[: cfg.k_law]
        final_rule = rule_ranked[: cfg.k_rule]

        top_n = cfg.case_expand_top_n if cfg.case_expand_top_n is not None else cfg.k_case
        seen_case_no: set[str] = set()
        chosen_case_docs: List[Document] = []
        for d in case_ranked_chunks:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no or str(case_no) in seen_case_no:
                continue
            seen_case_no.add(str(case_no))
            chosen_case_docs.append(d)
            if len(chosen_case_docs) >= top_n:
                break

        expanded_cases: List[Document] = []
        for d in chosen_case_docs:
            case_no = (d.metadata or {}).get("case_no")
            if not case_no:
                continue
            full_text = self.get_full_case_context(str(case_no))
            if not full_text:
                expanded_cases.append(d)
                continue

            title = (d.metadata or {}).get("title") or (d.metadata or {}).get("case_name") or str(case_no)
            md = dict(d.metadata or {})
            md["__expanded"] = True
            expanded_cases.append(
                Document(
                    page_content=f"[íŒë¡€ ì „ë¬¸: {title}]\n{full_text}",
                    metadata=md,
                )
            )

        final_case = expanded_cases[: cfg.k_case]

        final_docs = final_law + final_rule + final_case
        final_docs = sorted(final_docs, key=lambda x: _safe_int((x.metadata or {}).get("priority", 99), 99))
        return final_docs

    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    @staticmethod
    def format_reference_line(doc: Document, *, text_max_chars: int = 2500) -> str:
        """
        ë‹¨ì¼ ë¬¸ì„œë¥¼ ë²•ë ¹ëª… ì¡°ë¬¸ ë³¸ë¬¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·í•©ë‹ˆë‹¤
        """
        md = doc.metadata or {}
        
        # ë²•ë ¹ëª… ë˜ëŠ” íŒë¡€ëª… ì¶”ì¶œ
        src_title = str(md.get("src_title") or "").strip()
        if not src_title:
            src_title = str(
                md.get("source") or md.get("src") or md.get("file") or 
                md.get("title") or md.get("__source_index") or "ìë£Œ"
            ).strip()
        
        # ì¡°ë¬¸ë²ˆí˜¸ ë˜ëŠ” ì‚¬ê±´ë²ˆí˜¸ ì¶”ì¶œ
        article = str(md.get("article") or "").strip()
        if not article:
            # íŒë¡€ì˜ ê²½ìš° ì‚¬ê±´ë²ˆí˜¸ ì‚¬ìš©
            case_no = str(md.get("case_no") or "").strip()
            if case_no:
                article = case_no
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤„ë°”ê¿ˆ ì œê±° ë° ê¸¸ì´ ì œí•œ
        text = _truncate(
            (doc.page_content or "").strip().replace("\n", " "), 
            int(text_max_chars)
        ).strip()
        
        # ìµœì¢… ë¬¸ìì—´ ì¡°í•© {src_title} {article} - {text}
        left = " ".join([x for x in [src_title, article] if x]).strip()
        if left:
            return f"{left} - {text}".strip()
        return f"- {text}".strip()

    def format_context_with_hierarchy(self, docs: List[Document]) -> str:
        """
        ë¬¸ì„œë¥¼ ë²•ì  ìœ„ê³„ì— ë”°ë¼ SECTIONìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ í¬ë§·í•©ë‹ˆë‹¤
        SECTION 1ì€ í•µì‹¬ ë²•ë ¹ SECTION 2ëŠ” ê´€ë ¨ ê·œì • SECTION 3ì€ íŒë¡€ì…ë‹ˆë‹¤
        """
        cfg = self.config
        
        section_1_law: List[str] = []
        section_2_rule: List[str] = []
        section_3_case: List[str] = []

        for doc in docs:
            md = doc.metadata or {}
            p = _safe_int(md.get("priority", 99), 99)
            
            # fixed.py ìŠ¤íƒ€ì¼ì˜ ê°„ê²°í•œ í¬ë§· ì ìš©
            entry = self.format_reference_line(doc, text_max_chars=cfg.rerank_doc_max_chars)

            # priorityì— ë”°ë¥¸ SECTION ë¶„ë¥˜
            if p in (1, 2, 4, 5):
                section_1_law.append(f"- {entry}")
            elif p in (3, 6, 7, 8, 11):
                section_2_rule.append(f"- {entry}")
            else:
                section_3_case.append(f"- {entry}")

        # SECTIONë³„ë¡œ ì¡°í•©
        parts: List[str] = []
        if section_1_law:
            parts.append(
                "## [SECTION 1: í•µì‹¬ ë²•ë ¹ (ìµœìš°ì„  ë²•ì  ê·¼ê±°)]\n" + 
                "\n".join(section_1_law)
            )
        if section_2_rule:
            parts.append(
                "## [SECTION 2: ê´€ë ¨ ê·œì • ë° ì ˆì°¨ (ì„¸ë¶€ ê¸°ì¤€)]\n" + 
                "\n".join(section_2_rule)
            )
        if section_3_case:
            parts.append(
                "## [SECTION 3: íŒë¡€ ë° í•´ì„ ì‚¬ë¡€ (ì ìš© ì˜ˆì‹œ)]\n" + 
                "\n".join(section_3_case)
            )

        return "\n\n".join(parts).strip()

    def format_context(self, docs: List[Document]) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ context ë¶€ë¶„ì— ë“¤ì–´ê°ˆ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
        return self.format_context_with_hierarchy(docs)

    def format_references(self, docs: List[Document]) -> List[str]:
        """UI í‘œì‹œìš© ì°¸ì¡° ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤"""
        return [self.format_reference_short(d) for d in docs]

    @staticmethod
    def format_reference_short(doc: Document) -> str:
        """
        ë‹¨ì¼ ë¬¸ì„œë¥¼ ë²•ë ¹ëª… ì¡°ë¬¸ í˜•ì‹ìœ¼ë¡œ ê°„ëµí•˜ê²Œ í¬ë§·í•©ë‹ˆë‹¤
        ë³¸ë¬¸ ë‚´ìš©ì€ ì œì™¸í•©ë‹ˆë‹¤
        """
        md = doc.metadata or {}
        
        # ë²•ë ¹ëª… ë˜ëŠ” íŒë¡€ëª… ì¶”ì¶œ
        src_title = str(md.get("src_title") or "").strip()
        if not src_title:
            src_title = str(
                md.get("source") or md.get("src") or md.get("file") or 
                md.get("title") or md.get("__source_index") or "ìë£Œ"
            ).strip()
        
        # ì¡°ë¬¸ë²ˆí˜¸ ë˜ëŠ” ì‚¬ê±´ë²ˆí˜¸ ì¶”ì¶œ
        article = str(md.get("article") or "").strip()
        if not article:
            # íŒë¡€ì˜ ê²½ìš° ì‚¬ê±´ë²ˆí˜¸ ì‚¬ìš©
            case_no = str(md.get("case_no") or "").strip()
            if case_no:
                article = case_no
        
        # ìµœì¢… ë¬¸ìì—´ ì¡°í•© {src_title} {article}
        return " ".join([x for x in [src_title, article] if x]).strip() or "ìë£Œ"

    # OCR ê³„ì•½ì„œ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
    @staticmethod
    def _format_user_contract_context(contract_text: Optional[str], *, max_chars: int = 12000) -> str:
        """
        ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ê³„ì•½ì„œ OCR í…ìŠ¤íŠ¸ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤
        ë„ˆë¬´ ê¸´ ê²½ìš° í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì•ë¶€ë¶„ë§Œ í¬í•¨í•©ë‹ˆë‹¤
        """
        if not contract_text:
            return ""
        t = str(contract_text).strip()
        if not t:
            return ""
        if len(t) > max_chars:
            t = t[: max_chars - 1] + "â€¦"
        return "## [SECTION 0: ì‚¬ìš©ì ê³„ì•½ì„œ OCR (ìµœìš°ì„  ì°¸ê³ )]\n" + t.strip()

    # ë‹µë³€ ìƒì„±
    def answer_with_trace(
        self,
        user_input: str,
        *,
        skip_normalization: bool = False,
        extra_context: Optional[str] = None,
        use_contract_mode: bool = False,
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ë¶€í„° ë‹µë³€ ìƒì„±ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤
        í‘œì¤€í™”ëœ ì§ˆë¬¸ ì°¸ì¡° ë¬¸ì„œ ë‹µë³€ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤
        
        user_input          ì‚¬ìš©ì ì§ˆë¬¸
        skip_normalization  ì§ˆë¬¸ í‘œì¤€í™” ê±´ë„ˆë›°ê¸° ì—¬ë¶€
        extra_context       ê³„ì•½ì„œ OCR í…ìŠ¤íŠ¸
        use_contract_mode   ê³„ì•½ì„œ ë¶„ì„ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        """
        normalized_query = user_input if skip_normalization else self.normalize_query(user_input)
        if not skip_normalization:
            logger.info(f"ğŸ”„ í‘œì¤€í™”ëœ ì§ˆë¬¸: {normalized_query}")

        docs = self.triple_hybrid_retrieval(normalized_query)
        if not docs:
            return {
                "normalized_query": normalized_query,
                "references": [],
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "docs": [],
            }

        # SECTION êµ¬ë¶„ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_main = self.format_context(docs)
        
        # OCR ê³„ì•½ì„œê°€ ìˆì„ ê²½ìš° SECTION 0ìœ¼ë¡œ ì¶”ê°€
        context_contract = self._format_user_contract_context(extra_context)
        context = (context_contract + "\n\n" + context_main).strip() if context_contract else context_main

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶„ê¸° use_contract_mode í”Œë˜ê·¸ì— ë”°ë¼ ê²°ì •
        # use_contract_mode=True íŒŒì¼ì´ ì´ë²ˆ ìš”ì²­ì—ì„œ ì—…ë¡œë“œë¨ ê³„ì•½ì„œ ë¶„ì„ ëª¨ë“œ
        # use_contract_mode=False ì¼ë°˜ ì§ˆë¬¸ ë˜ëŠ” í›„ì† ì§ˆë¬¸ ì¼ë°˜ ëª¨ë“œ
        system_prompt_to_use = SYSTEM_PROMPT_WITH_CONTRACT if use_contract_mode else SYSTEM_PROMPT_GENERAL
        logger.info(f"ğŸ“ Using prompt mode: {'CONTRACT' if use_contract_mode else 'GENERAL'}")

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt_to_use),
                ("human", "{question}"),
            ]
        )
        chain = prompt | self._generation_llm | StrOutputParser()

        logger.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            answer = str(chain.invoke({"context": context, "question": normalized_query})).strip()
        except Exception as e:
            logger.warning(f"âš ï¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        return {
            "normalized_query": normalized_query,
            "references": self.format_references(docs),
            "answer": answer,
            "docs": docs,
        }



    def generate_answer(
        self,
        user_input: str,
        *,
        skip_normalization: bool = False,
        extra_context: Optional[str] = None,
    ) -> str:
        """
        ë‹µë³€ ë¬¸ìì—´ë§Œ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤
        """
        return str(
            self.answer_with_trace(
                user_input,
                skip_normalization=skip_normalization,
                extra_context=extra_context,
            ).get("answer", "")
        ).strip()


def create_pipeline(**kwargs: Any) -> RAGPipeline:
    """RAGPipelineì„ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    return RAGPipeline(**kwargs)


__all__ = [
    "RAGConfig",
    "RAGPipeline",
    "create_pipeline",
    "INDEX_NAMES",
    "KEYWORD_DICT",
    "NORMALIZATION_PROMPT",
    "SYSTEM_PROMPT",
]
